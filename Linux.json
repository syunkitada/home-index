{"Entry":0,"TokenDocumentIndexMap":null,"IdTextMap":[{"Text":"# Linux\n\n## Basic Contents\n\n| Link                                           | Description                                              |\n| ---------------------------------------------- | -------------------------------------------------------- |\n| [OS](os.md)                                    | OS ついて                                                |\n| [カーネルと起動時の処理](kernel_boot.md)       | カーネルの場所、起動時の流れ                             |\n| [プロセスとスケジューラ](process_scheduler.md) | プロセス実行の仕組み、スケジューリングの仕組み、割り込み |\n| [ファイルシステム](filesystem.md)              | ファイル、ファイルシステム、ブロックデバイス             |\n| [デバイス](device.md)                          | デバイス                                                 |\n| [メモリ](memory.md)                            | メモリ管理                                               |\n\n## Kernel\n\n| Link                                         | Description              |\n| -------------------------------------------- | ------------------------ |\n| [カーネル](kernel.md)                        | カーネルメモ             |\n| [カーネルデバイスドライバ](kernel_driver.md) | カーネルデバイスドライバ |\n| [カーネルパニック](kernel_panic.md)          | カーネルパニック         |\n| [デバッグ](debugging.md)                     |                          |\n| [テスト](testing.md)                         |                          |\n\n## CPU\n\n| Link                                 | Description      |\n| ------------------------------------ | ---------------- |\n| [CPU](cpu.md)                        | CPU              |\n| [CPU(ハードウェア)](cpu_hardware.md) | CPU の仕組みとか |\n\n## Memory\n\n| [メモリ](memory.md) | メモリの仕組みとか |\n| [メモリ(ハードウェア)](memory_hardware.md) | メモリの仕組みとか |\n| [メモリ(チューニング)](memory_hardware.md) | メモリのチューニングについて |\n\n## Network\n\n| Link                                  | Description                               |\n| ------------------------------------- | ----------------------------------------- |\n| [Network Hostory](network_history.md) | ネットワークの歴史について                |\n| [Network Basic](network_basic.md)     | ネットワークの基礎知識、OSI とか TCP とか |\n| [Network](network.md)                 | ネットワークについて                      |\n| [iptables](iptables.md)               | iptables メモ                             |\n| [xdp](xdp.md)                         | xdp メモ                                  |\n\n## IO Device\n\n| Link                               | Description                                  |\n| ---------------------------------- | -------------------------------------------- |\n| [ファイルシステム](filesystem.md)  | ファイル、ファイルシステム、ブロックデバイス |\n| [ブロックデバイス](blockdevice.md) | ブロックデバイス                             |\n| [デバイス](device.md)              | デバイス                                     |\n\n## その他\n\n| Link                      | Description          |\n| ------------------------- | -------------------- |\n| [container](container.md) | コンテナ技術について |\n| [systemd](systemd.md)     | systemd              |\n| [strace](strace.md)       | strace いろいろ      |\n\n## References\n\n- kernel 全般\n  - [GeeksforGeeks: The Linux Kernel](https://www.geeksforgeeks.org/the-linux-kernel/)\n  - [Linux Kernel Teaching](https://linux-kernel-labs.github.io/refs/heads/master/index.html)\n  - [The Linux Kernel documentation v4.15](https://www.kernel.org/doc/html/v4.15/index.html)\n  - [kernel_map](http://www.makelinux.net/kernel_map/)\n  - [Linux perf tools](http://www.brendangregg.com/Perf/linuxperftools.png)\n  - [Understanding the Linux Virtual Memory Manager](https://www.kernel.org/doc/gorman/html/understand/)\n  - [openSUSE: System Analysis and Tuning Guide](https://doc.opensuse.org/documentation/leap/archive/42.3/tuning/html/book.sle.tuning/book.sle.tuning.html)\n- Linux Advent Calendar\n  - [2013](https://qiita.com/advent-calendar/2013/linux)\n  - [2014](https://qiita.com/advent-calendar/2014/linux)\n  - [2015](https://qiita.com/advent-calendar/2015/linux)\n  - [2016](https://qiita.com/advent-calendar/2016/linux)\n  - [2017](https://qiita.com/advent-calendar/2017/linux)\n  - [2018](https://qiita.com/advent-calendar/2018/linux)\n  - [2019](https://qiita.com/advent-calendar/2019/linux)\n- NetworkStack\n  - [Network data flow](https://mwiki.static.linuxfound.org/images/1/1c/Network_data_flow_through_kernel.png)\n  - [Netfilter packet flow](https://upload.wikimedia.org/wikipedia/commons/3/37/Netfilter-packet-flow.svg)\n- StorageStack\n  - [Linux Storage Stack Diagram](https://www.thomas-krenn.com/en/wiki/Linux_Storage_Stack_Diagram)\n  - [Linux Multi-Queue Block IO Queueing Mechanism (blk-mq)](\u003chttps://www.thomas-krenn.com/en/wiki/Linux_Multi-Queue_Block_IO_Queueing_Mechanism_(blk-mq)\u003e)\n- Tuning\n  - [openSUSE: Part V Kernel Tuning](https://doc.opensuse.org/documentation/leap/archive/42.3/tuning/html/book.sle.tuning/part.tuning.kernel.html)\n- CaseStudy\n  - Memory\n    - [Linkedin: Optimizing Linux Memory Management for Low-latency / High-throughput Databases](https://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases)\n","UpdatedAt":"2021-04-11T23:24:33.8380111+09:00"},{"Text":"# ブロックデバイス\n\n## References\n\n- [電源を切っても消えないメモリとの付き合い方](https://speakerdeck.com/fadis/dian-yuan-woqie-tutemoxiao-enaimemoritofalsefu-kihe-ifang)\n- [極めて速いストレージとの付き合い方](https://speakerdeck.com/fadis/ji-metesu-isutorezitofalsefu-kihe-ifang)\n","UpdatedAt":"2021-03-15T21:55:34.733569+09:00"},{"Text":"# Container\n\n- コンテナの仕組みについて\n\n## コンテナ上でプロセスを動かすということ\n\n- プロセスにおける権限の制限\n  - systemd(root) は、fork して、execve で sshd(root)を実行\n  - sshd(root)は、ユーザから ssh で接続されると、fork して setuid でそのユーザへ変更し、execve で bash を実行する\n  - 子プロセスは、親の権限やファイルディスクリプタを引き継ぐので、execve する前に権限を落としたり、見られたくないファイルを閉じる\n  - 通常のプロセスではユーザの権限などである程度の制限をかけることは可能だが限界がある\n- コンテナとはプロセスに制限を加えるためのユーザ空間のようなもので、以下のような技術からなる\n  - chroot\n    - プロセスがパス解決する際の root を変更する\n  - capabilities\n    - root が持つ特権を分割したものを capabilities と呼び、通常は子プロセスもこれをそのまま継承するが、この権限を制限することができる\n  - namespace(名前空間)\n    - プロセスの閲覧できるもの、利用できるものを制限する\n    - namespace には以下のものがある(\\$ ls -iL /proc/self/ns で確認できる)\n      - mnt\n      - uts\n      - pid\n      - pid_for_children namespace\n      - ipc\n      - user\n      - net\n  - cgroup\n    - プロセスのリソースを制限する\n\n## bind mount\n\n- mount コマンドは、--bind オプションを付けるとディレクトリを別のディレクトリにマウントできる\n- ファイルパスを解決するとき、マウントされたディレクトリ(flag のついてる dentry)ではその inode ではなく、マウントテーブルから対応する dentry を引いてパス解決する\n\n```\n$ sudo mount [ブロックデバイス] [マウントポイント]\n$ sudo mount --bind [ディレクトリ] [マウントポイント]\n```\n\n## chroot\n\n- プロセスを指定したディレクトリ以下に閉じ込める\n- chroot する前にオープンしたファイルはそのパスにかかわらず読み書きできるので注意\n\n```\n# /usrをrootとしてbashを起動\n$ sudo chroot /usr /bin/bash\nbash-4.2# ls /\nbin  etc  games  include  lib  lib64  libexec  local  sbin  share  src  tmp\n```\n\n```\n# /tmp/xrootをrootとしてbashを起動\n-bash-4.2$ sudo chroot /tmp/xroot /bin/bash\nchroot: failed to run command ‘/bin/bash’: No such file or directory\n\n-bash-4.2$ sudo mkdir /tmp/xroot/bin\n-bash-4.2$ sudo mkdir /tmp/xroot/lib64\n-bash-4.2$ sudo mount --bind /usr/bin/ /tmp/xroot/bin/\n-bash-4.2$ sudo mount --bind /usr/lib64/ /tmp/xroot/lib64/\n-bash-4.2$ sudo chroot /tmp/xroot /bin/bash\nbash-4.2# ls /\nbin  lib64\n\n# 単体のコマンドを実行する場合は以下のように実行できる\n-bash-4.2$ sudo chroot /tmp/xroot /bin/ls /\nbin  lib64\n\n# procをマウントする\nbash-4.2# mkdir /proc\nbash-4.2# mount -t proc none /proc\nbash-4.2# ls /proc/\n1     1369  14    2     2660  2865  30    3415  3585  52   737  843        consoles     filesystems  keys        modules       self           timer_stats\n10    1370  1462  20    2681  2875  32    3416  37    53   740  849        cpuinfo      fs           kmsg        mounts        slabinfo       tty\n11    1375  1504  21    2682  2876  3229  3463  38    66   751  9          crypto       interrupts   kpagecount  mtrr          softirqs       uptime\n12    1379  16    2140  2697  2877  3231  3467  39    670  757  96         devices      iomem        kpageflags  net           stat           version\n1229  1380  18    22    2698  29    3386  3526  40    7    765  acpi       diskstats    ioports      loadavg     pagetypeinfo  swaps          vmallocinfo\n13    1381  182   23    27    2991  3389  3548  48    718  772  buddyinfo  dma          irq          locks       partitions    sys            vmstat\n1359  1382  183   24    2798  2993  3391  3578  5     720  785  bus        driver       kallsyms     mdstat      sched_debug   sysrq-trigger  zoneinfo\n1360  1383  19    25    28    2994  3395  3579  50    721  796  cgroups    execdomains  kcore        meminfo     schedstat     sysvipc\n1361  1384  1916  26    2825  3     3396  3581  51    733  8    cmdline    fb           key-users    misc        scsi          timer_list\n```\n\n## 権限(capabilities)\n\n- root が持つ特権を分割したものを capabilities と呼び、通常は子プロセスもこれをそのまま継承するが、この権限を制限することができる\n- 詳細は、man capabilities を参照\n\n```\n$ sudo yum install libcap-ng-utils\n\n# プロセスのCapabilitiesを確認する\n$ pscap -a\nppid  pid   name        command           capabilities\n0     1     root        systemd           full\n1     1462  root        systemd-journal   chown, dac_override, dac_read_search, fowner, setgid, setuid, sys_ptrace, sys_admin, audit_control, mac_override, syslog\n1     1504  root        systemd-udevd     full\n1     1916  root        auditd            full\n1     2682  root        gssproxy          full\n1     2698  dbus        dbus-daemon       audit_write +\n1     2798  chrony      chronyd           net_bind_service, sys_time +\n1     2825  root        systemd-logind    chown, dac_override, dac_read_search, fowner, kill, sys_admin, sys_tty_config, audit_control, mac_admin\n1     2875  root        crond             full\n1     2876  root        agetty            full\n1     2877  root        agetty            full\n1     2991  root        sshd              full\n1     2993  root        tuned             full\n1     2994  root        rsyslogd          full\n1     3229  root        master            full\n2991  3389  root        sshd              full\n2991  3391  root        sshd              full\n```\n\n## namespace\n\n- /proc/N/ns からプロセス N がどの namespace に所属しているかわかる\n  - /proc/N/ns 以下のファイルをネームスペース識別ファイル\n- ネームスペースに所属する最後のプロセスが exit すると、そのネームスペースも消滅する\n- ネームスペースを維持したい場合は、bind mount してファイルシステムツリーから参照した状態にしておく\n- システムコール\n  - clone\n  - unshare\n  - setns\n\n```\n# namespaceの確認\n$ ls -iL /proc/self/ns\n4026531839 ipc  4026531840 mnt  4026531956 net  4026531836 pid  4026531837 user  4026531838 uts\n\n# プロセスのNSを表示する\n$ lsns\nNS TYPE  NPROCS   PID USER  COMMAND\n4026531836 pid        3  3396 goapp -bash\n4026531837 user       3  3396 goapp -bash\n4026531838 uts        3  3396 goapp -bash\n4026531839 ipc        3  3396 goapp -bash\n4026531840 mnt        3  3396 goapp -bash\n4026531956 net        3  3396 goapp -bash\n\n# psコマンドでプロセスのNSを表示する\n$ sudo ps -eo pid,pidns,netns,mntns,comm,args\n...\n```\n\n## uts namespace\n\n```\n# 親NS\n-bash-4.2$ ls -iL /proc/self/ns/uts\n4026531838 /proc/self/ns/uts\n-bash-4.2$ hostname\nlocalhost\n\n# 子NSを作成してhostnameを変更する\n-bash-4.2$ sudo unshare --uts /bin/bash\n[root@localhost /]# hostname\nlocalhost\n[root@localhost /]# hostname myhost\n[root@localhost /]# hostname\nmyhost\n[root@localhost /]# ls -iL /proc/self/ns/uts\n4026532216 /proc/self/ns/uts\n[root@localhost /]# exit\nexit\n\n# 親NSのhostnameはそのまま\n-bash-4.2\\$ hostname\nlocalhost\n```\n\n```\n# straceで観察する\n$ sudo LC_ALL=C strace -o unshare.log unshare --uts /bin/true\nexecve(\"/bin/unshare\", [\"unshare\", \"--uts\", \"/bin/true\"], 0x7ffdee418530 /* 27 vars _/) = 0\n...\nunshare(CLONE_NEWUTS) = 0\nexecve(\"/bin/true\", [\"/bin/true\"], 0x7ffea1ea2c78 /_ 27 vars \\_/) = 0\n...\n\n```\n\n## pid, pid_for_children namespace\n\n- pid とプロセスの対応関係を分離する\n- namespace を削除するとその配下のプロセスも削除される\n- pid namespace\n  - 自身が所属する\n- pid_for_children\n  - 自身から派生する子プロセスが所属する\n\n```\n# kernel\n$ ls -iL /proc/self/ns/pid*\n4026531836 /proc/self/ns/pid\n\n# kernel 4.12から\n$ ls -iL /proc/self/ns/pid*\n4026531836 /proc/self/ns/pid  4026531836 /proc/self/ns/pid_for_children\n```\n\n```\n# 子NSを作成してsleepを実行する\n$ sudo unshare --pid --fork --mount-proc /bin/bash\n# sleep 3600 \u0026\n# pstree -anpl\nbash,1\n  ├─sleep,16 3600\n    └─pstree,17 -anpl\n```\n\n```\n# 親から子NSのpidを見る\n$ sudo ls -iL /proc/3944/ns/pid\n4026532217 /proc/3944/ns/pid\n\n$ sleep 60 \u0026\n[1] 3993\n\n-bash-4.2$ ls -iL /proc/4010/ns/pid\n4026531836 /proc/4010/ns/pid\n\n-bash-4.2$ sleep 600 \u0026\n[1] 4010\n-bash-4.2$ sudo ps -eo pid,pidns,comm,args  | grep sleep\n 3944 4026532217 sleep           sleep 3600\n 4010 4026531836 sleep           sleep 600\n 4012 4026531836 grep            grep --color=auto sleep\n```\n\n## mnt namespace\n\n- プロセスから見えているマウントの集合、操作を分離する\n- Namespace 内の mount, umount は他の Namespace には影響しない\n\n```\n-bash-4.2$ sudo mount --bind /tmp/mnttestsrc /tmp/mnttestdest\n\n-bash-4.2$ sudo unshare --mount /bin/bash\n[root@localhost /]# mount | grep mnttest\n/dev/vda1 on /tmp/mnttestdest type xfs (rw,relatime,seclabel,attr2,inode64,noquota)\n[root@localhost /]# umount /tmp/mnttestdest\n[root@localhost /]# mount | grep mnttest\n[root@localhost /]#\n```\n\n親からは mount されたまま\n\n```\n-bash-4.2$ mount | grep mnttest\n/dev/vda1 on /tmp/mnttestdest type xfs (rw,relatime,seclabel,attr2,inode64,noquota)\n```\n\n```\n# 親側で新たにmountする\n-bash-4.2$ mount | grep mnttest\n/dev/vda1 on /tmp/mnttestdest type xfs (rw,relatime,seclabel,attr2,inode64,noquota)\n/dev/vda1 on /tmp/mnttestdest2 type xfs (rw,relatime,seclabel,attr2,inode64,noquota)\n\n# 子側でには反映されない\n[root@localhost /]# mount | grep mnttest\n[root@localhost /]#\n\n```\n\nunsahre --mount --propaganation slave\n親から子へ伝搬する\n\nunsahre --mount --propaganation shared\n双方へ伝搬する\n\n## net namespace\n\n- ネットワークデバイス、アドレス、ルーティングテーブル、ARP テーブル、ソケット、iptables の設定を分離\n- 親からの継承はなしで、ほぼ新しい空間が用意される\n\n```\n# unshareコマンドでnet namespaceを作成\n$ sudo unshare --net /bin/bash\n\n[root@localhost /]# ip a\n1: lo: \u003cLOOPBACK\u003e mtu 65536 qdisc noop state DOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n```\n\n```\n# ipコマンドで名前付きでnet namespaceを作成\n$ sudo ip netns add mynet\n\n$ sudo nsenter --net=/var/run/netns/mynet ip a\n1: lo: \u003cLOOPBACK\u003e mtu 65536 qdisc noop state DOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n```\n\n```\n# netnsの一覧\n$ sudo ip netns\nmynet\n\n# netns内でコマンドを実行する\n$ sudo ip netns exec mynet bash\n[root@localhost /]# ip a\n1: lo: \u003cLOOPBACK\u003e mtu 65536 qdisc noop state DOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n```\n\n## user namespace\n\n- user namespace は一般ユーザでも作れる\n- namespace 内のユーザを特権ユーザとして作成することもできる\n  - 権限は作ったユーザの権限となる\n  - uid は 0 になる\n\n```\n$ unshare --user id\nuid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup)\n\n$ unshare --user --map-root-user id\nuid=0(root) gid=0(root) groups=0(root),65534(nogroup)\n```\n\n## References\n\n- [Linux コンテナの内部を知ろう](https://speakerdeck.com/tenforward/osc-2018-kyoto)\n- [Linux Namespaces](https://www.slideshare.net/masamiichikawa/linux-namespaces-53216942)\n- [Kubernetes で cgroup がどう利用されているか](https://valinux.hatenablog.com/entry/20210114)\n","UpdatedAt":"2021-04-11T23:23:30.6943036+09:00"},{"Text":"# cpu\n\ntaskset -c 3\n\n# governor\n\n```\n$ cat /proc/cpuinfo| grep MHz\ncpu MHz         : 1555.574\ncpu MHz         : 1551.938\ncpu MHz         : 3893.328\ncpu MHz         : 3892.303\ncpu MHz         : 1554.666\ncpu MHz         : 1556.543\ncpu MHz         : 1375.079\ncpu MHz         : 1374.843\ncpu MHz         : 1374.393\ncpu MHz         : 1374.590\ncpu MHz         : 1374.630\ncpu MHz         : 1374.328\n\n\n# driverの確認\n$ cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_driver\nacpi-cpufreq\n\n# governorの確認\n$ cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor\nondemand\n\n# 最大周波数\n$ cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_max_freq\n3400000\n\n# 最小周波数\n$ cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_min_freq\n1550000\n\ngrep \"\"\n$ cat /sys/devices/system/cpu/cpu0/cpufreq/bios_limit\n3400000\n\n\n# 以下のディレクトリないにcpuの周波数変化の統計情報がある\n# /sys/devices/system/cpu/cpu0/cpufreq/stats/*\n#\nhttps://www.kernel.org/doc/Documentation/cpu-freq/cpufreq-stats.txt\n\n\n\n$ grep '' /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n/sys/devices/system/cpu/cpu0/cpufreq/scaling_governor:ondemand\n/sys/devices/system/cpu/cpu1/cpufreq/scaling_governor:ondemand\n/sys/devices/system/cpu/cpu10/cpufreq/scaling_governor:ondemand\n/sys/devices/system/cpu/cpu11/cpufreq/scaling_governor:ondemand\n/sys/devices/system/cpu/cpu2/cpufreq/scaling_governor:ondemand\n/sys/devices/system/cpu/cpu3/cpufreq/scaling_governor:ondemand\n/sys/devices/system/cpu/cpu4/cpufreq/scaling_governor:ondemand\n/sys/devices/system/cpu/cpu5/cpufreq/scaling_governor:ondemand\n/sys/devices/system/cpu/cpu6/cpufreq/scaling_governor:ondemand\n/sys/devices/system/cpu/cpu7/cpufreq/scaling_governor:ondemand\n/sys/devices/system/cpu/cpu8/cpufreq/scaling_governor:ondemand\n/sys/devices/system/cpu/cpu9/cpufreq/scaling_governor:ondemand\n\n```\n","UpdatedAt":"2021-03-15T21:55:34.736566+09:00"},{"Text":"# CPU(ハードウェア)\n\n## CPU の基本原理\n\n- CPU ができるのはデータのコピー、四則演算、論理演算、数値比較などだけ\n  - 高度な演算もこれらの基本演算を組み合わせて実現しているだけ\n  - トランジスタのスイッチ特性を利用することで、これらの演算を実行するための回路を構成している\n  - CPU はメモリから値をレジスタにロードして、何らかの演算をし、レジスタに格納されている結果をメモリに書き込む、基本的にはこの繰り返し\n- トランジスタ\n  - マイクロプロセッサのものになる電子回路のほとんどはトランジスタの組み合わせでできている\n  - トランジスタは、増幅器としても使えるが、高速なスイッチの役割を持っており、その高速な ON/OFF 動作でさまざまな計算を行っている\n  - プロセス・テクノロジー\n    - 半導体チップの製造技術のこと\n    - プロセスノードの微細化は直近の約 10 年間で、90nm -\u003e 65nm -\u003e 45nm -\u003e 32nm -\u003e 22nm -\u003e 14nm -\u003e 10nm と進化してきた\n    - Intel の 10nm プロセスでは、1mm^2 当たり 1 億 80 万個のトランジスタを詰め込む\n  - 参考文献\n    - [Intel: トランジスターの仕組み](https://www.intel.co.jp/content/www/jp/ja/innovation/transworks.html)\n- レジスタ\n  - CPU はレジスタと呼ばれる小規模な記憶装置を何個か持っている\n    - レジスタは 8bit, 16bit, 32bit, 64bit といった数値を記憶することができる\n  - アドレッシングモード\n    - レジスタに記録された数値をそのまま数値として解釈する場合とアドレスとして解釈する場合がある\n      - アドレスで解釈する場合はさらに、絶対的なアドレスとして解釈するのか、相対的なアドレスとして解釈するのか、あるいは計算結果のアドレスとして解釈するかの違いがある\n      - この解釈の指定の仕方をアドレッシングモードといい、機械語命令で指定できる\n    - アドレッシングモードの種類\n      - 即値 (immediate または literal): 指定されている数値そのものを指す\n      - 絶対アドレス (absolute address): 指定されている数値で示されるメモリー上のアドレスを指す\n      - レジスタ直接 (register direct or 単に register): レジスタの数値そのものを指す\n      - レジスタ間接 (register indirect): レジスタの数値で示されるメモリ上のアドレスを指す\n      - ベースオフセット (base plus offset or base plus displacement): 指定したレジスタ(ベース)に符号つき数値 (オフセット) を加えたアドレスを指す\n        - レジスタ間接に機能を足したもの\n      - レジスタ自動加算 (register auto increment): 指定したレジスタのアドレスを指すが、命令が終わった後、符号付き数値を加算・減算します\n        - レジスタ間接に機能を足したもの\n      - プログラムカウンタ相対アドレス (PC-relative address): 現在のプログラムに指定されている符号つき数値を加えたアドレスを指します。\n  - 特殊レジスタ\n    - 汎用レジスタは、アドレッシングモードによって使い分けしているが、特定の用途専用の特殊レジスタがある\n    - 代表的な特殊レジスタ\n      - プログラムカウンタ\n        - CPU はプログラムカウンタの指し示すメモリから機械語コードを読み取り(fetch)、その意味を解釈し(decode)、実行し(execute)、結果を書き込む(store)\n        - fetch が終わったときにプログラムカウンタのアドレスを一つ進める\n        - また、条件分岐などで特定のアドレスに値を代入することもある(分岐命令)\n        - fetch, decode, execute, store のサイクルを命令サイクル(instruction cycle, fetch-and-execute cycle, fetch-decode-execute cycle, FDX)と呼びます\n        - CPU は、一つの機械語命令実行で必ず命令サイクルを 1 通り行う\n        - しかし、命令サイクルを一つづつ実行する必要はなく、ある命令を実行している間に次の命令の fetch を行うというように効率よく進めることも可能\n          - このような方式に、パイプライン方式、スーパースカラ方式などがある\n      - スタックポインタ\n        - 関数や手続き、サブルーチンの呼び出しの実用に用いる\n        - 関数 a, b, c を入れ子で呼び出しているとき、その返りは c が終了すると b に戻り、b が終了すると a に戻るという LIFO になっている\n        - 例\n          - サブルーチンを呼び出す命令を fetch/decode すると execute/store のサイクルで、スタックポインタの指し示すアドレスに現在のプログラムカウンタの値を書き込んで、スタックポインタを増やし、プログラムカウンタに呼び出す先のサブルーチンのアドレスを代入する\n          - そして、サブルーチンが終了して、復帰する命令(return)を fetch/decode すると execute/store のサイクルで、スタックポインタを減らし、スタックポインタの指し示すアドレスから値を読み取ってプログラムカウンタに代入する\n      - フラグ\n        - 計算結果を格納する\n        - N(negative)フラグ: 計算結果が負だと 1、それ以外で 0 になる\n        - Z(zero)フラグ: 計算結果が 0 もしくは等しかった場合に 1、それ以外で 0 になる\n        - C(carry)フラグ: 符号なしの整数だと思って計算したときに、桁上がりもしくは桁下がりが起こった場合に 1、それ以外で 0 になる\n        - V(overflow)フラグ: 符号付きの整数だと思って計算したときに、桁あふれが起こった場合に 1、それ以外で 0 になる\n- 機械語\n  - CPU が解釈する命令は、実際には対応する数値で表される\n  - 命令の集合の事を命令セットとよび、CPU のメーカや世代によって異なる\n  - 機械語とは CPU によって直接実行される命令とデータの体系のこと\n  - 機械語をそのまま覚えるのは難しいため、機械語と人が読めるように一対一で対応させた言語がアセンブリ言語\n  - 通常のプログラムはこの機械語に翻訳(コンパイル)されることで CPU で実行可能になる\n\n## 用語\n\n- パイプライン処理\n  - 命令サイクルを一つづつ実行する必要はなく、ある命令を実行している間に次の命令の fetch を行うというように、各工程を流れ作業のように処理すること\n- スーパースカラ\n  - 一つのコアで命令処理を実行する回路(パイプライン)を複数持ち、複数の命令処理を同時に実行する仕組み\n- 論理プロセッサコア(マルチスレッディング、ハイパースレッディング)\n  - 実際の物理コア数よりも多く(基本 2 倍)のコアを OS に見せる技術\n  - OS が複数のプロセスを別々のコアに割り当てても、物理的には同じコアで複数のプロセスが同じ実行エンジンを共有している\n  - これによりパイプラインを効率的に使える\n- 分岐予測、投機実行\n  - 命令処理の結果によって次に実行する命令処理が異なる場合、どの命令処理が実行されるかを予測する仕組み\n    - 過去の実行履歴にもとづき、次に実行される可能性が高い分岐の命令予測する\n  - その予想にもとづいて、命令処理を先に実行する仕込みを投機実行という\n- アウトオブオーダ\n  - 実行すべき命令は基本的に順に実行していかなければならないが、結果に影響を与えないのであれば順序を問わずどんどん実行することにより、複数命令の同時実行を可能にする\n    - 「順序を守らない実行」の意である(順序通りに実行することを、インオーダと呼ぶ)\n  - アウトオブオーダプロセッサは過度な投機実行を行い、その命令が本当に「正しい」命令であるかどうかを確認する必要がある\n- リオーダバッファ\n  - アウトオブオーダで発行された各種演算ユニットで処理された命令を、処理が完了した段階で回収し、最後に順番を命令順にそろえるユニットのこと\n    - 言い換えると、命令をインオーダ完了させるためのユニット\n  - リオーダバッファに溜め込まれ、前の命令(条件分岐命令)の結果が確定したことをもって、その命令が有効かどうかを決定する\n    - このステージで「命令がコミット」されると、その命令が実行されることを確定し、リオーダバッファからレジスタファイルへのデータ書き戻しなどの確定処理が行われる\n      - そうでなければ、その命令はリオーダバッファから破棄される\n    - このとき、コミットされた命令は命令の実行が確定したとして、最後の書き戻し処理を行って「リタイア」される\n- ミスプレディクションウィンドウ\n  - 投機実行では、分岐予測が完了するまでとりあえず次の命令を fetch して発行し続けるが、これらの命令は条件分岐予測が外れることにより破棄されるかもしれない\n  - ミスプレディクションウィンドウは、この条件分岐命令において、どれくらいの後続の命令が発行されるかを示しており、このウィンドウが大きいほど多くの命令が破棄される\n- ノンブロッキングキャッシュ制御\n  - ある命令処理に必要なデータがキャッシュメモリになく、メインメモリまでデータを取りに行く間に、別の命令処理に必要なデータをキャッシュメモリに取りに行くことができる仕組み\n- キャッシュメモリ\n  - CPU のチップ上に搭載されている高速少量のメモリ\n    - DRAM よりも高速で高価な SRAM が使われている\n  - アクセスする頻度の高いデータや命令をキャッシュメモリに保存しておくことで、メモリアクセスの時間を短縮できる\n- キャッシュヒット、キャッシュミス\n  - CPU はデータを読み込む際に、まずキャッシュメモリ上を探す\n  - これがあればキャッシュヒット、そのキャッシュからデータをレジスタにストアする\n  - これがなければキャッシュミス、メインメモリからデータをレジスタにストアし、このときキャッシュにもストアする\n- キャッシュデータ、アンキャッシュデータ\n  - リアルタイムデータや制御データなどのタイムクリティカルなデータは、キャッシュに入れてしまうと実際にコアの外のバスに流れるのがいつになるかわからないので、アンキャッシュなデータとしてレジスタに持ってきたり、レジスタからストアする\n  - アンキャッシュなデータはキャッシュを汚すことがないというのが一つの利点であり、またすぐにコア外のバスまで出ていくのでタイミングクリティカルな制御信号などを流すのに使われる\n- ハードウェアプリフェッチ\n  - プログラムのメモリアクセスの規則性から、今後のデータアクセスをハードウェア自身により予測し、あらかじめメインメモリからキャッシュメモリにデータを読み込む仕組み\n\n## Meltdown と Spectore\n\n- どちらも CPU の投機的実行の設計ミスを利用した攻撃(サイドチャネル攻撃)\n- Spector\n  - アプリケーション内で悪意あるプログラムを埋め込み、そのアプリケーション内のメモリ領域を推測する攻撃\n  - 攻撃例\n    - ブラウザが悪意ある JavaScript を実行したとき、ブラウザがメモリ領域上に保存しているパスワードなどを読むことができる\n- Meltdown\n  - ユーザ空間のプロセスが、カーネルのメモリ空間の中身を推測する攻撃\n  - 攻撃例\n    - 悪意あるプログラムが、ホストのカーネル空間を読むことができる\n    - 仮想マシンが、親ホストのカーネル空間のアドレスを読むことができる？\n- Variant 1: bounds check bypass(Spector)\n  - 一般的なアプリケーションが、配列にアクセスしようとするとき、バッファオーバーフローなどの対策のために Bounds Check を行う\n    - 例えば、配列の長さを調べて、その範囲内でのみアクセスするといったコード\n  - しかし、投機実行では if 文のあとの配列にアクセスする処理が先に実行される場合があり、その計算結果がキャッシュに乗ってしまう\n  - その後、そのキャッシュにヒットするかで、その値を推測する\n  - この脆弱性はバウンドチェックをしているコードパターンがないとできないらしい\n  - 対策\n    - アプリケーションによる修正が必要\n- Variant 2: branch target injection(Spector)\n  - 条件分岐で pc のアドレスが飛ぶとき、投機実行では、すでに飛んだことある履歴から実行することができる\n  - これを使うと、本来飛ぶところではないところに、投機的に実行でき、その結果がキャッシュに乗ってしまう\n  - その後、そのキャッシュにヒットするかで、その値を推測する\n  - ゲスト側から kvm のシンボルのアドレスが分かる\n  - 対策\n    - アプリケーションによる修正が必要\n- Variant 3: rogue data cache load(Meltdown)\n  - カーネルメモリアドレスを読み込む命令を実行し、その読み込んだデータを使って別計算をする\n  - このとき、本来はセグメンテーションフォルト例外となるが、それが確定する前に投機実行で次の計算が実行されてしまい、その結果がキャッシュに乗ってしまう\n    - リオーダの段階になって例外が発生し、リタイアするがキャッシュは消えない\n  - その後、そのキャッシュにヒットするかで、その値を推測する\n  - 対策\n    - OS による修正が必要\n    - KPTI(Kernel Page-Table Isolation)\n      - ユーザモードとカーネルモードで利用するページテーブルが共通であることが原因\n      - そのためユーザモードとカーネルモードで利用するページテーブルを分けた\n      - しかし、ユーザモードとカーネルモードが切り替わるときにページテーブルも切り替える必要があるためコストが高い\n        - TLB フラッシュが伴う\n      - ユーザモードのときはカーネルのページ部分をアンマップし、カーネルモードになるときにマップする\n        - ページテーブルの切り替えではない？\n- 派生攻撃\n  - BranchScope\n    - BPU(Branch Prediction Unit)の方向予測をターゲットにした攻撃\n    - http://www.itmedia.co.jp/news/articles/1803/28/news062.html\n\n### 対策コードを無効にする\n\n- https://access.redhat.com/articles/3311301\n- https://access.redhat.com/ja/articles/3316261\n\n```\n# CentOS\n$ echo 0 \u003e /sys/kernel/debug/x86/pti_enabled\n$ echo 0 \u003e /sys/kernel/debug/x86/ibpb_enabled\n$ echo 0 \u003e /sys/kernel/debug/x86/ibrs_enabled\n\n# centos6ではdebugfsをマウントする必要がある(centos7ではデフォルトでマウントされている)\n$ mount -t debugfs nodev /sys/kernel/debug\n```\n\n## Microcode\n\n- CPU を外部から制御する場合の命令単位をインストラクションと呼ぶ\n- ある命令を実行するために必要な手続きがマイクロコードとして実装されている\n- Microcode は CPU 内部のメモリに保存されている\n- MCU(Microcode Update)\n  - Microcode を修正するためのコード\n  - 各 CPU にあわせて作成される\n  - BIOS および OS に保存される\n    - マシンのブート時に BIOS 内の MCU が CPU にロードされ、OS 起動時に BIOS の MCU より新しい revision が OS 内にあればそれをロードする\n  - MCU は、マザーボード・OS ベンダーに配布され、BIOS や OS に組み込まれた状態でエンドユーザにに提供される\n- 参考\n  - [Intel Microcode の基礎](http://datyotosanpo.blog.fc2.com/blog-entry-180.html)\n  - [Linux カーネルが x86 microcode を扱う処理について](https://qiita.com/akachochin/items/ae91efec12297fd05c0b)\n\n## CPU の仕様書・設計 IP・製品について\n\n- 仕様書\n  - ISA(Instruction Set Architecture: 命令仕様)のこと\n  - x86\n    - クロスライセンス契約により Intel, AMD のみ利用可能\n  - ARMv7, ARMv8\n    - これを直接利用するためには ARM からアーキテクチャライセンスを購入する必要がある\n  - RISC-V\n    - オープンソースで誰でも利用可能\n- IP(Intellectual Property)\n  - 仕様書をもとに作成された回路設計データのこと\n    - ARM などはこのデータを半導体メーカに提供し、ライセンス料をもらう事で商売をしている\n  - x86\n    - Intel, AMD がそれぞれ独自に所持している\n  - ARM\n    - Arm 社の Cortex シリーズ\n      - これを利用するためには、ARM からプロセッサライセンスを購入する必要がある\n    - Qualcomm 社の Kryo シリーズ\n    - Apple 社の A シリーズに搭載されている SoC など\n- 製品\n  - IP をもとに製造された製品の事\n  - x86\n    - Intel, AMD がそれぞれ独自に CPU を製造している\n  - ARM\n    - Qualcomm 社の Snapdragon シリーズ\n    - Apple 社の A シリーズ\n- x86 のライセンスについて\n  - 初期の Intel は自社だけでは CPU を安定供給できない状況から、他の半導体メーカにも製造してもらうためにセカンドソース契約を結んでいた\n    - ライセンス料の代わりに、設計データや製造に必要な情報を渡して、他社にも自社の CPU を製造できるようにする契約\n    - Intel は、8086/8088 では、AMD/米 Harris/独シーメンス/NEC/日立などとセカンドソース契約を結んでいた\n  - 初期の Intel は、セカンドソースを利用していたが、1985 年にその供給と止めると発表した\n  - そこから Intel 他者を敵に回し、特に AMD とは激しい訴訟合戦が始まる\n  - 1994 年に両者がクロスライセンス契約を結ぶことで合意した(2001 年に締結）\n    - クロスライセンスとは、両社が必要とする技術を含んだ特許を交換し、持ちあうというもの\n    - また AMD は、クロスライセンス契約の締結をただ待つのではなく、独自に x86 互換の CPU を製造するようになった\n  - このような経緯から、x86 を製造できるのは Intel と AMD のみとなっている\n  - 2009 年に AMD が製造部門を分離して「Global Foundries」を設立したため、Intel がこれを訴て、また一波乱あった\n- 参考\n  - [半導体業界における「IP」とは何なのかを説明したい](https://msyksphinz.hatenablog.com/entry/2020/10/04/040000)\n\n## 参考文献\n\n- [Intel: トランジスターの仕組み](https://www.intel.co.jp/content/www/jp/ja/innovation/transworks.html)\n- [なんちゃって電子工学](http://doku.bimyo.jp/electronics/ShoddyElectronics882.htm)\n- [コンピュータの基本構成と動作原理～知識編](https://qiita.com/zacky1972/items/ef4486e8a6d95edb68fd)\n- [FPGA 開発日記: Meltdown, Spectre で学ぶ高性能コンピュータアーキテクチャ](http://msyksphinz.hatenablog.com/entry/2018/01/06/020000)\n- [FPGA 開発日記: プロセッサにおけるアウトオブオーダの考え方について(リオーダバッファの考え方について)](http://msyksphinz.hatenablog.com/entry/2016/05/07/025243)\n- [FPGA 開発日記: プロセッサにおけるアウトオブオーダの考え方について(リネームレジスタの例外時の処理について)](http://msyksphinz.hatenablog.com/entry/2016/04/24/030607)\n- [CPU の脆弱性[Spectre], [Meltdown] は具体的にどのような仕組みで攻撃する？影響範囲は？](http://milestone-of-se.nesuke.com/nw-advanced/nw-security/meltdown-spectre/)\n","UpdatedAt":"2021-03-15T21:55:34.7392549+09:00"},{"Text":"# Debugging\n\n- [Debugging](https://linux-kernel-labs.github.io/refs/heads/master/lectures/debugging.html)\n- [uname コマンドから始めるデバッグ＆カーネルハック入門](https://kernhack.hatenablog.com/entry/2019/12/01/120907)\n  - strace, gdb, bpftrace などの簡単な使い方\n- [perf](http://www.brendangregg.com/perf.html)\n- [perf から読み解くプロセッサトレースの仕組み (perf + Intel PT/ARM CoreSight)](https://qiita.com/RKX1209/items/41758b6dcac6fb2fcee6)\n  - perf 内部の話で使い方ではないが、読み物としてはおすすめ\n\n## gdb\n\n## bpftrace\n\n## tracepoint\n","UpdatedAt":"2021-03-15T21:55:34.7402597+09:00"},{"Text":"# strace\n\n- strace いろいろ\n\n## Linux サーバでネットワーク I/O で刺さっている接続先を発見する\n\n- 問題対応のため、怪しいプロセスを strace してみる\n- read(2)や write(2)でブロックしていることを発見する\n- read(2)や write(2)、connect(2)の引数にはファイルディスクリプタ番号がみえる\n- プロセス ID とファイルディスクリプタ番号を使って、/proc//fd/ の中身をみると、ソケット I/O で刺さっている場合はソケット番号を発見できる\n- netstat からソケット番号で grep して接続先を発見する\n- /proc を直接見ずに、strace してから lsof -i -a -p \u003cpid\u003e などを使っているかもしれない\n\n```\n$ sudo strace -p 10471\nProcess 10471 attached - interrupt to quit\nread(58,  \u003cunfinished ...\u003e\nProcess 10471 detached\n\n$ sudo readlink /proc/10471/fd/58\nsocket:[1148032788]\n\n# IPアドレス 10.0.0.11 に対する3306番ポート(MySQL)の接続で詰まっていることがわかる。\n$ netstat -ane | grep 1148032788\ntcp        0      0 10.0.0.10:44566            10.0.0.11:3306           ESTABLISHED 48         1148032788\n```\n\n## 子プロセスも一緒に、トレースする\n\n```\nstrace -f [command]\n```\n\n## fd のファイルパスを表示する\n\n```\nstrace -y [command]\n```\n\n```\n$ strace -v -s 1024 -e trace=uname -C  uname -a\nuname({sysname=\"Linux\", nodename=\"owner-desktop\", release=\"5.4.0-48-generic\", version=\"#52~18.04.1-Ubuntu SMP Thu Sep 10 12:50:22 UTC 2020\", machine=\"x86_64\", domainname=\"(none)\"}) = 0\nuname({sysname=\"Linux\", nodename=\"owner-desktop\", release=\"5.4.0-48-generic\", version=\"#52~18.04.1-Ubuntu SMP Thu Sep 10 12:50:22 UTC 2020\", machine=\"x86_64\", domainname=\"(none)\"}) = 0\nuname({sysname=\"Linux\", nodename=\"owner-desktop\", release=\"5.4.0-48-generic\", version=\"#52~18.04.1-Ubuntu SMP Thu Sep 10 12:50:22 UTC 2020\", machine=\"x86_64\", domainname=\"(none)\"}) = 0\nLinux owner-desktop 5.4.0-48-generic #52~18.04.1-Ubuntu SMP Thu Sep 10 12:50:22 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\n+++ exited with 0 +++\n% time     seconds  usecs/call     calls    errors syscall\n------ ----------- ----------- --------- --------- ----------------\n  0.00    0.000000           0         3           uname\n  ------ ----------- ----------- --------- --------- ----------------\n  100.00    0.000000                     3           total\n```\n","UpdatedAt":"2021-03-15T21:55:34.7419544+09:00"},{"Text":"# デバイス\n\n## デバイスとは\n\n- デバイスの種類\n  - block-device\n  - character stream device\n  - network device\n  - 時計とタイマ\n- データ転送の種類\n  - programmed I/O(PIO)\n    - CPU を使ってデータ転送\n      - キャラクタデバイスなど、低遅延、少量のデータ転送に向いている\n  - Direct Memory Access(DMA)\n    - PC には DMA 補助回路が備わっており、CPU を使わずに、RAM と I/O デバイス間でデータ転送することができる\n      - ブロック、パケット、ストリーミングなど大容量のデータ転送に向いている\n    - 同期 DMA\n      - データ転送を開始するのは CPU\n      - 例: 音楽を出力中のサウンドカード\n        - サウンドカードの DSP(Digital Signal Processor)に対応するデバイスファイルを書き込む\n        - サウンドカードのデバイスドライバは、サンプルをカーネルバッファに蓄積する\n        - デバイスドライバは、決まったタイミングでサンプルをカーネルバッファから DSP に転送するようにサウンドカードに命令を出す\n        - サウンドカードはデータ転送が終了すると割り込みを発行する\n        - このとき、デバイスドライバは演奏しようとしているサンプルがカーネルバッファ内にまだ残っているかを確認し、残っている場合には、DMA データ転送をもう一度起動する\n    - 非同期 DMA\n      - データ転送を開始するのはデバイス\n      - 例: NIC のパケット受信\n        - NIC は I/O 共有メモリにフレームを保存すると、割り込みを発行する\n        - NIC のデバイスドライバは、割り込みを検知すると、I/O 共有メモリからカーネルバッファへフレームを転送するようネットワークカードに命令する\n        - データ転送が完了すると、ネットワークカードはもう一度割り込みを発行する\n        - デバイスドライバは 2 度目の割り込みを検知すると、カーネルの上位層へ新しいフレームが到着したことを通知する\n\n## CPU とデバイスのやり取り\n\n- デバイスコントローラとデバイスドライバ\n  - 各 I/O 機器はそれぞれデバイスコントローラを持ち、デバイスコントローラと CPU が接続されている\n    - 例外的にいくつかの機器が、ひとつのデバイスコントローラを共有する場合もある(例: SCSI)\n  - デバイスコントローラは、データ用と制御用のいくつかのレジスタを持つ\n  - 各デバイスコントローラに対して、OS はその I/O 機器用のデバイスドライバを持つ\n  - デバイスドライバは、デバイスコントローラのレジスタの読み書きを通して、機器を制御する\n- CPU は、番地(アドレス)を指定して、デバイスへのデータの読み書きする\n- アドレス空間は以下の 2 種類がある\n  - I/O(ポート)マップド I/O\n    - アドレス空間=I/O 空間\n    - 物理メモリ空間とは別のデバイスの I/O 空間に割り当てられたポートに対して、CPU の I/O 命令を実行してアクセスする\n    - ポート(port)とは論理的な通信の接続点\n  - メモリマップド I/O(現在主流)\n    - アドレス空間=メモリ空間\n    - I/O もメモリの一部として扱い、メモリ空間の番地を割り当てる\n    - 通常のメモリアクセス命令でアクセスする\n- CPU とデバイスは、制御信号線、データバス、アドレスバスで接続されている\n- メモリマップド I/O(あるプロセスが大量のデータを転送する場合の例)\n  - プロセスは転送用のシステムコールを呼ぶと、カーネルモードに移行し、システムコール中でデバイスドライバを開始する\n  - デバイスドライバはメモリ中の送信用メモリに転送データを書き、デバイスコントローラのコントロールレジスタに I/O 機器への転送開始命令を書き込んで、システムコール終了\n  - I/O 機器のデバイスコントローラは(計算機本体の CPU を使わずに)バスの利用権を獲得し、メモリからデバイスのバッファにデータを取り込む\n  - 終了後、デバイスドライバを(割り込みにて)起動(カーネルモードに移行)\n  - デバイスドライバは次のデータを送信用のメモリに上書きし、デバイスコントローラのコントロールレジスタに I/O 機器への転送開始命令を書き込みシステムコール終了\n  - これの繰り返し\n- I/O の終了のチェック方法\n  - 割り込み\n    - I/O 機器が終了を割り込みで CPU に知らせる\n    - 割り込み処理プログラムが起動\n    - 終了報告がある\n  - ポーリング\n    - CPU が I/O 機器の状態を定期的にチェックする\n    - I/O の状態 bit をチェックする\n    - 終了を問い合わせる\n\n* 以下のコマンドで、ioport のポートアドレスがわかる\n\n```\n$ cat /proc/ioports\n0000-0cf7 : PCI Bus 0000:00\n  0000-001f : dma1\n  0020-0021 : pic1\n  0040-0043 : timer0\n  0050-0053 : timer1\n  0060-0060 : keyboard\n  0064-0064 : keyboard\n  ...\n```\n\n- 以下のコマンドで、iomem のアドレスがわかる\n\n```\ncat /proc/iomem\n00000000-00000fff : reserved\n00001000-0009d7ff : System RAM\n0009d800-0009ffff : reserved\n000a0000-000bffff : PCI Bus 0000:00\n000c0000-000ce5ff : Video ROM\n000d0000-000d3fff : PCI Bus 0000:00\n000d4000-000d7fff : PCI Bus 0000:00\n000d8000-000dbfff : PCI Bus 0000:00\n000dc000-000dffff : PCI Bus 0000:00\n000e0000-000fffff : reserved\n```\n\n- lspci でそのデバイスの ioports、iomem、IRQ 番号などを確認できる\n\n```\n$ lspci -v\n...\n00:19.0 Ethernet controller: Intel Corporation Ethernet Connection (2) I218-V\n        DeviceName:  Onboard LAN\n        Subsystem: ASUSTeK Computer Inc. Ethernet Connection (2) I218-V\n        Flags: bus master, fast devsel, latency 0, IRQ 26\n        Memory at f7100000 (32-bit, non-prefetchable) [size=128K]\n        Memory at f7138000 (32-bit, non-prefetchable) [size=4K]\n        I/O ports at f040 [size=32]\n        Capabilities: \u003caccess denied\u003e\n        Kernel driver in use: e1000e\n        Kernel modules: e1000e\n```\n\n## デバイスドライバ\n\n- kernel はデバイスのメジャー番号、マイナー番号で必要なドライバを識別してる\n\n```\n# メジャー番号の確認\n$ cat /proc/devices\nCharacter devices:\n  1 mem\n  4 /dev/vc/0\n  4 tty\n  4 ttyS\n  5 /dev/tty\n  5 /dev/console\n  5 /dev/ptmx\n  5 ttyprintk\n  6 lp\n  7 vcs\n 10 misc\n 13 input\n...\n\n# マイナー番号の確認\n$ cat /proc/misc\ncat /proc/misc\n232 kvm\n235 autofs\n 56 memory_bandwidth\n 57 network_throughput\n 58 network_latency\n...\n```\n\n## ハードウェア情報を調べる\n\n```\n$ sudo lshw\nowner-all-series\n    description: Desktop Computer\n    product: All Series (All)\n    vendor: ASUS\n    version: System Version\n    serial: System Serial Number\n    width: 64 bits\n    capabilities: smbios-2.8 dmi-2.7 vsyscall32\n    configuration: administrator_password=disabled boot=normal chassis=desktop family=ASUS MB frontpanel_password=disabled keyboard_password=disabled power-on_password=disabled sku=All uuid=6000AC1B-DAD7-DD11-928F-08626634CC08\n  *-core\n       description: Motherboard\n       product: H97I-PLUS\n       vendor: ASUSTeK COMPUTER INC.\n       physical id: 0\n       version: Rev X.0x\n...\n```\n\n```\n$ sudo lshw -short\nsudo lshw -short\nH/W path         Device       Class          Description\n========================================================\n                              system         All Series (All)\n/0                            bus            H97I-PLUS\n/0/0                          memory         64KiB BIOS\n/0/3c                         memory         16GiB System Memory\n/0/3c/0                       memory         8GiB DIMM DDR3 Synchronous 1333 MHz (0.8 ns)\n/0/3c/1                       memory         8GiB DIMM DDR3 Synchronous 1333 MHz (0.8 ns)\n/0/47                         processor      Intel(R) Pentium(R) CPU G3258 @ 3.20GHz\n/0/47/48                      memory         128KiB L1 cache\n/0/47/49                      memory         512KiB L2 cache\n...\n```\n\n```\n$ sudo fdisk -l\nDisk /dev/ram0: 64 MiB, 67108864 bytes, 131072 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\n\n\nDisk /dev/ram1: 64 MiB, 67108864 bytes, 131072 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\n...\n```\n\n```\n$ sudo hdparm -i /dev/sda1\n\n/dev/sda1:\n\n Model=TOSHIBA THNSNJ256GCSU, FwRev=JURA0101, SerialNo=359S10DDT7SW\n Config={ Fixed }\n RawCHS=16383/16/63, TrkSize=0, SectSize=0, ECCbytes=0\n BuffType=unknown, BuffSize=unknown, MaxMultSect=16, MultSect=off\n CurCHS=16383/16/63, CurSects=16514064, LBA=yes, LBAsects=500118192\n IORDY=on/off, tPIO={min:120,w/IORDY:120}, tDMA={min:120,rec:120}\n PIO modes:  pio0 pio3 pio4\n DMA modes:  mdma0 mdma1 mdma2\n UDMA modes: udma0 udma1 udma2 udma3 udma4 *udma5\n AdvancedPM=yes: unknown setting WriteCache=enabled\n Drive conforms to: Unspecified:  ATA/ATAPI-3,4,5,6,7\n\n * signifies the current active mode\n```\n\n## smartctl\n\n- インストール\n\n```\n$ sudo yum install smartmontools\n```\n\n- デバイス情報を見る\n\n```\n$ sudo smartctl -i /dev/sda1\nsmartctl 6.5 2016-01-24 r4214 [x86_64-linux-4.4.0-59-generic] (local build)\nCopyright (C) 2002-16, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nDevice Model:     TOSHIBA THNSNJ256GCSU\nSerial Number:    359S10DDT7SW\nLU WWN Device Id: 5 00080d 9103488e2\nFirmware Version: JURA0101\nUser Capacity:    256,060,514,304 bytes [256 GB]\nSector Size:      512 bytes logical/physical\nRotation Rate:    Solid State Device\nForm Factor:      2.5 inches\nDevice is:        Not in smartctl database [for details use: -P showall]\nATA Version is:   ACS-2 (minor revision not indicated)\nSATA Version is:  SATA 3.1, 6.0 Gb/s (current: 6.0 Gb/s)\nLocal Time is:    Sat Jan 27 23:05:14 2018 JST\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\n```\n\n- デバイスのテストを行う\n\n```\n$ sudo /usr/sbin/smartctl -t short /dev/sda1\nsmartctl 6.5 2016-01-24 r4214 [x86_64-linux-4.4.0-59-generic] (local build)\nCopyright (C) 2002-16, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF OFFLINE IMMEDIATE AND SELF-TEST SECTION ===\nSending command: \"Execute SMART Short self-test routine immediately in off-line mode\".\nDrive command \"Execute SMART Short self-test routine immediately in off-line mode\" successful.\nTesting has begun.\nPlease wait 2 minutes for test to complete.\nTest will complete after Sat Jan 27 23:19:36 2018\n\nUse smartctl -X to abort test.\n\n\n$ sudo /usr/sbin/smartctl -l selftest /dev/sda\nsmartctl 6.5 2016-01-24 r4214 [x86_64-linux-4.4.0-59-generic] (local build)\nCopyright (C) 2002-16, Bruce Allen, Christian Franke, www.smartmontools.org\n\n\n$ sudo /usr/sbin/smartctl -l selftest /dev/sda\nsmartctl 6.5 2016-01-24 r4214 [x86_64-linux-4.4.0-59-generic] (local build)\nCopyright (C) 2002-16, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF READ SMART DATA SECTION ===\nSMART Self-test log structure revision number 1\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Short offline       Completed without error       00%      4758         -\n```\n","UpdatedAt":"2021-03-15T21:55:34.742957+09:00"},{"Text":"# ファイルシステム\n\n## ファイル\n\n- ファイル読み書きのチャネル\n  - ファイルを open するとファイルディスクリプタ（ファイル記述子）を得る\n- inode\n  - ファイルがディスク上のどこにあるかを示す\n  - その他のメタデータ\n    - サイズ、使用ブロック数、リンク数のカウント、アクセス権限、UID、GID、タイムスタンプなど\n  - inode はディスク上に保存されている(ファイルシステムによってはない場合もある)\n    - これをメモリにキャッシュして使ってる\n    - 基本的にはメモリ上にあるモノを inode と呼んでいる\n  - inode は一つのファイルシステムごとに一意になる\n  - inode は stat コマンドで確認できる\n    - 以下はその結果だが、ファイル名は実は inode には保存されてない\n    - ファイル名はディレクトリエントリで管理されている\n\n```\n$ stat README.md\n  File: 'README.md'\n  Size: 484             Blocks: 8          IO Block: 4096\nDevice: 801h/2049d      Inode: 1574959     Links: 1\nAccess: (0664/-rw-rw-r--)  Uid: ( 1000/   owner)   Gid: ( 1000/   owner)\nAccess: 2017-12-17 21:29:50.091897601 +0900\nModify: 2017-12-17 21:29:50.091897601 +0900\nChange: 2017-12-17 21:29:50.091897601 +0900\n Birth: -\n```\n\n- ファイルに関するシステムコール\n  - open\n  - read\n  - write\n  - see\n\n## ディレクトリとディレクトリエントリ(dentry)\n\n- ディレクトリとはファイル整理のためのデータベース、実態は dentry\n- dentry\n  - 実態はディスク上にあり、メモリにキャッシュして利用している\n  - ファイル名の情報を持っており、inode が紐づいている\n- パスの解決\n  - dentry をたどるとパスを解決できる\n  - 相対パスも絶対パスも開始地点が違うだけで、たどり方は同じ\n\n## ファイルディスクリプタとファイルオブジェクト\n\n- プロセスごとにファイルディスクリプタテーブルがつくられる\n  - ファイルディスクリプタテーブルとは、ファイルディスクリプタとファイルの対応表\n- 複数のファイルディスクリプタで同一のファイルを操作することも可能\n- ファイルディスクリプタ\n  - dentry やシーク位置やフラグを管理してる\n  - ファイルオブジェクトに紐づけられる\n  - ファイルオブジェクトの先にファイルシステムがある\n- ファイルディスクリプタと clone\n  - 子プロセスを生成すると\n    - ファイルディスクリプタテーブルだけコピーされる\n    - ファイルオブジェクトはコピーされない\n  - スレッドを生成すると\n    - (指定がなければ)ファイルディスクリプタテーブルも共有する\n- lsof\n  - プロセスが open してるファイルを表示する\n  - /proc/[pid]/fd を見てる\n- パイプ\n  - パイプを使うとファイルディスクリプタが 2 つできて間にリングバッファができて読み書きできる\n- ネットワーク（ソケット）通信\n  - ファイルディスクリプタ（ソケット)の先がネットワークのプロトコルスタックになってる\n  - ソケット経由でプロセスはネットワーク通信を行う\n\n## ブロックデバイス\n\n- ランダムアクセス可能なデバイスをブロックデバイスとして抽象化している\n  - ブロックの「アドレス」を指定して読み書きが可能\n- MAJ:MIN\n  - デバイスのメジャーバージョンとマイナーバージョン\n  - 必要なデバイスドライバは、このバージョンから特定される\n\n```\n$ lsblk\nNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsda     8:0    0 238.5G  0 disk\n sda1   8:1    0 222.6G  0 part /\n sda2   8:2    0     1K  0 part\n sda5   8:5    0  15.9G  0 part [SWAP]\n\n# -Sでscsi関連の情報が出せる\n$ lsblk -S\nNAME HCTL       TYPE VENDOR   MODEL             REV TRAN\nsda  0:0:0:0    disk ATA      TOSHIBA THNSNJ25 0101 sata\n```\n\n## ブロックデバイスとファイルシステム\n\n- ブロックデバイスを「ブロック」に分割\n  - 512 バイト、1024 バイト、4096 バイト(x86_64 はこれが限界)\n- ファイルとブロックの対応関係を管理する\n  - 対応関係自体もブロックデバイス上に記録する\n  - ファイルシステムごとに、これをどのように記録するかが異なる\n- ext2 の場合\n  - 最初にブロックデバイス上に inode の領域を確保する\n  - bitmap を持ち、各ブロックが使ってるかどうかを、true, faulse の bit のマップで管理している\n  - inode には、ブロックデバイスのどのブロックを参照するかが書かれている\n    - inode の参照テーブルが枯渇する場合は、間接参照用のテーブルが確保されてそこからブロックを参照する\n\n## ページキャッシュ\n\n- ページキャッシュを io request queue へ接続\n  - リクエストから一定時間たてばディスクに書く\n  - ページキャッシュの dirty が上がったら書き込む(dirty_raitio)\n  - ページキャッシュが一定量上がったら書き込む(dirty_bytes)\n  - dirty_raitio と dirty_bytes は排他\n    - dirty_raitio を書いたら dirty_bytes は 0 が書き込まれて無効になる\n    - dirty_bytes を書いたら dirty_raitio は 0 が書き込まれて無効になる\n  - io elevator(io scheduler)によって、並んだ書き込みやリクエストをマージする\n    - cfq, deadline, noop\n  - デバイスが高速なら、マージはあまり意味はないので、noop or deadline でよい\n- iostat -x 1\n  - rrqm, wreqm がマージされた数\n  - dd しながら見てみるとよい\n- キューの長さを変える\n  - echo 1024 \u003e /sys/block/sda/queue/max_sectors_kb\n- メモリが枯渇していて、キャッシュが確保できない時は、確保できるまでプロセスが止められる\n- mount option: async\n  - キャッシュに書いたら完了にして、非同期に書き込む\n  - ページキャッシュに乗る\n- mount option: sync\n  - 同期的に書き込む\n  - ユーザプロセス内のバッファとデバイスで直接データ転送する\n  - ページキャッシュに乗らない\n- aio write\n  - IO リクエストしたら完了\n  - そして、非同期に書き込んで、その結果(成功、失敗）をシグナルで受け取る\n- aio read\n  - IO リクエストしたら完了\n  - そして、非同期に読み込んで、その結果(成功、失敗）をシグナルで受け取る\n  - 通知時には、プロセスのバッファにデータが入っている\n\n## ディスクとアドレッシング\n\n- ハードディスクの構成要素\n  - インターフェイス\n    - SATA\n  - スピンドル\n    - 回転軸\n  - プラッタ\n    - 円盤状の記録版\n    - プラッタは同心円状に並ぶトラックに分割され、さらにトラックはセクタに分割される\n    - シリンダは、データが存在するトラックを表す\n  - ヘッド\n    - プラッタあたりに一つ存在し、読み書きをする\n- CHS(シリンダ・ヘッド・セクタ)\n  - ヘッドをシリンダ(データのあるトラック)に移動して、プラッタを回転してセクタ位置まで動かし、ヘッダが読み書きする\n  - 昔は CHS アドレッシングにより読み書きを行っていた\n- LBA(Logical Block Addresing)\n  - CHS に対して、一意のブロック番号をマッピングし、ブロック番号からセクタを特定できるようにする\n  - OS はこのブロック番号を使ってデバイスにアクセスする\n- 参考リンク\n  - [HDD の構造と消耗、高密度プラッタが復旧難易度を上げてしまう理由](https://pc.watch.impress.co.jp/docs/column/storage/1231038.html)\n\n## SSD とアドレッシング\n\n- SSD の構成要素\n  - インターフェイス\n    - SATA\n    - PCIe\n      - PCIe スロット(x4) or M.2 スロットで接続する\n    - プロトコル\n      - AHCI\n        - Advanced Host Controller Interface\n        - SATA SSD によって使用されるプロトコルと同じ\n        - PCIe 接続でもプロトコルは AHCI を利用してる場合もある\n      - NVMe\n        - Non Volatile-Memory Express\n          - SSD ストレージ用に設計されたプロコル\n          - PCIe NVMe SSD は、ドライブに独自の NVMe ストレージコントローラを組み込んでる\n        - CPU を介さずに PCIe バス経由で DMA(直接メモリにアクセス)を利用する\n        - 割り込みシステム\n          - 転送を保留するためのキューセットと完了ステータス用のキューセットを持つ循環キュー方式\n          - RDMA(リモートダイレクトメモリアクセス)をつかてこれらのコマンドキューを解析し、完了キューを応答ブロックで処理することで割り込みを効率的に集約\n      - 独自プロトコル\n        - OS に独自のドライバを入れて利用する独自プロトコルもある\n          - Fusion-io(今はないけど)\n  - SSD コントローラ\n    - FTL(Flash Translation Layer)\n      - OS は論理アドレスに対して、インターフェイスを通して、命令を送ってくるためこれを処理するレイヤ\n    - キャッシュメモリ\n      - 論理アドレスと物理アドレスのマッピングを管理している\n    - ウェアレベリング\n      - セルには書き換え回数に寿命があるため、記録エリアごとに書き換え回数を常に監視し、その情報を管理することで同じエリアばかりにデータの記録が集中しないように平準化する機能\n    - 不良ブロック管理\n      - エラーが発生し、読み書きできなくなった記憶領域を管理リストに登録し、二度と使用しないようにする機能\n    - エラー訂正機能\n      - NAND メモリに記録したデータの信頼性を確保する機能\n    - ガベージコレクション\n      - 保持すべきデータだけを別ブロックに書き直して、空きブロックを増やす処理のこと\n      - ページの断片化を防ぐ効果もある\n      - 通常は、読み出しや書き込みをしていない間にガベージコレクションを実行する\n      - また、NAND フラッシュの空きスペースを計算し、Spare Block をバッファ領域として管理してるので、通常ガベージコレクションが SSD のパフォーマンスに影響を与えることはない\n  - NAND メモリコントローラ\n    - サポートする並列アクセス数により性能が大きく変わってくる\n      - 16 チャンネル並列アクセスから、32 チャンネル並列アクセスになると理論上の速度は 2 倍になる\n  - NAND メモリチップ\n    - TSOP(Thin Small Outline Package)タイプの NAND メモリチップを搭載してる製品が多い\n    - 通常、コントローラの並列アクセス数と同じ数、またはその 2 倍の数のチップを実装している\n  - NAND メモリブロック\n    - NAND セルを束ねたもの\n    - NAND セル\n      - データを実際に格納するところ\n      - セルは、ControlGate(Gate)、Oxid、FloatingGate、TunnelOxid(トンネル障壁)、NPN(Source, GND, Drain)シリコンで構成される\n        - データの保持は FloatingGate で行い、Oxid、TunnelOxid で挟まれており、電化は外に流れない状態となっている\n        - Gate と Source に電圧をかけると、FloatingGate に電化が入り、Gate と Drain に電圧をかけると電化が抜ける\n        - セルには寿命があり、データの出し入れをすると、TunnelOxide が劣化するため、データをうまく保存できなくなる(FloatingGate から電化が抜けてしまう)\n        - 1 つのメモリセルに何 bit 記録するかでタイプ分けしている\n          - 1bit(Single), 2bit(Multi), 3bit(Triple), 4bit(Quad)\n        - メモリセルの容量、寿命、速度はトレードオフ\n          - 1 セルあたりの bit 数が多いほど、大容量化しやすい(コストに対して容量を稼げる)\n          - 1 セルあたり bit 数が多いほど、1bit あたりの書き込み回数の上限は低くなり、速度も落ちる\n          - 書き込み回数の上限\n            - SLC: 90000-100000\n            - MLC: 8000-10000\n            - TLC: 3000-5000\n            - QLC: 1000\n- アドレッシング\n  - 読み書きを「ページ」と呼ばれる単位で行う\n  - 消去は複数のページをまとめた「ブロック」という単位で行う\n- データの上書き\n  - NAND メモリではデータの上書きは行えない\n  - ページ内のデータを変更する処理(ブロックコピー)の流れ\n    - 空きブロックを用意しておく\n    - 書き込み先のブロックから書き換えるセクタ以外のデータをそのままコピーする\n    - 書き換えたいセクター（新しいデータ）を消去済みのブロックに書き込む\n    - 書き込んだブロックを元のブロックと入れ替える（アドレステーブル更新)\n    - 元のブロックを消去し、空きブロックにする\n    - 空きブロックがないと\n- fstrim\n  - xfs などの FS ではファイルを削除する場合、論理的に削除するだけ(OS 上で削除してるように見えるだけ)で SSD 上ではそのブロックは削除されない\n  - そこで、fstrim を実行することで、削除されたブロックをデバイスに通知することができる\n  - SSD 側は、削除されたブロックを知ることができるので、ガベージコレクションが効率よく行えるようになる\n  - fstrim の注意点\n    - 実行中に空き領域を探索する理由から、ロックをとるため、他のワークロードに影響がないかを確認しておくとよい\n  - 大量のファイルが頻繁に作成、削除されるようなワークロードの場合、SSD 側の物理ブロックも断片化しやすいため定期的に fstrim をしておくとよい\n  - 大きめのファイルを作成して、そこが頻繁に書き換えられるワークロードの場合は、断片化は起こりにくい\n  - 基本的に空き容量を使いつぶすような運用をしなければ、空きブロックが不足するということもないので、fstrim は必須というわけではない\n\n## アドレッシング\n\n- VFS がデータを読み込むときの例\n  - マッピングレイヤによって、データの物理的な位置を特定する\n    - ファイルのブロック長、データ範囲、ブロック番号を求める\n      - ファイルシステムのブロック長を求める\n      - ファイル内ブロック番号の形式で、要求を受けたデータの範囲を計算する\n      - ファイルは多くのブロックから形成されており、要求を受けたデータを含むブロック番号(ファイルの先頭からの相対位置)を求める\n    - ファイルシステムから論理ブロック番号を求める\n      - ファイルシステム固有の関数で、ファイルの i ノードにアクセスして、要求を受けたデータの位置を論理ブロック番号として求める\n      - 物理ディスクも多くのブロックから構成されており、要求を受けたデータを含むブロック番号(ディスクもしくはパーティションの先頭からの相対位置)を求める\n      - ファイルは物理ディスク上の隣接したブロックに格納されているとは限らないため、inode 内のデータ構造を用いて、ファイル内ブロック番号から論理ブロック番号への対応付を行っていく\n  - 汎用ブロック層により、要求を受けたデータを転送する I/O 操作を開始する\n    - データが隣接していない場合は、その分だけ I/O 処理を実行する\n  - IO スケジューラ層により、I/O 操作の並び替えや、マージが行われる\n  - デバイスドライバにより、I/O 操作が行われる\n    - デバイスドライバは、ディスクコントローラのハードウェアインターフェースに適切なコマンドを送ることで、データ転送を行う\n- ストレージアクセス用のパラメータ\n  - hw_sector_size, physical_block_size\n    - デバイスが動作できる最小の内部ユニット\n    - デバイスドライバはこのブロックの単位でデータ転送を行う\n  - logical_block_size\n    - デバイス上のアドレス指定で利用されるユニット\n  - minimum_io_size\n    - ランダムな I/O に対して推奨のデバイス最小ユニット\n  - optimal_io_size\n    - ストリーミング I/O に対して推奨のデバイスユニット\n  - alignment_offset\n    - 基礎となる物理的なアライメントのオフセットとなる Linux ブロックデバイス(パーティション/MD/LVM デバイス)の先頭部分のバイト数\n  - minimum_io_size, optimal_io_size は、RAID デバイスのチャンクサイズとストライプサイズに該当する\n\n```\n$ cat /sys/block/sdb/queue/hw_sector_size\n512\n\n$ cat /sys/block/sdb/queue/physical_block_size\n512\n\n$ cat /sys/block/sdb/queue/logical_block_size\n512\n\n$ cat /sys/block/sdb/queue/minimum_io_size\n512\n\n$ cat /sys/block/sdb/queue/optimal_io_size\n0\n\n$ cat /sys/block/sda/alignment_offset\n0\n```\n\n## loopback device\n\n- 一般的なファイルを、ブロックデバイスのように扱うための機能\n- イメージファイルなどを直接操作したい場合に使う\n  - 仮想イメージ qcow2 を row に変換して、loopback device にアタッチして、loopback device をマウントして、中身を操作するなどできる\n\n## devicemapper\n\n- ブロックデバイスドライバおよびそれをサポートするライブラリ群\n  - ブロック層の仮想化レイヤでありフレームワーク\n  - LVM2 でも利用されている\n- コピーオンライト、スナップショットなどが可能\n- device-mapper は仮想的なブロックデバイスを作成し、その仮想デバイスは受け取った I/O を処理する\n- 物理的なブロックデバイスが行うべき処理を完全に emulate する\n- 仮想デバイスは仮想デバイスの上に作成することも可能\n  - このような特性から stacked device とも呼ばれる\n  - さまざまなシンプルな機能を仮想デバイスとして stack することで、複雑な機能を持った仮想デバイスを実現することが可能\n\n## LVM\n\n- Logical Volume Manager\n- 複数のハード・ディスクやパーティションにまたがった記憶領域を一つの論理的なディスクとして扱うことのできるディスク管理機能\n\n- 仕組み\n\n```\n# 物理ボリューム（PV）を数十Mバイトの多数の小さな領域、物理エクステンド（PE）に分割する\n____________[PV]______________\n [PE][PE][PE][PE][PE][PE][PE]\n\n# PEをボリュームグループ（VG）に所属させる\n____________[VG]______________\n [PE][PE][PE][PE][PE][PE][PE]\n\n# 必要な分のPEを論理ボリューム（LV）に割り当てる\n# ボリュームグループを仮想的なディスクとするならば論理ボリュームは仮想的なパーティション(デバイス)と考えることができる\n_____[LV]_____  ______[LV]_______\n [PE][PE][PE]    [PE][PE][PE][PE]\n\n# LVをディレクトリにマウントして利用\n\n__[LV]__  __[LV]__\n /home      /var\n```\n\n- 利用手順\n\n```bash\n# パッケージインストール\n$ sudo yum install lvm2\n\n# LVMサービスをスタート\n$ service lvm2-lvmetad start\n\n# ボリューム領域を作成する\n# ddで空ファイルを作成する、もしくはパーティションを切ってもよい\n$ dd if=/dev/zero of=/tmp/test-volume bs=1 count=0 seek=10G\n\n# losetupでloopデバイスを作成したファイルと接続する\n# losetupは、loopデバイスを通常ファイルやブロックデバイスと接続・切断する\n# アタッチ\n$ losetup /dev/loop2 /tmp/test-volume\n\n# デタッチ\n# $ losetup -d /dev/loop2\n\n# 確認\n$ losetup /dev/loop2\n/dev/loop2: []: (/tmp/test-volume)\n\n# pvcreateでデバイスを初期化する\n# pvcreate は、物理ボリュームとして利用するブロックデバイスを初期化しPEに分割する\n# デバイスを初期化\n$ sudo pvcreate /dev/loop2\n  Physical volume \"/dev/loop2\" successfully created\n\n# 確認\n$ sudo pvscan\n  PV /dev/vda2    VG centos   lvm2 [31.51 GiB / 44.00 MiB free]\n  PV /dev/loop2               lvm2 [1.00 GiB]\n  Total: 2 [32.51 GiB] / in use: 1 [31.51 GiB] / in no VG: 1 [1.00 GiB]\n\n# lvmdiskscanで物理ボリュームとして利用できるブロックデバイスをスキャンできます\n$ sudo lvmdiskscan\n  /dev/centos/root [      28.27 GiB]\n  /dev/vda1        [     500.00 MiB]\n  /dev/centos/swap [       3.20 GiB]\n  /dev/loop2       [       1.00 GiB] LVM physical volume\n  /dev/vda2        [      31.51 GiB] LVM physical volume\n  2 disks\n  1 partition\n  0 LVM physical volume whole disks\n  2 LVM physical volumes\n\n# vgcreateでボリュームグループを作成\n$ sudo vgcreate volume00 /dev/loop2\n  Volume group \"volume00\" successfully created\n\n# ボリュームグループを設定ファイルに付け足す\n$ vim /etc/lvm/lvm.conf\n\u003e volume_list = [ ..., \"volume00\" ]\n\n# サービスをリスタート\n$ service lvm2-lvmetad restart\n\n# lvcreateでLVを作成する\n# LVを作成\n$ sudo lvcreate -L 50M -n lv01 volume00\n  Rounding up size to full physical extent 52.00 MiB\n  Logical volume \"lv01\" created.\n\n# 確認\n$ sudo lvdisplay volume00\n  --- Logical volume ---\n  LV Path                /dev/volume00/lv01\n  LV Name                lv01\n  VG Name                volume00\n  LV UUID                94LquW-xjcj-Ol35-5RQA-7QzZ-GV7O-6FbET3\n  LV Write Access        read/write\n  LV Creation host, time rabbit0, 2015-10-17 18:18:24 +0900\n  LV Status              available\n  # open                 0\n  LV Size                52.00 MiB\n  Current LE             13\n  Segments               1\n  Allocation             inherit\n  Read ahead sectors     auto\n  - currently set to     8192\n  Block device           253:2\n```\n\n- LV は通常のブロックデバイスと同様に利用できる\n\n```\n# フォーマット\n$ sudo mkfs -t xfs /dev/volume00/lv01\n[sudo] password for owner:\nmeta-data=/dev/volume00/lv01     isize=256    agcount=2, agsize=6656 blks\n         =                       sectsz=512   attr=2, projid32bit=1\n         =                       crc=0        finobt=0\ndata     =                       bsize=4096   blocks=13312, imaxpct=25\n         =                       sunit=0      swidth=0 blks\nnaming   =version 2              bsize=4096   ascii-ci=0 ftype=0\nlog      =internal log           bsize=4096   blocks=853, version=2\n         =                       sectsz=512   sunit=0 blks, lazy-count=1\nrealtime =none                   extsz=4096   blocks=0, rtextents=0\n\n# マウント\n$ sudo mount -t xfs /dev/volume00/lv01 /mnt\n\n# 書き込みテスト\n$ touch /mnt/helloworld\n$ ls /mnt\n```\n\n- 後片付け\n\n```bash\n# アンマウント\n$ sudo umount /mnt\n\n# LV, VG を削除\n$ sudo lvremove lv01 volume00\n  Volume group \"lv01\" not found\n  Cannot process volume group lv01\nDo you really want to remove active logical volume lv01? [y/n]: y\n  Logical volume \"lv01\" successfully removed\n\n$ sudo vgremove volume00\n\n# loopデバイスからファイルをデタッチ\n$ sudo losetup -d /dev/loop2\n\n# ファイルを削除する\n$ sudo rm /tmp/test-volume\n```\n\n## マウントしている FS を調べる\n\n- mount\n\n```\n$ mount\nsysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)\nproc on /proc type proc (rw,nosuid,nodev,noexec,relatime)\nudev on /dev type devtmpfs (rw,nosuid,relatime,size=8162844k,nr_inodes=2040711,mode=755)\ndevpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)\ntmpfs on /run type tmpfs (rw,nosuid,noexec,relatime,size=1637448k,mode=755)\n/dev/sda1 on / type ext4 (rw,relatime,errors=remount-ro,data=ordered)\n...\n```\n\n- df\n\n```\ndf -a\nFilesystem     1K-blocks      Used Available Use% Mounted on\nsysfs                  0         0         0    - /sys\nproc                   0         0         0    - /proc\nudev             8162844         0   8162844   0% /dev\ndevpts                 0         0         0    - /dev/pts\ntmpfs            1637448      9568   1627880   1% /run\n/dev/sda1      229613780 143513692  74413332  66% /\n...\n```\n\n## dumpe2fs\n\n- ext2/ext3/ext4\n\n```\n$ sudo dumpe2fs /dev/sda1 | less\nFilesystem volume name:   \u003cnone\u003e\nLast mounted on:          /\nFilesystem UUID:          c5a29305-9548-4405-a4d2-5687eda29d87\nFilesystem magic number:  0xEF53\nFilesystem revision #:    1 (dynamic)\nFilesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery extent flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isize\nFilesystem flags:         signed_directory_hash\nDefault mount options:    user_xattr acl\nFilesystem state:         clean\nErrors behavior:          Continue\nFilesystem OS type:       Linux\nInode count:              14589952\nBlock count:              58351872\nReserved block count:     2917593\nFree blocks:              21537480\nFree inodes:              10921380\nFirst block:              0\nBlock size:               4096\nFragment size:            4096\nReserved GDT blocks:      1010\nBlocks per group:         32768\nFragments per group:      32768\nInodes per group:         8192\nInode blocks per group:   512\nFlex block group size:    16\nFilesystem created:       Tue May  5 15:48:41 2015\nLast mount time:          Sun Feb 25 20:28:31 2018\nLast write time:          Sun Feb 25 20:28:30 2018\nMount count:              269\nMaximum mount count:      -1\nLast checked:             Tue May  5 15:48:41 2015\nCheck interval:           0 (\u003cnone\u003e)\nLifetime writes:          11 TB\nReserved blocks uid:      0 (user root)\nReserved blocks gid:      0 (group root)\nFirst inode:              11\nInode size:               256\nRequired extra isize:     28\nDesired extra isize:      28\nJournal inode:            8\nFirst orphan inode:       3686347\nDefault directory hash:   half_md4\nDirectory Hash Seed:      e8aebce0-015c-4d7c-acac-9ed0558f5452\nJournal backup:           inode blocks\nJournal features:         journal_incompat_revoke\nJournal size:             128M\nJournal length:           32768\nJournal sequence:         0x0048eda4\nJournal start:            8573\n```\n\n## パーティション\n\n```\n$ sudo parted -l\nModel: ATA TOSHIBA THNSNJ25 (scsi)\nDisk /dev/sda: 256GB\nSector size (logical/physical): 512B/512B\nPartition Table: msdos\nDisk Flags:\n\nNumber  Start   End    Size    Type      File system     Flags\n 1      1049kB  239GB  239GB   primary   ext4            boot\n 2      239GB   256GB  17.0GB  extended\n 5      239GB   256GB  17.0GB  logical   linux-swap(v1)\n\n$ mount\nsysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)\nproc on /proc type proc (rw,nosuid,nodev,noexec,relatime)\nudev on /dev type devtmpfs (rw,nosuid,relatime,size=3842644k,nr_inodes=960661,mode=755)\ndevpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)\ntmpfs on /run type tmpfs (rw,nosuid,noexec,relatime,size=1611440k,mode=755)\n/dev/sda1 on / type ext4 (rw,relatime,errors=remount-ro,data=ordered)\n...\n```\n\n## seq ファイルシステム\n\n- /proc などに代表される仮想ファイルのためのフレームワーク的なファイルシステム\n- seq ファイルシステムを利用した特殊なファイルに対して read システムコールを実行した際、通常はカーネル側で確保された PAGE_SIZE(基本は 4K)と同じ容量のバッファを経由して、read システムコールに指定されたユーザー空間側のバッファ領域にデータをコピーする\n- /proc/net/tcp などのデータ量が多くなりがちなファイルを読もうとすると read を何度も実行することになる\n  - strace /proc/net/tcp などするとその様子を見ることができる\n  - このため、ss コマンドなどはネットワークの情報を取るときには netlink を利用している\n- 参考\n  - [netstat コマンドを高速化する](https://qiita.com/mutz0623/items/7b000a6ac0f75df5dafd)\n  - [seq ファイルシステムについて](https://qiita.com/akachochin/items/98085494081b8bc39cbb)\n\n## nfs\n\n## iscsi\n\n## メモ\n\n- proc をマウントする\n  - mount -t proc abc /tmp/abc\n- hachoir tests\n  - バイナリファイルに対する wireshark\n- ioctl [ファイルディスクリプタ][デバイス]\n  - デバイスによって挙動がいろいろ\n  - kvm もこれで呼ばれる\n- 基本的に kernel 自体は io をすることはない（すべきでもない）\n  - 例外\n    - core dump ぐらい\n  - swap in, swap out\n  - モジュール読み込み、modprob はモジュールをメモリに読み込んでからカーネルに渡すので、カーネルは IO しない\n- ディスク性能を計測するときはキャッシュを消してから行う\n  - echo 1 \u003e /proc/sys/vm/drop_caches\n","UpdatedAt":"2021-03-15T21:55:34.7449565+09:00"},{"Text":"# iptables\n\n## netfilter とは\n\n- カーネルのパケット処理を hook してユーザランドで制御できるようにしたもの\n- ユーザ側で操作、参照しやすい形態\n- iptables もそのコンポーネントの一つ\n- conntrack, ulogd, ip6tables, arptables, ebtables 等も含まれる\n\n## conntrack\n\n- connection tracking の略ですべての通信がリアルタイムに記録される\n- TCP の通信が終了した場合でも、設定されたタイムアウトまで蓄積される\n- タイムアウトすると conntrack table から破棄される\n- src dst IP と Port の tuple を hash 化して hash table で処理している\n- conntrack table の確認\n  - conntrack table は多い時で 10 万近いリストになることもある\n  - cat /proc/net/nf_conntrack でも見れるが古いやり方(最新のカーネルだと、procfs 自体から消えてる\n  - conntrack-tools を使う\n  - $ yum install conntrack-tools, $ apt-get install conntrack\n  - \\$ conntrack -L\n  - [ASSURED]は破棄されず、[UNREPLIED]は timeout か上限数に達すると破棄される\n- 観測パターン\n  - [ASSURED] + SYN_SENT が増える(SYN_ACK が返ってこない)\n    - コネクション要求に失敗してる\n  - HALF-ASSURED が増える([ASSURED][unreplied]に遷移しない])\n    - コネクション要求に失敗してる\n  - SYN_SENT まで通る場合\n    - その先のロードバランサやリアルサーバの問題の可能性が多い\n  - HALF-ASSURED が増加する場合\n    - コネクションオープンまたは、クローズの要求に応答が戻ってこない状態\n    - NW 的に疎通不可能か、レスポンスが極端に低下している\n  - [ASSURED] + ESTABLISHED が増える（終了しない通信が増える)\n    - コネクションは確立しているがレスポンスがない\n  - CLOSE が増加する場合\n    - コネクションは確立したけど、レスポンスがないため開きっぱなしのコネクションが増加\n    - リアルサーバからレスポンスがなく通信が FIN へ遷移しないコネクションが増加している状態\n    - 逆 SYN flood 状態\n\n## iptables\n\n```\n$ iptables [-t テーブル] コマンド [マッチ][ターゲット/ジャンプ]\n```\n\n- テーブルとチェイン\n  - それぞれのテーブルの中で、どのタイミングでフィルタリングするかを決めるのがチェイン\n- テーブル\n  - filter テーブル\n    - INPUT、OUTPUT、FORWARD\n  - nat テーブル\n    - POSTROUTING、PREROUTING、OUTPUT\n  - mangle テーブル\n    - POSTROUTING、PREROUTING、INPUT、OUTPUT、FORWARD\n  - raw テーブル\n    - PREROUTING、OUTPUT\n- チェイン\n  - INPUT\n    - 入力（受信）に対するチェイン\n  - OUTPUT\n    - 出力（送信）に対するチェイン\n  - FORWARD\n    - フォアード（転送）に対するチェイン\n  - PREROUTING\n    - 受信時に宛先アドレスを変換するチェイン\n    - タイミングとしては filter で適用されるルールより手前\n  - POSTROUTING\n    - 送信時に送信元アドレスを変換するチェイン\n    - これも filter の後でパケットが送信される直前\n- パケットフロー\n  - http://inai.de/images/nf-packet-flow.png\n\n## コマンド\n\n- -A（--append） 指定チェインに 1 つ以上の新しいルールを追加\n- -D（--delete） 指定チェインから 1 つ以上のルールを削除\n- -P（--policy） 指定チェインのポリシーを指定したターゲットに設定\n- -N（--new-chain） 新しいユーザー定義チェインを作成\n- -X（--delete-chain） 指定ユーザー定義チェインを削除\n- -I（--insert） 指定したチェーンにルール番号を指定してルールを挿入する。（ルール番号を省略した際にはルール番号は 1 に設定され、チェーンの先頭に挿入される。）\n\n## パラメータ\n\n- -s (--source) パケットの送信元を指定。特定の IP（192.168.0.1）や範囲（192.168.0.0/24）を指定する\n- -d (--destination) パケットの宛先を指定。指定方法は-s と同じ。\n- -p (--protocol) チェックされるパケットのプロトコル。 指定できるプロトコルは、 tcp、udp、icmp、all のいずれか 1 つか、数値。\n- -i (--in-interface) パケットを受信することになるインターフェース名。eth0、eth1 など\n- -o (--out-interface) 送信先インターフェース名を指定\n- -j (--jump) [ターゲット]\n  - ターゲットの一覧\n    - ACCEPT: パケットの通過を許可\n    - DROP: パケットを破棄。応答を返さない。\n    - REJECT: パケットを拒否し、ICMP メッセージを返信\n    - REDIRECT: 特定ポートにリダイレクト\n    - LOG: マッチしたパケットのログを記録\n    - MASQUERADE: NAT 時のポートの変換\n\n## NAT（Network Address Translation)\n\n- NAT とは IP アドレスを変換して転送する仕組み\n  - IP Masquerde は、NAT に加えて UDP/TCP のポート番号の 変換まで行う\n  - NAT されてフォワードした通信は、conntrack に記録され、戻りの通信は再度変換されて送信元に戻る\n- DNAT: Destination IP を NAT する\n- SNAT: Source IP を NAT する\n\n```\n# ホスト上のローカルネットワークからNATして外部と通信する例\nsudo iptables -t nat -A POSTROUTING -p TCP -s 172.16.100.0/24 ! -d 172.16.100.0/24 -j MASQUERADE --to-ports 30000-40000\nsudo iptables -t nat -A POSTROUTING -p UDP -s 172.16.100.0/24 ! -d 172.16.100.0/24 -j MASQUERADE --to-ports 30000-40000\nsudo iptables -t nat -A POSTROUTING -s 172.16.100.0/24 -d 255.255.255.255 -j RETURN\n```\n\n## Note\n\n```\n# どのルールにマッチしたかを確認する\n$ sudo iptables -t nat -vL\n\nChain POSTROUTING (policy ACCEPT 6602 packets, 403K bytes)\n pkts bytes target     prot opt in     out     source               destination\n    0     0 MASQUERADE  all  --  any    !docker0  172.17.0.0/16        anywhere\n    9   540 MASQUERADE  tcp  --  any    any     192.168.100.0/24    !192.168.100.0/24     masq ports: 30000-40000\n   29  2112 MASQUERADE  udp  --  any    any     192.168.100.0/24    !192.168.100.0/24     masq ports: 30000-40000\n    0     0 MASQUERADE  all  --  any    any     192.168.100.0/24    !192.168.100.0/24\n```\n\n## References\n\n- [俺史上最強の iptables をさらす](http://qiita.com/suin/items/5c4e21fa284497782f71)\n","UpdatedAt":"2021-03-15T21:55:34.7459547+09:00"},{"Text":"# Kernel\n\n- perf_tools: http://www.brendangregg.com/Perf/linux_perf_tools_full.png\n- Kernel Map: http://makelinux.net/kernel_map/\n- Kernel Source(github tovals) : https://github.com/torvalds/linux\n- アーカイブ: https://www.kernel.org/\n- [linux カーネルで学ぶＣ言語のマクロ](http://qiita.com/satoru_takeuchi/items/3769a644f7113f2c8040)\n- [システムコールと Linux カーネルのソース](http://www.coins.tsukuba.ac.jp/~yas/coins/os2-2012/2012-12-04/)\n- https://kernelnewbies.org/LinuxVersions\n\n## ディレクトリ構造(kernel 4.10.8)\n\n- arch/\n  - アーキテクチャ固有のカーネルコードが含まれている\n  - 割り込み処理はほとんどすべてプロセッサ固有のものなので、この配下にある\n    - arch/x86/kernel/irq.c\n- include/\n  - カーネルコードをビルドするのに必要なインクルードファイルが大量に含まれている\n- init/\n  - カーネルの初期化コードが含まれている、カーネルの動作の仕組みはここから。\n- mm/\n  - メモリ管理(memory management)コードが含まれている。\n  - アーキテクチャ固有のメモリ管理コードは、arch/i386/mm/fault.c といった/arch/\\*/mm ディレクトリ以下にある\n  - ページフォルト処理コードは mm/memory.c\n  - メモリマッピングとページキャッシュのコードは mm/filemap.c\n- drivers/\n  - システム上のデバイスドライバ（device drivers)\n  - /block: ブロックデバイスドライバ\n  - /char: tty やシリアルポート、マウスなどのキャラクタベースのデバイスがある\n  - /cdrom: CD-ROM のコード\n  - /pci: PCI 仮想ドライバのコード(システム全体の定義は include/linux/pci.h にある)\n  - /scsi: SCSI のコード\n  - /net: ネットワークデバイスドライバが見つかる場所\n  - /sound: サウンドカードドライバが見つかる場所\n- ipc/\n  - カーネルのプロセス間通信(inter-process communications)に関するコードが含まれている\n  - 共有メモリは、ipc/shm.c\n  - セマフォは ipc/sem.c\n- fs/\n  - ファイルシステム(file system)\n  - /xfs, /ext4 などサポートするファイルシステムごとに分かれている\n- kernel/\n  - 主要なカーネルコードが置かれている。\n  - アーキテクチャ固有のカーネルコードは arch/\\*/kernel にある\n  - スケジューラは kernel/sched\n  - fork は kernel/fork.c\n- net/\n  - ネットワーク\n  - その大部分の include ファイルは include/net にある\n  - net/socker.c\n  - net/ipv4\n- block/\n- certs/\n- crypto/\n- firmware/\n- lib/\n  - カーネルのライブラリ\n  - アーキテクチャ固有のライブラリは、arch/\\*/lib で見つけることができる\n- samples/\n- scripts/\n  - カーネルを設定するときに使用されるスクリプト（awk, tk など）\n- security/\n- sound/\n- tools/\n- usr/\n- virt/\n  - kvm/\n  - lib/\n","UpdatedAt":"2021-03-15T21:55:34.7469544+09:00"},{"Text":"# カーネルの場所と起動時の処理\n\n## 目次\n\n| Link                                                                                                   | Description                                              |\n| ------------------------------------------------------------------------------------------------------ | -------------------------------------------------------- |\n| [カーネルの場所](#カーネルの場所)                                                                      | Centos7, Ubuntu16 の/boot の中身                         |\n| [ブートローダの仕組み](#ブートローダの仕組み)                                                          | ブートローダの仕組み                                     |\n| [カーネルモジュール](#カーネルモジュール)                                                              | カーネルモジュールについて、モジュールのロードアンロード |\n| [udev とデバイスの自動認識とモジュールの自動ロード](#udevとデバイスの自動認識とモジュールの自動ロード) | udev、デバイスの自動認識フロー                           |\n| [lspci](#lspci)                                                                                        | lspci の見方                                             |\n| [参考](#参考)                                                                                          | 参考                                                     |\n\n## カーネルの場所\n\n- Centos7 の/boot\n  - config-3.10.0-514.10.2.el7.x86_64\n    - Kernel ビルド時のオプション\n  - initramfs-3.10.0-514.10.2.el7.x86_64.img\n    - 起動時に使われる ram ファイルシステム\n    - initrd の代わりに使われる\n      - 単独でファイルシステムなので RAM ディスクが不要になった\n      - その初期値が cpio で与えられるようになった\n  - symvers-3.10.0-514.10.2.el7.x86_64.gz\n    - モジュールのビルド時に利用\n    - モジュールが組み込み先のカーネルとあったものかをチェックするために利用\n      - このチェックする仕組みを「Module versioning」と呼ぶ\n    - 仕組み\n      - 「シンボルの C 言語のプロトタイプを CRC ハッシュ計算したもの」をカーネルとモジュールのそれぞれに保存しておく\n        - シンボルの C 言語のプロトタイプとは、関数の返り値・引数のインターフェイス\n        - このインターフェイスで、モジュールが利用できるかを保証できる\n      - モジュールの読み込み時に両方が一致しないとモジュールの読み込みを拒否(disagrees about version of symbol [名前])する\n      - CRC ハッシュ、シンボル、モジュールを 1 行にまとめたもの「0x013246d0 snd_hdac_ext_bus_parse_capabilities sound/hda/ext/snd-hda-ext-core EXPORT_SYMBOL_GPL」を並べて記述したもの\n  - System.map-3.10.0-514.10.2.el7.x86_64\n    - メモリ上でシンボル名とアドレスの対応関係を示す\n      - シンボル名とは値もしくは関数名である場合が多い\n      - シンボル名のアドレスまたはアドレスの示すシンボル名が必要とされるケースにおいて要求される\n        - カーネルパニック\n        - Linux kernel oops\n          - Linux カーネルがエラーログを生成する、正常な動作からの逸脱状態のこと\n          - その状態に陥った際に発せられるメッセージを指す場合もある\n  - vmlinuz-3.10.0-514.10.2.el7.x86_64\n    - カーネル本体\n    - 圧縮されてる(z がついてる)\n  - initramfs-3.10.0-514.10.2.el7.x86_64kdump.img\n  - initramfs-0-rescue-df877a200226bc47d06f26dae0736ec9.img\n  - vmlinuz-0-rescue-df877a200226bc47d06f26dae0736ec9\n  - grub\n  - grub2\n- Ubuntu16 の/boot\n  - config-4.4.0-59-generic\n    - Centos と同上\n  - initrd.img-4.4.0-59-generic\n    - 起動時に使われる ram ディスク\n    - ファイルシステムではなくディスクなので、ここに fs イメージを入れてそれをマウントして使う\n  - System.map-4.4.0-59-generic\n    - Centos と同上\n  - vmlinuz-4.4.0-34-generic\n    - Centos と同上\n  - abi-4.4.0-59-generic\n  - grub/\n    - grub.cfg\n    - grubenv\n\n## ブートローダの仕組み\n\n- ファームウェア(bios or uefi)が grub をメモリにロードする\n- grub が/boot から vmlinuz と initramfs をメモリにロードする\n  - initramfs は初期起動時に利用する最低限のファイルシステム\n    - isci など最低限のドライバが入ってる\n    - initramfs の中のドライバモジュールを使ってルートファイルシステムが入ったディスクを読みこむ\n      - ルートファイルシステムがネットワーク上にある場合も、このような仕組みでロードできる\n    - 不要になったら、途中で本体のファイルシステムと入れ替えられる\n- カーネルの実行引数: /proc/cmdline\n  - BOOT_IMAGE=/boot/vmlinuz-4.4.0-59-generic root=UUID=c5a29305-9548-4405-a4d2-5687eda29d87 ro hugepagesz=1G hugepages=8 default_hugepagesz=1G quiet splash vt.handoff=7\n- 参考\n  - [initramfs について](https://qiita.com/akachochin/items/d38b538fcabf9ff80531)\n\n## カーネルモジュール\n\n- カーネルの機能をモジュールとして分離したもの\n- モジュールは、/lib/modules/に配置されてる\n- カーネルコンパイル時のオプションで標準で組み込こむか、組み込まないか、ユーザに有効・無効をゆだねるか、を指定できる\n  - 組み込むモジュールが多いと、その分ディスク・メモリリソースを使うので、明示的にモジュールを削っている\n- 実行中のロード・アンロードが可能\n  - insmod: ロード\n  - rmmod: アンロード\n  - modprobe: 依存関係を考慮したロード・アンロード\n  - lsmod: 現在利用中のモジュール一覧表示\n  - modinfo: 情報表示\n  - depmod: 依存関係情報の生成\n    - /lib/modules/[version]/modules.dep にモジュール間の依存関係が定義されている\n      - depmod により自動生成される\n    - 各モジュールの未解決のシンボル(関数や変数の名前)を追跡して、依存関係を検出する\n\n## udev とデバイスの自動認識とモジュールの自動ロード\n\n- udev(User Space Device Manager)\n  - カーネル 2.6 で導入されたデバイスマネージャー\n  - カーネルと連携してホットプラグ機能を提供する\n  - udevd というユーザ空間で動作するデーモンが、カーネルからの uevent というイベント通知を受けて、ルールファイルに従ってデバイス関連の処理をする\n    - udevadm monitor で uevent をリアルタイムで見れる\n- デバイスの自動認識フロー\n  - device manager がバスをスキャンしていて、デバイスを挿入すると、それを検知する\n  - ドライバのアタッチ、初期化、devtmpfs によりデバイスファイルが作成される\n  - ドライバコア部により、ソケット通信の一つである「netlink」プロトコルを使用して、uevent を生成し、udevd に通知する\n    - この devtmpfs によるデバイスファイルは、カーネルにより名前が決定される(dev/sda, dev/sdb のように)\n    - 任意のデバイスを永続的なデバイス名に紐図けておくには、udevd による名前変更が必要\n  - udevd は、デバイスの ID を検知して対応するデバイスモジュールを検索して、modprobe でモジュールをロードする\n    - modinfo の alias はデバイスドライバであることを意味しており、このデバイスに対応してると宣言してる\n    - 同じデバイスに対応したモジュールが複数あり競合するケースもある\n      - 不要なモジュールを blacklist に乗せてロードしないようにするなどの工夫が必要\n  - udevd は、uevnet 通知を受けると udev のルールファイル(udev rules)を参照し、その情報とカーネルが提供している sysfs にあるデバイス情報を照らし合わせて、次に実行する動作を決定する\n    - udev rules はデバイスの識別と処理をルールの形式で記述した設定ファイルのこと\n\n## lspci\n\n- PCI に接続されたカードの固有情報を取る\n- カードの固有情報を取る(lspci)\n\n```\n# そのままたたくと、人が見るように見やすくしたものが表示される\n$ lspci\n00:00.0 Host bridge: Intel Corporation 4th Gen Core Processor DRAM Controller (rev 06)\n00:01.0 PCI bridge: Intel Corporation Xeon E3-1200 v3/4th Gen Core Processor PCI Express x16 Controller (rev 06)\n00:02.0 VGA compatible controller: Intel Corporation Xeon E3-1200 v3/4th Gen Core Processor Integrated Graphics Controller (rev 06)\n00:03.0 Audio device: Intel Corporation Xeon E3-1200 v3/4th Gen Core Processor HD Audio Controller (rev 06)\n00:14.0 USB controller: Intel Corporation 9 Series Chipset Family USB xHCI Controller\n00:16.0 Communication controller: Intel Corporation 9 Series Chipset Family ME Interface #1\n00:19.0 Ethernet controller: Intel Corporation Ethernet Connection (2) I218-V\n00:1a.0 USB controller: Intel Corporation 9 Series Chipset Family USB EHCI Controller #2\n00:1b.0 Audio device: Intel Corporation 9 Series Chipset Family HD Audio Controller\n00:1d.0 USB controller: Intel Corporation 9 Series Chipset Family USB EHCI Controller #1\n00:1f.0 ISA bridge: Intel Corporation 9 Series Chipset Family H97 Controller\n00:1f.2 SATA controller: Intel Corporation 9 Series Chipset Family SATA Controller [AHCI Mode]\n00:1f.3 SMBus: Intel Corporation 9 Series Chipset Family SMBus Controller\n\n# プログラムが読むのは以下のようなデータ\n# バスのタイプ、ベンダID、デバイスIDが含まれてる\n$ lscpi -n\n00:00.0 0600: 8086:0c00 (rev 06)\n00:01.0 0604: 8086:0c01 (rev 06)\n00:02.0 0300: 8086:0402 (rev 06)\n00:03.0 0403: 8086:0c0c (rev 06)\n00:14.0 0c03: 8086:8cb1\n00:16.0 0780: 8086:8cba\n00:19.0 0200: 8086:15a1\n00:1a.0 0c03: 8086:8cad\n00:1b.0 0403: 8086:8ca0\n00:1d.0 0c03: 8086:8ca6\n00:1f.0 0601: 8086:8cc6\n00:1f.2 0106: 8086:8c82\n00:1f.3 0c05: 8086:8ca2\n```\n\n## 参考\n\n- ARM http://www.ujiya.net/linux/%5bARM%5d%20compressed%20kernel#fn-k61n1\n","UpdatedAt":"2021-03-15T21:55:34.7489531+09:00"},{"Text":"# Kernel build\n\n## 参考\n\n- [Ubuntu 16.04 でカーネルを再ビルドする](https://www.hiroom2.com/2016/05/18/ubuntu-16-04%E3%81%A7%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E3%82%92%E5%86%8D%E3%83%93%E3%83%AB%E3%83%89%E3%81%99%E3%82%8B/)\n- https://wiki.ubuntu.com/Kernel/BuildYourOwnKernel\n","UpdatedAt":"2021-03-15T21:55:34.7499606+09:00"},{"Text":"# カーネルデバイスドライバ\n\n```bash\n$ sudo yum install make gcc kernel kernel-devel kernel-headers\n\n$ sudo reboot\n\n$ rpm -qa | grep kernel\nkernel-3.10.0-327.el7.x86_64\nkernel-devel-3.10.0-514.6.1.el7.x86_64\nkernel-3.10.0-514.6.1.el7.x86_64\nkernel-tools-3.10.0-327.el7.x86_64\nkernel-headers-3.10.0-514.6.1.el7.x86_64\nkernel-tools-libs-3.10.0-327.el7.x86_64\n\n$ uname -r\n3.10.0-514.6.1.el7.x86_64\n```\n\nhello.c\n\n```\n/**\n * @file    hello.c\n * @author  Derek Molloy\n * @date    4 April 2015\n * @version 0.1\n * @brief  An introductory \"Hello World!\" loadable kernel module (LKM) that can display a message\n * in the /var/log/kern.log file when the module is loaded and removed. The module can accept an\n * argument when it is loaded -- the name, which appears in the kernel log files.\n * @see http://www.derekmolloy.ie/ for a full description and follow-up descriptions.\n*/\n\n#include \u003clinux/init.h\u003e             // Macros used to mark up functions e.g., __init __exit\n#include \u003clinux/module.h\u003e           // Core header for loading LKMs into the kernel\n#include \u003clinux/kernel.h\u003e           // Contains types, macros, functions for the kernel\n\nMODULE_LICENSE(\"GPL\");              ///\u003c The license type -- this affects runtime behavior\nMODULE_AUTHOR(\"Derek Molloy\");      ///\u003c The author -- visible when you use modinfo\nMODULE_DESCRIPTION(\"A simple Linux driver for the BBB.\");  ///\u003c The description -- see modinfo\nMODULE_VERSION(\"0.1\");              ///\u003c The version of the module\n\nstatic char *name = \"world\";        ///\u003c An example LKM argument -- default value is \"world\"\nmodule_param(name, charp, S_IRUGO); ///\u003c Param desc. charp = char ptr, S_IRUGO can be read/not changed\nMODULE_PARM_DESC(name, \"The name to display in /var/log/kern.log\");  ///\u003c parameter description\n\n/** @brief The LKM initialization function\n *  The static keyword restricts the visibility of the function to within this C file. The __init\n *  macro means that for a built-in driver (not a LKM) the function is only used at initialization\n *  time and that it can be discarded and its memory freed up after that point.\n *  @return returns 0 if successful\n */\nstatic int __init helloBBB_init(void){\n   printk(KERN_INFO \"EBB: Hello %s from the BBB LKM!\\n\", name);\n   return 0;\n}\n\n/** @brief The LKM cleanup function\n *  Similar to the initialization function, it is static. The __exit macro notifies that if this\n *  code is used for a built-in driver (not a LKM) that this function is not required.\n */\nstatic void __exit helloBBB_exit(void){\n   printk(KERN_INFO \"EBB: Goodbye %s from the BBB LKM!\\n\", name);\n}\n\n/** @brief A module must use the module_init() module_exit() macros from linux/init.h, which\n *  identify the initialization function at insertion time and the cleanup function (as\n *  listed above)\n */\nmodule_init(helloBBB_init);\nmodule_exit(helloBBB_exit);\n```\n\nMakefile\n\n```\nobj-m+=hello.o\n\nall:\n        make -C /lib/modules/$(shell uname -r)/build/ M=$(PWD) modules\nclean:\n        make -C /lib/modules/$(shell uname -r)/build/ M=$(PWD) clean\n```\n\n```\n$ ls\nMakefile  hello.c\n\n$ make\n\n$ ls\nMakefile  Module.symvers  hello.c  hello.ko  hello.mod.c  hello.mod.o  hello.o  modules.order\n\n# insmodコマンドはカレントディレクトリにあるドライバをロードするためのもの\n# modprobeコマンドは「/lib/modules/`uname -r`」からドライバを探してロードする\n$ sudo insmod hello.ko\n\n$ modinfo hello.ko\nfilename:       /home/fabric/helloworld/hello.ko\nversion:        0.1\ndescription:    A simple Linux driver for the BBB.\nauthor:         Derek Molloy\nlicense:        GPL\nrhelversion:    7.3\nsrcversion:     0DD9FE0DE42157F9221E608\ndepends:\nvermagic:       3.10.0-514.6.1.el7.x86_64 SMP mod_unload modversions\nparm:           name:The name to display in /var/log/kern.log (charp)\n\n\n# vermagic: ビルドした環境のカーネルバージョン SMP(マルチプロセッサ対応) mod_unload(ドライバアンロード可)\n# vermagic:       3.10.0-514.6.1.el7.x86_64 SMP mod_unload modversions\n\n$ lsmod | grep hello\n#       Size(メモリサイズ) Used(参照カウンタ)  by(依存ドライバ）\nhello   12535              0\n\n$ cat /proc/modules | grep hello\nhello 12535 0 - Live 0xffffffffa04e9000 (OE)\n\n$ ls /sys/module/ | grep hello\nhello\n\n$ ls /sys/module/hello\ncoresize  initsize   notes       refcnt       sections    taint   version\nholders   initstate  parameters  rhelversion  srcversion  uevent\n\n$ sudo rmmod hello.ko\n$ lsmod  | grep hello\n```\n\n# Refereces\n\n- [Writing a Linux Kernel Module ? Part 1: Introduction](http://derekmolloy.ie/writing-a-linux-kernel-module-part-1-introduction/)\n","UpdatedAt":"2021-03-15T21:55:34.7519666+09:00"},{"Text":"# カーネルパニック\n\n## Double Fault\n\n- 例外処理中に、何らかの理由により処理できない場合に発生する\n- CPU によって発生することが多いが、Kernel のバグによって発生することもある\n- 基本的にユーザ空間で発生するものではない\n- また、Double Fault の発生中に、何らかの理由により処理できない場合は Triple Fault が発生し、CPU が Shutdown する\n\n# nmi_watchdog\n\n- watchdog は、カーネルハング（カーネルストール）を検出し、カーネルハング時にカーネルパニックさせる。\n- 定期的に CPU に対して NMI 割り込み(NonMaskable Interapt)を発生させる。\n- カウンタ値を確認し、前回の値から変化がない場合にはをインクリメントする。\n- エラーカウンタが一定以上の値になった際、カーネルハングとみなしてパニックとなる\n\n## sysrq により明示的に誘発する\n\n```\ngoapp@ubuntu:/$ sudo sh -c 'echo c \u003e /proc/sysrq-trigger'\n[sudo] password for goapp:\n[33742.800945] sysrq: SysRq : This sysrq operation is disabled.\n\ngoapp@ubuntu:/$ sudo sysctl -w kernel.sysrq=1\nkernel.sysrq = 1\n\ngoapp@ubuntu:/$ sudo sh -c 'echo c \u003e /proc/sysrq-trigger'\n[33992.507658] sysrq: SysRq : Trigger a crash\n[33992.508433] BUG: unable to handle kernel NULL pointer dereference at 0000000000000000\n...\n```\n\n## Sysctl\n\n```\nkernel.hardlockup_panic = 0\nkernel.hung_task_panic = 0\nkernel.panic = 0\nkernel.panic_on_io_nmi = 0\nkernel.panic_on_oops = 0\nkernel.panic_on_rcu_stall = 0\nkernel.panic_on_unrecovered_nmi = 0\nkernel.panic_on_warn = 0\nkernel.panic_print = 0\nkernel.softlockup_panic = 0\nkernel.unknown_nmi_panic = 0\nvm.panic_on_oom = 0\n```\n\n## kdump\n\n- kdump は、クラッシュ時にメモリ内容をファイルに保存する機能\n- kdump を利用するには、システムカーネルとは別にダンプキャプチャカーネルが必要\n  - システムカーネルがクラッシュすると、通常はメモリの内容は失われる\n  - kdump からキックされる kexec によってダンプキャプチャカーネルを起動され、メモリの内容を保存する\n  - ダンプキャプチャカーネルはあらかじめシステムカーネルのメモリ領域に読み込ませておく必要がある\n\n### ubuntu\n\n- https://help.ubuntu.com/lts/serverguide/kernel-crash-dump.html\n\n```\n$ sudo apt-get install kdump-tools\n$ sudo grep crash /etc/default/grub.d/kdump-tools.cfg\nGRUB_CMDLINE_LINUX_DEFAULT=\"$GRUB_CMDLINE_LINUX_DEFAULT crashkernel=512M-:192M\"\n$ sudo update-grub\n$ sudo reboot\n$ cat /proc/cmdline\n... crashkernel=512M-:192M\n```\n\n- crashkernel=512M-:192M\n  - メモリが 512M-以上なら、192M を確保する\n  - そうでないなら、メモリを確保しない\n\n## crash によるデバッグ\n\n```\n\n\\$ crash .../vmcore\n\n```\n\n```\n\n```\n","UpdatedAt":"2021-03-15T21:55:34.7548221+09:00"},{"Text":"# メモリ\n\n## ページ/VSS/RSS/PageFault/デマンドページング/SegmentationFault\n\n- Linux では、メモリを「ページ」と呼ばれる単位で管理している\n- Linux では基本的に 4K バイトのページサイズを使用しており、4K バイト単位でメモリページをプロセスに割り当てている\n  - プロセスは、メモリが必要になると OS に mmap を発行し、仮想アドレス(VSS)の使い方（処理方法）を指定する\n  - それを受けて OS のメモリ管理機構(以降 MM)は新たな仮想アドレス域をプロセスに割り当てる\n  - この仮想アドレスにプロセスがアクセスすると、まだページが存在しない場合(初めてアクセスしたときなど)、PageFault という例外が発生する\n  - OS は、ユーザプログラムのアクセス先を調べて、mmap によって得た仮想アドレスの場合には先ほどの mmap の指定に従ってメモリを割り当てる\n  - この実際に割り当てられた物理アドレスが RSS となる\n  - このような実際にメモリーが必要になった際に動的にメモリーを割り当てる方法を「デマンドページング」という\n- SegmentationFault\n  - プロセスが仮想アドレスにアクセスしたとき、アクセス先を調べた際に、mmap によって得た仮想アドレスでなかった場合、SegmentationFault という例外が発生し、OS はプロセスにシグナル(SIGSEGV)を送信する\n  - このような状態は、OS 側では対応できないので、プロセス側にその後の処理を決めてもらう、たいていのプロセスはこれを受け取ると終了する\n- CPU キャッシュ\n  - CPU は、メモリにアクセスする前に、CPU キャッシュにアクセスする\n  - キャッシュがヒットすれば、そのまま読む\n  - キャッシュがヒットしなければ(ミス)、メモリから読みだして CPU に供給する\n  - また、次回のアクセス時に利用できるように読みだしたデータを CPU キャッシュ保持する\n  - キャッシュへのアクセスは物理アドレスでアクセスされる\n\n## 仮想アドレスと物理アドレスとページテーブルと TLB\n\n- ページテーブルは、仮想アドレスの「ページ」を物理アドレスに変換するためのテーブル\n  - 変換は、MMU(Memory Management Unit)が行う\n    - MMU は CPU の内部にある\n- 各プロセスには仮想アドレス空間とページテーブルが存在し、プロセスが実行されるときに、プロセスのページテーブルを CR3 レジスタにセットする\n- プロセスが仮想アドレスにアクセスすると、MMU は CR3 レジスタにセットされたページテーブルを使って、仮想アドレスを物理アドレスに変換して物理メモリにアクセスする\n- ページテーブルには、そのプロセスのメモリのほかに、kernel のメモリも乗っている\n  - ページテーブルには、割り当て情報以外にアクセス権の情報も入ってるので、kernel のメモリはシステムコールなどで kernel モードになっていなければアクセスできない\n  - KPTI(Kernel Page-Table Isolation)を有効にした場合は、プロセスのページテーブルと kernel のページテーブルは分離されるようになる\n- メモリに関するシステムコールとライブラリ\n  - システムコール\n    - execve(): メモリ中のプログラムを入れ替え、アドレス空間を作る\n    - mmap(): ファイルをメモリにマップする\n    - brk(), sbrk(): ヒープメモリを増やす\n    - mprotect(): メモリの保護モードを変更する\n    - mlock(): メモリをページアウトされないようにする(pinning)\n    - munlock(): mlock()で pinning した状態を解除する\n  - ライブラリ\n    - malloc(): ヒープメモリからメモリを割り当てる(ヒープは、mmap(), brk(), sbrk()で得る)\n    - free(): malloc()で割り当てたメモリを開放する\n  - その他\n    - スタックの自動拡張(許された範囲で(ulimit -s))、自動的にスタックを大きくする\n- プログラムは、起動すると仮想メモリ空間を作成し、そこに以下の情報をマッピングする(32bit の例)\n  - カーネル空間(0xffffffff-0xc0000000、1G を固定で高い番地から低い番地へ固定で割り当てられる)\n  - 引数、環境変数(カーネル空間に続いて、高い番地から低い番地へ固定で割り当てられる)\n  - スタック(引数、環境変数の空間に続いて、高い番地から低い番地へ割り当てられ拡張されていく)\n    - スタックポインタが指すところ\n  - ヒープ(BSS 空間に続いて、低い番地から高い番地へ割り当てられ拡張されていく)\n    - スタックとヒープのメモリ空間は連続していないので、互いに拡張できる\n  - BSS(データに続いて、低い番地から高い番地へ固定で割り当てられる)\n    - 初期値なしの静的変数(OS が勝手に初期化する)\n  - データ(プログラムファイルに続いて、低い番地から高い番地へ固定で割り当てられる)\n    - 初期値ありの静的変数\n  - プログラムファイル(0 番値付近を空けて、低い番地から高い番地へ固定で割り当てられる)\n    - 読み込み専用の機械語\n    - PC がこの先頭を指し、プログラムが実行される\n  - 0 番地(0x00000000)付近はメモリを割り当てないことが一般的\n    - 割り込みのエントリポイントになってたりする?\n- 仮想アドレス空間の大きさ\n  - 32 ビットシステムで、0x00000000-0xffffffff\n  - 64 ビットシステムで、0x00000000-0xffffffffffffffff (実際には、 もう少し小さい。0x00ffffffffffff くらい)\n- 仮想アドレスから物理アドレスへの変換の仕組み\n  - ページサイズを 4KB とする\n    - 仮想アドレスは、上位(セグメントセレクタ)と下位(オフセット)からなる\n      - 仮想アドレスを論理アドレスと言ったりもする\n    - セグメントセレクタは MMU がページテーブルを使って変換し、物理メモリのページを指定するアドレスとして使われる\n    - オフセット(1 ページの 4KB)は、変換されずにそのままアドレスを、物理メモリのページ内を指定するオフセットとして使われる\n    - 32 ビットシステムの場合は、\n      - 上位 20 ビットがページテーブルにより変換され、下位 12 ビットがそのまま使われる\n  - ページテーブルは、多段の変換テーブルになっている\n  - 32bit の場合\n    - 1 ページ(4KB)を 1024 のテーブルにしてこれを 1 エントリ(PTE: Page Table Entry)とし 4MB のエントリを作る\n    - PTE を 1024 のテーブル(PDT: Page Directory Table)として 4GB の仮想メモリ空間を扱えるようにしている\n  - 64bit の場合\n    - 32bit の PDT の上に、PDPT(Page Directory Pointer Table)、PML4(Page Map Level4 Table)が追加されて 256TB の仮想メモリ空間を扱えるようにしている\n    - PTE が 1 ページ(4KB)を 512 エントリで管理し(2MB)、PDT が PTE を 512 エントリ管理し(1GB)、PDPT が PDT を 512 エントリ管理し(512GB)、PML4 が PDPT を 512 エントリ管理する(256TB)\n  - ページテーブルを参照してページをたどることをページウォークと呼ぶ\n  - ページウォークしても物理ページが見つからない場合、ページフォールトを発生し、例外ハンドラが必要な処理を行う\n    - 通常は物理ページを割り当てる(デマンドページング)\n- TLB\n  - Translation Lookaside Buffer: アドレス変換バッファ\n    - 仮想アドレスと物理アドレスの変換をページテーブルでやっていては時間がかかるので、一度行ったアドレス変換をキャッシュしている\n    - TLB は、仮想アドレスと物理アドレスの 1 対 1 の変換表になっている\n  - MMU は、仮想アドレスを受け取ると、それに対応する物理アドレスがあるかを TLB を検索して、物理アドレスがあれば(TLB ヒット)、その物理アドレスでメモリにアクセスする\n  - 見つからない場合は(TLB ミス)、ページテーブルを参照してセグメントセレクタを変換し、その結果を TLB に保存する\n  - TLB はコンテキストスイッチの際などにクリアされる\n  - TLB はキャッシュと同様多層化されており、アクセス速度、容量が異なる\n    - Skylake\n      - L1\n        - ITLB: 4KB (128), 2MB/4MB (8/thread) の 2 種類\n        - DTLB: 4KB (64), 2MB/4MB (32), 1GB (4) の 3 種類\n      - L2\n        - STLB: 4KB と 2MB/4MB の共有 (1536), 1GB (16) の 2 種類\n- HugePage\n  - Linux では 4KB を超えるページを HugePage と呼ぶ(OS によって SuperPage, LargePage と呼ばれることもある)\n  - デフォルトのページサイズは 4KB だが、2M、1G バイトのページサイズも扱うことができる\n  - 大きいページサイズを扱うことで、ページウォークの回数が 2M なら 1 段、1G なら 2 段省略できる\n  - また、TLB のエントリ数も少なくなるため TLB のヒット率も上がる\n  - 参考\n    - https://gist.github.com/shino/5d9aac68e7ebf03d4962a4c07c503f7d\n    - https://access.redhat.com/documentation/ja-JP/Red_Hat_Enterprise_Linux/7/html/Virtualization_Tuning_and_Optimization_Guide/sect-Virtualization_Tuning_Optimization_Guide-Memory-Tuning.html\n- THP(Transparent Huge Page)\n  - アプリケーションに対してはページサイズを 4K バイトに見せつつ勝手に(x86-64 なら)2M バイトのページサイズのページテーブルに変換するもの\n  - THP の量は/proc/meminfo に記載されている\n    - AnonHugePages: 274432 kB\n  - ヒープやスタック領域などの Anon に割り当てられる\n  - THP は連続的に使用する論理アドレスがある場合に 2M バイトのページを割り当てるほか、ばらばらの 4K バイトのページが並んでる場合でも整理、並べ替えを行って 2M バイトのページサイズのページテーブルに変換する\n    - カーネルの khugepaged スレッドが実行時にメモリーを動的に割り当てる\n  - THP のメリット\n    - 2M のページとなるため、ページテーブルエントリが減り、TLB のヒット率も上がるため、アプリケーションのパフォーマンスが向上する\n  - THP のデメリット\n    - メモリ割り当て速度が 4K ではなく 2M になるため遅い\n    - 小さいメモリで十分な場合にも 2M の割り当てとなるため、メモリの無駄遣いとなる場合がある\n    - メモリをデフラグするため重い\n  - ワークロードによっては THP は無効のほうがよい\n    - Hadoop や Cassandra やデータベースなどでは無効にしているケースが多い\n  - HugePage を使えるなら THP はいらない\n    - HugePage を利用できるなら THP を無効にしても、ページテーブルエントリや TLB のメリットを受けることができる\n  - /sys/kernel/mm/transparent_hugepage/enabled\n    - always: 有効\n    - madvise: アプリケーションが明示的に要求した場合に THP を利用する\n    - never: 無効\n\n## file map と anon\n\n- VSZ, RSS に含まれるメモリーは、file map 域と anonymous 域に大別されます\n- file map 域はファイルの内容をメモリに張り付けた領域\n  - プログラムや共有ライブラリ、データファイルなどをメモリー張り付ける際に使用\n- anonymouse 域はファイルとは無関係の領域\n  - ヒープ(malloc で得られるメモリー）やスタックに使います\n\n## numa_map\n\n- numa_map によりプロセスのメモリーマップ状態を詳しく確認できる\n- cat 自体の numa_map を確認してみる\n  - file=... が file_map 域、そうでないのが anonymous 域\n  - mapped=, anon= はページフォルトの発生によりマップされたページ数を表している\n  - file_map なのに anon がカウントされてる個所は、ファイルの中身を anonymous 域にコピーしてから、マッピングしていることを表している\n    - ファイルを読むだけなら、ファイルをメモリにマップして読めばいい\n    - しかし、プライベートの書き込みを行う場合は、そのまま書き込むとファイルを変更してしまうので、いったん anonymous 域にコピーしてからコピー先に書き込む\n    - このような、「書き込み時にコピーする」ような仕組みを Copy On Write(COW)と呼ぶ\n      - COW の仕組みはメモリに限らずファイルシステムにおけるスナップショットや仮想マシンイメージ(qcow2 など)にも利用されている\n\n```bash\n$ cat /proc/self/numa_maps\n00400000 default file=/usr/bin/cat mapped=7 N0=7 kernelpagesize_kB=4\n0060b000 default file=/usr/bin/cat anon=1 dirty=1 N0=1 kernelpagesize_kB=4\n0060c000 default file=/usr/bin/cat anon=1 dirty=1 N0=1 kernelpagesize_kB=4\n0145f000 default heap anon=3 dirty=3 N0=3 kernelpagesize_kB=4\n7f25e17da000 default file=/usr/lib/locale/locale-archive mapped=12 mapmax=7 N0=12 kernelpagesize_kB=4\n7f25e7d03000 default file=/usr/lib64/libc-2.17.so mapped=83 mapmax=27 N0=83 kernelpagesize_kB=4\n7f25e7eb9000 default file=/usr/lib64/libc-2.17.so\n7f25e80b9000 default file=/usr/lib64/libc-2.17.so anon=4 dirty=4 N0=4 kernelpagesize_kB=4\n7f25e80bd000 default file=/usr/lib64/libc-2.17.so anon=2 dirty=2 N0=2 kernelpagesize_kB=4\n7f25e80bf000 default anon=3 dirty=3 N0=3 kernelpagesize_kB=4\n7f25e80c4000 default file=/usr/lib64/ld-2.17.so mapped=27 mapmax=26 N0=27 kernelpagesize_kB=4\n7f25e82da000 default anon=3 dirty=3 N0=3 kernelpagesize_kB=4\n7f25e82e2000 default anon=1 dirty=1 N0=1 kernelpagesize_kB=4\n7f25e82e3000 default file=/usr/lib64/ld-2.17.so anon=1 dirty=1 N0=1 kernelpagesize_kB=4\n7f25e82e4000 default file=/usr/lib64/ld-2.17.so anon=1 dirty=1 N0=1 kernelpagesize_kB=4\n7f25e82e5000 default anon=1 dirty=1 N0=1 kernelpagesize_kB=4\n7ffca3575000 default stack anon=3 dirty=3 N0=3 kernelpagesize_kB=4\n...\n```\n\n## reclaim 処理(メモリ回収)\n\n- アプリケーションやカーネルはメモリが必要になると、メモリの割り当てを MM に要求する\n- 未使用のメモリが十分にあれば、MM はそこからメモリを割り当てる\n- メモリが不十分な場合は、使用中のメモリを回収して割り当てようとする\n- メモリの回収とは使用中のメモリページを一つ一つ調べて、可能な場合にはそれを未使用の状態に戻す処理のこと\n  - これを reclaim(回収)処理という\n- MM は、2 種類の reclaim 処理を行う\n  - background reclaim\n    - バックグラウンドで動き、kswapd というカーネルデーモンが、この処理を実行する\n    - kswapd は「Low」(低)「High」という 2 つの閾値をもっていて未使用メモリーの量が Low を下回ると起動し、High を上回るまで reclaim 処理を続ける\n  - direct reclaim\n    - メモリーの割り当て要求の発行元に、セルフサービスでメモリー回収を行うように指示する\n    - そして、メモリーの割り当て要求を出したスレッドがそのまま、reclaim 処理を実施してメモリーを回収し、割り当てを試みます\n      - 当然アプリケーションは reclaim の処理時間分待たされることになるのでパフォーマンス低下の要因となる\n    - background reclaim では間に合わない場合があるので、用意されている\n\n## reclaim 処理(メモリ回収)の流れ\n\n- Linux では割り当て可能なページが足りなくなってくると recliaim 処理を実行する\n- reclaim 処理は利用中のページリストをスキャンし、メモリ回収が可能か否かの判断する\n  - MM がアクセス履歴を調査し、ページが\"最近\"使われたように見える場合には回収しない仕組みになっている\n  - メモリー獲得の勢いが強いと reclaim 処理の実施サイクルが詰まって判定が頻繁に行われるようになる\n  - その結果、\"最近\"という時間の長さがより短くなる\n- メモリー回収方法は、ページが file なのか、anon なのか、あるいはカーネル(slab)なのか、ページの内容によって異なる\n- ページが file の場合\n  - ファイルキャッシュとして使われている場合は、そのページをキャッシュの管理構造から外すことによってメモリを回収する\n  - 場合によっては、キャッシュの内容をハードディスク(HDD)に書き出す必要がある、その場合は IO 処理が終わるのを待ってから回収する\n- ページが anon の場合\n  - その内容を swap に書き込むことによって追い出す(swap in)\n  - 追い出した後にデータが必要になった際は、swap の内容を読みこむ(swap out)\n- ページが slab の場合\n  - slab と呼ばれるカーネルがメモリ上に作るオブジェクトの一部も回収の対象となる\n  - メモリ上のオブジェクトがどこからも参照されない状態にであれば回収する\n  - また、何らかのキャッシュとして動作しているオブジェクトは、各自のアルゴリズムによって管理されており、必要に応じて回収処理が動作する\n- 回収処理の流れ\n  1. anon は最初、Active リストに追加される\n  2. file は基本的には、最初は Inactive リストに追加される\n  3. reclaim 処理で Inactive リストがスキャンされた際、ページがまだ使われてそうなら Active リストにいれる、そうでなければ回収する\n  4. reclaim 処理で Active リストがスキャンされた際、ページがまだ使われそうならリスト末尾に戻し、そうでなければ Inactive リストに入れる\n- reclaim 処理が走ると遅くなる場合\n  - 回収可能なメモリがほとんどなかったり、メモリー回収時にディスクへの書き込みが必要となり処理に時間がかかったりすることもある\n  - reclaim 処理に時間がかかると、メモリ獲得処理で遅延が発生するのみならず、kswapd によって CPU 負荷も増大する\n  - このような状態を「メモリ不足」と呼ぶことが多い\n- reclaim 処理から除外されるページ\n  - 以下のページは Unevictable リストに追加され、reclaim 処理の対象外となる\n    - ramfs が所有するページ\n    - 共有メモリロックのメモリリージョンにマッピングされているページ\n    - VM_LOCKED フラグがセットされたメモリリージョンにマッピングされているページ\n  - また、mlock()というシステムコールでメモリーを固定するできる\n    - これは Mlocked リストに追加される\n\n## メモリ不足を確認したり、予兆を把握するにはどうすればよいか？\n\n- /proc/meminfo で状況を把握\n  - MemTotal: 実際に搭載されている DIMM の容量より少し少ない値になる(BIOS が使用する分や、メモリー管理が起動する前に使われた分など、OS のメモリ管理下にないメモリーが存在するため)\n  - MemFree: 未使用メモリ量、kswapd はこの量を監視して動作する\n  - Buffers: 一時的に I/O と紐づいている量\n  - Cached: ファイルキャッシュ量(メモリー不足時に swap に追い出すべき tmpfs などのページも含まれている)\n  - SwapCached: スワップのキャッシュ\n    - Inactive(anon)、Active(anon)\n  - Active(anon) + Inactive(anon): anon のメモリ量、追い出し先は swap\n    - swap free がないと追い出せないので注意\n  - Active(file) + Inactive(file): file のメモリ量、追い出し先は swap 以外\n  - Slab: slab の合計\n    - カーネルメモリのうち slab という定型のメモリアロケータを使うもの\n    - Active, Inactive には合算されないので、これはこれで見る必要がある\n  - SReclaimable: slab の中で、回収可能なメモリ\n  - SUnreclaim: slab の中で、回収不可なメモリ\n  - CommitLimit: プロセスが確保できるメモリの制限値\n  - Commited_AS: プロセスが割り当て要求の総量(使用量ではない)\n- メモリ不足かどうかの判断基準\n  - swap が利用されていても in/out が頻発していない場合は問題ない\n    - anon を swap 域に追い出してもアクセスが生じなければ、性能への影響はない\n    - swap in/out が頻発する場合は、性能が大きく低下するのでこれは避けなければならない\n  - ファイルキャッシュについても捨てると性能が大きく低下する場合もあるし、そうでないこともある\n  - この辺は、ケースバイケースだが、基本的には anon は考慮せずにファイルキャッシュ分を空きメモリと捉えてよい\n    - 体感的な空きメモリ = MemFree + (Inactive(file) + Active(file) + Sreclaimable) \\* 0.7-0.9(係数はシステムに依存)\n  - 単純なメモリー使用量で判断できない場合は、直近のメモリ量以外の統計情報（IO 量やアプリケーションのレイテンシ、kswapd の稼働時間など）を加味して判断するとよい\n\n## メモリの自動監視\n\n- メモリの自動監視向けのインターフェイス[vmpressure]がカーネル 3.10 以降含まれている\n- reclaim 処理が回収した量/reclaim 処理がスキャンした量の比を計算し、直近のメモリー回収にかかったコストがどの程度なのかを監視プログラム（デーモン)に通知する\n- この数値が下がるとメモリの使用状況が悪化したと判断し、カーネルから監視デーモンに通知を行います。\n- 通知を受けた監視デーモンはアプリケーションにメモリを解散させたり、強制終了させたり、ネットワークデータのキャッシュを破棄したりして、未使用メモリ領域を増やそうとします\n- 仮想マシンと連携してバルーニングする（ある仮想マシンからメモリを強制的に解放する)ことも考えられる\n\n## ライトバック\n\n- ファイルキャッシュについては、ディスクに書き戻してから回収する\n- このディスクに書き戻す処理をライトバックと呼ぶ\n- ライトバックのアルゴリズムや設定がアプリケーション性能に大きく影響する\n- ファイルキャッシュとライトバック\n  - データを CPU の近く（高速にアクセスできる場所）に置くことを「キャッシュする」という\n  - キャッシュにデータがあると read()はディスクアクセスを回避できる\n  - 書き込み時、write()はキャッシュに書き、キャッシュからディスクへ適宜ライトバックされる\n    - キャッシュ上のデータを改変した場合、それをストレージに反映させる処理が必要になる（これをライトバックと呼ぶ）\n    - キャッシュ上で改変されたためにストレージにライトバックする必要があるものを「ダーティーなキャッシュ」と呼ぶ\n    - ライトバックしなくてよいものを「クリーンなキャッシュ」と呼ぶ\n- Linux のライトバックでは、プロセスによるキャッシュへの書き込み速度とカーネルによるストレージへの書き込み速度を観測し、その結果をもとにプロセスによるキャッシュへの書き込み速度やライトバック頻度を調整している\n  - 書き込み速度を調整するわけ\n    - 空きメモリーができるだけキャッシュとして有効に使われつつ、かつメモリー獲得処理の邪魔にならないように、うまくコントロールする必要がある\n    - メモリ獲得時に未使用のメモリが少ないと、ファイルキャッシュの追い出しが必要になることがある\n    - そして、キャッシュの追い出しをするには、キャッシュの情報がストレージに反映されている状態（クリーンな状態）であることが必要\n- ライトバックの問題点\n  - ライトバック自体の処理にメモリが必要なこと\n  - ストレージへの I/O 発行にメモリーが必要、NFS などではネットワーク通信にメモリが必要\n  - クリーンなページ、つまりすぐに Reclaim 処理ができるページを安定した量確保しておくことは、メモリ獲得処理が安定して動作することにつながる\n  - メモリがダーティなものばかりになると、I/O を特に行っていないプロセスまで、I/O を発行しているプロセスの影響を受けがちになる\n    - このような状態になると、I/O をしていないプロセスでもメモリ獲得時の Reclaim 処理でライトバックのための I/O 待ちで遅延することがある\n- ライトバック速度を調整するための設定値\n  - background_dirty_raito\n    - ライトバックを行うカーネルスレッド「flusher」を起動するしきい値\n    - システムの「回収可能なメモリー」に対して、ダーティーなキャッシュの比率が background_dirty_raito を超えた場合にカーネルスレッドを起動する\n    - カーネルスレッドの起動タイミングは、この数値が小さいと早めに、大きいと遅めになる\n  - dirty_raito\n    - システム上のダーティメモリ量を制限する閾値\n    - 回収可能なメモリに対して、ダーティなキャッシュの比率がこの値を超えないように、write()を行うプログラムの速度を調整する\n    - この dirty raito の挙動はカーネル 3.2 から大きく変更された\n      - カーネル 3.1 以前の動作は write()発行時にダーティ量と dirty_raito を比較し、dirty_raito を上回っている場合は write()をしばらく止めて寝かせる\n        - 一定のところまでは好きなだけ書かせておいて一定値まで到達したらいきなり（ひょっとしたら長時間）寝ることになる\n      - 3.2 以降は、ダーティの量が増加するにしたがってこまめに wait を入れ、write()の速度をコントロールするようになっている\n        - これにより従来よりも write0 を行うアプリケーションが「長く寝る」ことが少なくなっている\n      - 3.1 以前は閾値にぶつかったところで、メモリ管理から直接ライトバック処理を呼び出す処理が動作し、これが効率的な I/O を阻害する要因になっていた\n      - 3.2 以降は、この処理が取り除かれて、I/O は基本的に flusher スレッドが行うため、従来よりも効率的に I/O を行えるようになった\n    - タスクごとにコントロール\n      - dirty raito にはタスクごとのコントロールやブロックデバイスごとの設定項目もある\n        - /var/log のあるブロックデバイスの dirty raito 制限を緩和し、遅くて dirty がたまりがちな USB ドライブの dirty raito を下げるといった設定が可能\n        - これらのチューニングでダーティ・ライトバック量を制御することでメモリ獲得コストや一度に出る I/O の量を下げてシステムの安定性を高められる可能性がある\n\n## アウトオブメモリー\n\n- メモリーを十分に回収できず、MM が要求されたメモリを割り当てられないような状態を OOM(Out Of Memory)と呼ぶ\n  - reclaim 処理で解放できるメモリ容量には限界がある\n- メモリを十分に回収できない状態\n  - Anon ばかりで追い出せない\n    - スワップデバイスがないシステムは、Anon をスワップに追い出せない\n      - 明示的にスワップを作らない場合はよくある\n    - このような場合、回収可能なメモリはファイルキャッシュに限られる\n    - つまりユーザプログラムが大容量のメモリ割り当てを要求すると、ファイルキャッシュを追い出し切った時点で回収可能なメモリがなくなり、OOM に陥る\n    - スワップデバイスがあっても、スワップ領域を使い切ってしまうと、OOM になることがある\n  - Anon に加えてファイルやカーネルが相当量ある場合でも、I/O 待ちが長く、ファイルキャッシュがダーティな状態のページばかりになると OOM が発生することがある\n    - dirty_raito を低くすると OOM が生じにくくなるので、性能要件と空きメモリを見ながら適切にチューニングするとよい\n  - NUMA による OOM\n    - 最適化の観点から「特定のノードからのみメモリーを割り当てる」という設定を行っていると、(他のノードでメモリが余っていても)ノードのメモリを割り当てられなくて OOM が生じる\n  - mlock や共有メモリのロックによってメモリを大量に固定したために、メモリを回収できなくなって OOM\n  - ブートパラメータや sysctl による hugetlbfs の容量設定のミスによって大量のメモリが Hugetlbfs に張り付いてしまい OOM\n  - fork()を連発してメモリのほとんどがカーネルメモリとして消費されてしまい OOM\n  - 共有メモリーを数千ものプロセスから共有したためにページテーブルが多くなって OOM\n- OOM 状態に陥った際にプロセスを自動的に削除(Kill)してメモリを強制的に開放する仕組みを OOM Killer と呼ぶ\n  - 各プロセスのスコアを計算し、最も高いプロセスを選ぶ。\n  - スコアは、メモリ使用量（物理メモリー使用量+ページテーブルサイズ+スワップ使用量)が高いと高くなり、\n  - これに、係数(oom_score_adj)がついて最終的なスコアとなる\n  - oom_score_adj を-1000 に設定しておくと、絶対に kill されなくなる\n    - sshd などのデーモンの中には自分でこの値を設定するものがある\n- OOM killer ではどうにもならない場合\n  - 例えば、fork()の連発で OOM になるケースは kill したとたんに新しいプロセスが fork()されたり、共有メモリのページテーブルに絡むトラブルの場合はプロセスを多少 Kill しても焼け石に水\n- OOM Killer の強制開放後の状況予測が難しいことから、クラスタを組んでる場合には、OOM 発生時に OOM Killer をどうさせせずにカーネルパニックを起こすように設定するケースもある\n  - 「vm.panic_on_oom」の値を「1」にする\n  - OOM Killer で必要なアプリケーションがまともに動作しない状況になる可能性があるなら、システムをカーネルパニックで落としてフェイルオーバーさせたほうがよい\n\n## ダイレクト I/O と COW\n\n- プロセスがファイルを読み込む際に、ディスクから読みだしたデータが OS によってシステムメモリにキャッシュされる\n- しかし、データベースなどのようなディスクへデータ記憶を主眼に置いたアプリケーションの場合は、自らデータのキャッシュ管理を行うので、OS のキャッシュ機構を利用しない場合もある\n  - アプリケーションのメモリとディスクの間で直接データをやり取りする「ダイレクト IO」という仕組みが用意されています\n- ダイレクト I/O の処理の流れ\n  - read()はファイルキャッシュにデータがあるかどうかを確認し、もし存在するならキャッシュのデータをユーザのバッファにコピーする\n  - キャッシュに存在しない場合、OS がファイルシステムを経由してその下のブロックデバイスからデータを獲得する\n  - そして、そのデータをファイルキャッシュに入れた後に、そこからユーザーバッファにコピーする\n  - ダイレクト I/O の場合、read()ではファイルキャッシュにデータがあるかを確認しない\n  - 単に、ファイルシステムやデバイスにユーザバッファの場所を指定して、そこにデータを読み込む\n  - このデータ転送は、基本的には OS がデバイスにデータ転送先の物理アドレスを教えることによって、行われる\n  - データ転送がおわるまで、OS はデータ転送先の Anon メモリが解放されたり、スワップアウトされたりしないよう、メモリ監視をする\n- COW\n  - プロセスを複製する fork()で使われている\n  - fork()が実行されると、OS はスレッドや利用しているメモリー、オープン中のファイル情報などを複製し、新たなプロセスを生成する\n  - 複製なのでメモリも複製するわけだが、単純にコピーすると fork()の処理時間が長くなるし、fork()したあと、execve()によって別プログラムを起動する場合にはメモリーをすべて捨てることになるので、全コピーは無駄となる\n  - メモリーの実態はコピーせずに、fork()時にはページテーブルのみをコピーする\n  - そして、複数されたプロセス間でメモリを共有する\n  - その後、どちらかのプロセスからメモリへのデータ書き込みが発生した時点でメモリの内容を実際にコピーし、ページテーブルが指す物理アドレスを分離する\n  - この書き込みの発生を捕まえるために、fork()時にページテーブルをコピーする際、親プロセスと子プロセスともに Anon メモリーをリードオンリーでマップする\n    - リードオンリーのメモリに対して書き込みが発生すると、CPU からイベントが通知される\n    - OS はその通知を受け、メモリをコピーします\n  - その後、親プロセスと子プロセスともに Anon メモリーを今度はリードライトでマップする\n  - つまり、fork()時はメモリーをコピーせずに、必要になったときにはじめてページ単位でコピーされる\n- ダイレクト IO と COW\n  - 流れ\n    1. プロセスがダイレクト I/O の read を発行、それを受けてカーネルがブロックデバイス層に対してデータ転送の実施を依頼\n    2. プロセスが fork を発行\n    3. 親プロセスが何らかの理由で、(1)で read を出していたバッファと同じページに書き込みを実施し、それに伴い、COW でメモリがコピーされる\n    4. データ転送が完了\n  - 3.で親プロセスが書き込みを実施したことにより、新たなページが割り当てられる\n  - つまり、1 で read()のデータ転送先になっていたページとは、物理アドレスが異なるページが 3 において親プロセスに割り当てられる\n  - 1 で生じたデータ転送は古いページに対して行われるので、子プロセス側のバッファに転送されて、親プロセスは read()が終わったと思ってバッファを見に行っても正しいデータは転送されない\n  - つまり、ダイレクト I/O をしている間に fork をする場合にはデータの整合性が保証されなくなるので注意が必要\n  - ダイレクト IO でない場合は、この問題は発生しない\n  - プロセスの論理アドレスを利用してデータがコピーされるため、親プロセスの新バッファに正しく読み込まれる\n\n## メモリマイグレーション\n\n- メモリのマッピングは、一度作られた後にずっと固定された状態にあり続けるとは限らない\n- 主に次の 2 つの場合に、最初のマッピング情報を破棄して、仮想アドレスと物理アドレスのマッピングを再構築する\n  - メモリーをスワップに送る\n    - OS のメモリ不足を解消するために、最近使用されていないメモリを回収する\n  - ページを移動する\n    - 連続メモリーを作る\n      - 処理によっては 4K のページでは足りず、連続したページの物理メモリが必要になるため、メモリの中身を移動して整理し、連続したメモリ域を作り出す\n    - メモリを CPU の近くに置く\n      - NUMA 構成の場合、各 CPU の下にメモリ(DIMM)が接続されるため、メモリまでの距離が一定ではない\n    - NUMA 構成の場合プロセスの CPU とメモリを近くに配置することによって性能が高まる\n- ページテーブルのエントリ\n  - ページテーブルの各エントリには各種情報が含まれている\n  - ページが存在するかを示すビット(Present Bit)、ページのマッピング属性を示す数ビットの情報、ページの部地理アドレスを示す数値\n  - 64 ビットアーキテクチャの場合、物理アドレスは 64 ビットです\n  - ページサイズは 4K バイトなので、マッピング情報も 4K バイト単位になる、\n  - エントリの下位 12 ビットをページの情報に、上位 52 ビットをページの位置情報に使用する\n  - present bit = 1 の場合はページが存在するので、CPU アーキテクチャごとに決められたフォーマットで、ページの物理アドレスと属性情報を記載する\n    - 一方、「present bit = 0」は、ページが存在しないことを意味する\n    - その場合は残り 63 ビットを自由に使ってよい\n  - スワップ情報を示すスワップエントリでは、present bit = 0 となり、残りの 63 ビットにスワップタイプとスワップ位置の情報を書き込んでいます\n  - ただし、これは CPU アーキテクチャによって規定されているフォーマットというわけではありません\n  - Present Bit = 0 のときは、OS の都合でどんな書き方をしてもよい\n  - Linux では、5 ビットをスワップタイプに、58 ビットをスワップ位置情報に使っている\n  - 全ビットが 0 の場合は、無効なエントリとして扱う\n  - スワップタイプはもともと、スワップタイプはもともと、スワップデバイスの番号を格納するためのものでした\n  - しかし、32 個(5 ビット)もスワップデバイスを付ける人がいないことから、いくつか拡張された使い方に利用されるようになった\n  - その利用方法の一つがページマイグレーションです\n- ページマイグレーションの流れ\n  1. ページテーブルエントリをマイグレーションエントリに置き換える\n  - Present Bit = 0 になるので、CPU は仮想アドレスに対応する物理メモリがないと判断する\n  - この状態でユーザプロセスが仮想アドレスにアクセスすると、対応する物理メモリがないのでページフォルトという例外処理が発生する\n  - ユーザ処理はページマイグレーションが終えるまで待たされる\n  2. カーネルは別のページにメモリの内容をコピーする\n  3. 新しいページの情報をページテーブルに書き込む\n  4. Present Bit = 1 に戻してユーザからのアクセスを可能にして完了\n- ページマイグレーションを利用したデフラグ\n  - 連続したページを確保できるように、デフラグする機能がある\n  - 以前は、連続したページを確保するために、強制的にメモリをスワップアウトしていた\n  - HugeTLB の利用においても 2M の連続メモリが必要なため、自動デフラグ機能は重要になってくる\n- NUMA 対応のメモリアロケータ\n  - Linux は、メモリアロケータがメモリをノードごとに分割する\n  - CPU スケジューラもノードを意識して階層構造になっている\n  - 基本的にスレッドにはそのスレッドが動作している CPU の所属ノード内のメモリを割り当てる\n  - そして、割り当てに失敗した場合にもう片方のノードからメモリを割り当てる\n  - メモリのアクセス速度は、当然同じノード上のメモリの方が早く、他ノードのメモリアクセスはその倍時間がかかる\n  - このため、NUMA 環境では、プロセスとそのメモリ配置がノードごとによるように固定するとよい\n  - これは、numactl というコマンドや、mempolicy というシステムコールで設定できます\n  - また、numad というカーネルデーモンを使うことで、自動配置を調整するような仕組みもある\n  - しかし、動的なページマイグレーションにはペナルティがあるので、できるだけ静的に設定するとよい\n- reclaim 処理もノード単位で行われる\n- カーネルスレッドの kswapd などもノード単位で動作する\n  - zone_reclaim_mode を有効にする必要がある\n\n## CPU やメモリの配置を操作する\n\n- cpuset は、CPU やメモリ、根とワークなどのリソースをグループ単位に割り当てたりできる「cgroup」の機能の一つ\n- プロセスに割り当てた CPU やメモリの配置をユーザが外部から操作できる API を備えている\n- cpuset は、ファイルシステムとしてマウントして使用する\n- cpuset を用いて配置を操作する際は、コントロールファイルと呼ばれるファイルに書き込みます\n\n```\n$ mount\ncgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd)\ncgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)\ncgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)\ncgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)\ncgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event,release_agent=/run/cgmanager/agents/cgm-release-agent.perf_event)\ncgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset,clone_children)\ncgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids,release_agent=/run/cgmanager/agents/cgm-release-agent.pids)\ncgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)\ncgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb,release_agent=/run/cgmanager/agents/cgm-release-agent.hugetlb)\ncgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)\ncgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)\n\n$ ls /sys/fs/cgroup/cpuset\ncgroup.clone_children  cpuset.cpu_exclusive   cpuset.effective_mems  cpuset.memory_migrate           cpuset.memory_spread_page  cpuset.sched_load_balance        notify_on_release\ncgroup.procs           cpuset.cpus            cpuset.mem_exclusive   cpuset.memory_pressure          cpuset.memory_spread_slab  cpuset.sched_relax_domain_level  release_agent\ncgroup.sane_behavior   cpuset.effective_cpus  cpuset.mem_hardwall    cpuset.memory_pressure_enabled  cpuset.mems                machine/                         tasks\n```\n\n```\n# あるプロセスの CPU やメモリの配置を操作するとします\n# まずは、cpuset の操作用ディレクトリを作る、すると、ディレクトリ内に自動的に各種コントロールファイルがつくられる\n/sys/fs/cgroup/cpuset/Group_A\n\n# このうち、「cgroup.procs」というファイルに操作するプロセスの ID を書き込む\ncat /sys/fs/cgroup/cpuset/machine/kubernetes-centos7-1.libvirt-qemu/vcpu0/cgroup.procs\n10607\n\n# そして、cpuset.cpus に利用する CPU, cpuset.mems にノード(今後このノードからリソースの割り当てを得る)を設定できる\ncat /sys/fs/cgroup/cpuset/machine/kubernetes-centos7-1.libvirt-qemu/vcpu0/cpuset.cpus\n0-1\ncat /sys/fs/cgroup/cpuset/machine/kubernetes-centos7-1.libvirt-qemu/vcpu0/cpuset.mems\n0\n```\n\n- メモリの割り当ては少し注意が必要で、値を書き込んで設定を変更しても、その効果は新たに割り当てられるノードにしか恩恵は受けられない\n- すでに使用中のメモリを移動させるには「cpuset.memory_migrate」に 1 を設定しておく\n- これにより、cpuset.mems に値を書き込んだ際に必要に応じてメモリマイグレーションが発生し、使用中のメモリがすべて指定ノードに移動される\n- この cpuset を利用してメモリ配置を自動調整するサービスが numad です。\n  - numad は、プロセスの CPU やメモリーを必要に応じて適切に配置なおしてくれるデーモン\n  - numad は一定時間ごと(15 秒ごと)にシステムをスキャンし、次の 2 種類のシステム情報を入手する\n- ノードごとの CPU アイドル率\n  - /proc/stats から取得\n  - 各 CPU ごとのフィールド 4 番目にシステム起動時からの通算アイドル時間が 10 ミリ秒単位で表示される\n  - これを繰り返し取得し、前回スキャン時からのアイドル時間の差分を差分を計算する\n  - この結果を経過時間で割ると、直近の CPU のアイドル率が求まる\n  - これを CPU_FREE とする\n- ノードごとの空きメモリ情報\n  - /sys/devices/system/node/node0/meminfo\n  - この MemFree の値を MEM_FREE とする\n- CPU_FREE \\* MEM_FREE の値をノードが持つリソースの余裕を評価する値として利用する\n- 全プロセスをスキャンして/proc/プロセス ID/stat ファイルから「CPU 使用量」および「メモリ使用量」を入手する\n  - これらを掛け合わせてスコアを出し、高い順にソートする\n  - リストの上位から順に、各プロセスのノード配置を変更するかどうか判定していく\n  - 判定では、/proc/[pid]/numa_maps を開き、各プロセスがどのノードでどれだけのメモリを利用しているのかチェックする\n  - 得られた（プロセスの)各ノードでのメモリー使用量を MEM_FREE の値に加味し、\n  - magnitude=CPU_FREE\\*MEM_FREE という式を使って各ノードのリソース空き状態を評価する\n  - この magnitude という値をもとに、利用するノードの優先度を決める\n  - 現状の利用ノードが優先度の高いノードとは異なっていた場合、プロセスの CPU やメモリを再配置する\n  - 再配置は、cpuset.memory_migrate に 1 を設定したうえで、cpuset.cpus ファイルに CPU、cpuset.mems にノードの値をセットすることで行われる\n  - プロセスを 1 つ移動したら、同じプロセスを何度も行ったり来たりさせないように最低 5 秒待ってから次回のスキャンを行う\n\n## 自動 NUMA バランス\n\n- 2012-2013 にかけて、カーネル側で自動的にアプリケーションのメモリアクセスを追跡し、CPU とメモリの配置をチューニング（バランス)する仕組みが開発された\n- CPU の移動とスケジューラ\n  - プロセスが実際にメモリが割り当てられるのは、プロセスが仮想メモリに実際にアクセスしたときです\n  - この時、OS はメモリアクセスを実施した CPU にもとっも近いノードからメモリを割り当てる\n  - しかし、スケジューラの都合によってプロセスが動作する CPU が別の NUMA ノードに代わると、メモリが遠くなってしまう\n  - プロセススケジューラはシステム全体を見ながら空いている CPU にプロセスを移動しようと、常に監視を続けています\n  - 移動の際、現在の CPU が属す NUMA ノード内の別の CPU を優先するよう配慮します\n  - カーネルによる NUMA 自動バランシングでは、メモリマイグレーションを利用して、自動的に CPU の位置とメモリの位置を調整します\n  - カーネルのスケジューラにプロセスとメモリの位置を追跡し、必要に応じてメモリを移動したり、（プロセスがスケジュールされるべき NUMA ノードを覚えておいて）チャンスがあれば元の位置にプロセスを戻したりする機能が付加されました\n  - これは自動で有効化されているため、無効化する場合は numa_balancing=disable をセットする\n- アクセス時のメモリ移動\n  - プロセスにメモリページが割り当てられた際、ページの位置がプロセスのページテーブルに登録される\n  - メモリアクセスの追跡においてもページテーブルが利用される\n  - numa_balancing が有効の場合、一定時間ごとにプロセスのメモリをスキャンして、メモリマッピングを一時的に引きはがす処理が行われます\n  - このとき、ページマイグレーションの場合と同じようにページテーブルの PresentBit を 0 にし、ページテーブルには「NUMA 情報収集のために、一時的にページを外した」ことを記録しておきます。\n  - こうしておくと、プロセスがメモリをアクセスした際に、PageFault が発生し、OS がメモリアクセスを補足できる\n  - PageFault が起こると、カーネルはプロセスが現在動作している CPU と引きはがしたページ位置(NUMA ノード)を比較し、\n  - それらが異なる NUMA ノードに置かれている場合にはページマイグレーションを発生させて、メモリを CPU のある NUMA ノードに移動させます\n- プロセスのアクセス追跡\n  - ページマイグレーションにはオーバヘッドがあるので、頻繁に行うわけにはいかない\n  - そこで、カーネルは、どの NUMA ノードで PageFault が起こったかを記録した統計データを集めて活用している\n  - この統計情報には、ページが単一のスレッドからアクセスされたのか、複数のスレッドから共有アクセスされたのか、ページマイグレーションが発生したのかなどが記録される\n  - numa_balancing の統計情報は/proc/vmstat で参照できる\n\n```\n$ cat /proc/vmstat | grep numa\nnuma_hit 6400786\nnuma_miss 0\nnuma_foreign 0\nnuma_interleave 23049\nnuma_local 6400786\nnuma_other 0\nnuma_pte_updates 0             # numa_balancingが変えたPate Table Entryの数\nnuma_huge_pte_updates 0        # numa_balancingが変えたHuge Pate Table Entryの数\nnuma_hint_faults 0             # 上記PTEへのPageFaultの総数\nnuma_hint_faults_local 0       # numa_hint_faultsのうち、元のメモリのノード位置とページフォルト時のノード位置が同じだった数\nnuma_pages_migrated 0          # ページマイグレーションの成功数\n```\n\n- スケジューラは統計データをもとに主に二つの判断を下す\n  - 一つはページスキャンの頻度です\n    - 例えば、複数のスレッドから共有アクセスされるページの場合、それらスレッドが複数の NUMA ノードに分散していると、最適なノードを決められない\n    - このようなページが多い場合は、スキャンの頻度を下げる\n    - 逆に、ページが単一のスレッドからアクセスされメモリマイグレーションが多く発生している場合はスキャンの頻度を高めて、CPU とメモリのバランスを改善できる\n  - もう一つは、プロセスが動作する CPU の決定に使われる\n    - CPU がすべて空いていれば、メモリに近い CPU でスレッドを走らせればよいが、そうでない場合は適切な CPU を割り出す\n    - 場合によっては、プロセスを最適な CPU に移動する\n\n## キャッシュと複数 CPU\n\n- CPU コアが増えた場合、複数の CPU コアから同時にアクセスされる可能性のあるメモリ領域の取り扱い\n- PER CPU メモリの仕組みを見てみる\n- CPU キャッシュとライン\n  - CPU からメモリへのアクセスを高速化するために、CPU にはキャッシュが搭載されている\n  - CPU の仕組みにも依存するが、ある CPU が数バイトのメモリを読み出すと、まずメモリからキャッシュにひと固まりのデータ(ラインと呼ぶ)がロードされる\n  - そして、CPU がキャッシュから必要なデータを読み出す\n  - 書き込む際は、いったんキャッシュを収め(write-allocate)、キャッシュラインに変更を加えた後、適切なタイミングでメモリに書き戻します(write-back)\n  - キャッシュ操作はライン単位で実施される\n  - キャッシュラインのサイズは、Intel CPU では 64 バイトと考えてよいようです\n- CPU0 と CPU1 の二つの CPU があるとする\n  - CPU0 がデータ X に書き込んだ後、CPU1 が同じデータ X に書き込んだ場合を考えてみる\n  - まず、CPU0 がデータ X を含むラインをメモリから読みだしてキャッシュに収め、キャッシュラインに改変を加える\n  - この後、CPU1 が同じデータ X に改変を加えるわけだが、メモリ上のデータ X は CPU0 のキャッシュラインにあるデータ X よりも古くなっている\n  - そこで、CPU0 のキャッシュラインを CPU1 に移します\n  - その際、CPU0 側のキャッシュを消して、CPU1 がデータ X を独占的に書き換えられるようにします\n  - なお、CPU0 と CPU1 がいずれもデータ X を読み出すだけで書き換えない場合は、CPU0 および CPU1 のキャッシュに同じラインが乗っていることもあります\n- False Sharing\n  - 前述のように、米 Intel 社製 CPU ではラインが 64 バイトもあるので、同じデータへのアクセスではなく、近くにあるデータにアクセスした場合にもキャッシュ間でのライン移動が生じる可能性があります\n  - 例えば、long が 8 バイトだとして、その一次元配列を定義し、その並列に各 CPU が異なるインデックスをアクセスした場合、8CPU(64/8)分のデータが同じラインに乗ることになる\n  - 各 CPU がそれぞれのインデクスのデータを書き換えるたびに、ラインの移動が発生し動作が極めて遅くなる\n  - このような状況を False Sharing と呼ぶ\n- キャッシュへのアクセスを最適化するため、Linux カーネルは各 CPU 固有のデータ域を作成する「PER CPU メモリ」という機能を持っている\n  - PER CPU メモリは対となる CPU からしか書き換えないという\"お約束\"のもとに利用できるカーネルメモリ\n  - 各 CPU を占有することにより、False Sharing を回避できるとともに、メモリアクセスに置いてロックなどの排他制御が不要になる\n  - 使い方は、まず次のようなオブジェクトを作成する\n    - x = aloc_percpu\n    - this_cpu(x)で各 CPU のローカルデータに、また per_cpu(x, cpu)で各 CPU 固有のデータ息にアクセスする\n    - 各 CPU のデータは、実際には CPU ごとにまとまった個別のメモリ域に配置され、this_cpu()などによってアドレス位置を計算した上でアクセスされる\n  - PER_CPU カウンタ\n    - 各 CPU 固有のメモリ域にカウンタを持たせることで、精度を犠牲にしてキャッシュラインの相次ぐ移動を避けて SMP 性能を引き上げる機能です\n    - int counter;\n    - per_cpu int pcp_counter;\n    - 各 CPU は pcp_counter を更新し、その絶対値が閾値以上になったら counter に反映する\n    - 例えば、1 ずつカウントアップする counter において閾値が 16 なら counter へのアクセスを 1/16 に減らすことができる\n    - このケースで CPU 数が 16 の場合には、最大で 240 の誤差がでる\n      - /proc から読み取れるデータの中には、こうした仕組みで算出されてるものもあるので、値が必ずしも正確とは限らない\n    - CPU が多い場合には、OS が見せる細かい数値の「正確さ」にはこだわらないように\n\n## リソースコントローラ\n\n- プロセスに割り当てる CPU やメモリの量、I/O やネットワーク帯域を、ユーザからの指示を受けて制御する機能を「リソースコントローラ」と呼ぶ\n- Linux カーネルには、プロセス群のリソースを管理する「cgroup」という仕組みが備わっている\n- memory cgroup\n  - ユーザプロセスのメモリやファイルキャッシュ、カーネルメモリの一部などを管理できる\n  - 各 memory cgroup はカウンタを一つ持っている\n    - そして、ページ獲得要求を受けた際のメモリ獲得制限に、そのカウンタの値を使う\n  - 例えば、memory cgroup のリミットが 4G バイトで、ここに 3 プロセスが所属していてそれぞれが 1G バイトづつ(合計 3G バイト)利用していたとする\n    - この場合、あと 1G バイトのメモリは獲得できる\n    - しかし、1G バイトを超えて獲得しようとした場合には、この memory cgroup 内で保持しているページ(各プロセスのファイルキャッシュやユーザメモリ)からメモリ回収を試みます\n    - また、swap も同じ memory cgroup で limit をかけることができる\n  - つまり、最大の limit は、メモリ + swap のトータルとなる\n  - これは、カーネルのメモリ管理機構が行うスワップアウトに干渉しない\n    - kwsapd などのシステム全体をメンテナンスするプログラムの動作には極力干渉しないようにするため\n- memory cgroup 単位でのメモリ回収と、システム全体でのメモリ回収が両立するように、memory cgroup を組み込んだ場合のメモリ回収用 LRU には工夫が凝らされている\n  - LRU とは Least Recentry Used: 最も長期間使われていないプロックを割り出すアルゴリズムのこと\n  - 基本的には、memory cgroup 単位に LRU を保持しており、システム全体からメモリ回収を行う場合には、すべての memory cgroup を順番にスキャンする\n  - memocy cgroup がある場合は、それらを一つづつ調べて、それぞれの LRU から回収するメモリを見つける\n  - そして、各 memory cgroup が持つページ数や直近のアクセス動向をベースに、メモリを回収すべきかを決定する\n  - memocy cgroup がない場合は、各 NUMA ノードが持つメモリの LRU リストから回収するメモリを見つける\n- カウンタの工夫\n  - memory cgroup はメモリ量を数えるカウンタを持っている\n  - 普通に数えると、性能のボトルネックになるので、PER CPU カウンタのような作りになっている\n  - このため、memory.usage に表示されるメモリ使用量の精度には最大 128K バイトほどの誤差が生じる\n  - とはいえ、タスクスイッチ時に誤差を解消するようなコードになってるので、あまり気にする必要はない\n\n## sysctl\n\n- reclaim 関連\n\n```\nvm.swapiness=1\n# 1 でページアウトよりページキャッシュ解放を優先させる\n\nvm.overcommit_memory=2\n# 2 でオーバーコミットしないようにする\n\nvm.overcommit_ratio=80\n# 仮想メモリ割り当てをメモリサイズ + スワップ領域のサイズ * 80%にする\n\nvm.min_free_kbytes=524288\n# デフォルト(動的に算出される)より大きめに設定して余裕をもってページ回収するようにできる\n# 512MB に設定すると、空きメモリが640MB(low pages)を下回ると kswapd がページ回収を開始し、768MB(high pages)を超えるとやめる\n# 空きメモリが 512MB を下回るとプロセスがメモリ要求時に同期でページ回収が実行される(direct reclaim)。\n\nvm.extra_free_kbytes=1048576\n# kernel 3.5以降\n# low pages、high pages に 1GB 加算し direct reclaim を発生しにくくする\n# メモリ使用率監視閾値(アラート)が 90% なら、low pages がメモリの 10%+a くらいにすると良さげ\n\nvm.zone_reclaim_mode = 1\n# numaノードごとでkswapdが動作するようになる\n# アプリケーションのメモリを特定numaノードからしかアサインしないようにしている場合は1にすべき\n```\n\n- dirty 関連\n\n```\nvm.dirty_background_bytes = 0\nvm.dirty_background_ratio = 10\nvm.dirty_bytes = 0\nvm.dirty_expire_centisecs = 3000\nvm.dirty_ratio = 20\nvm.dirty_writeback_centisecs = 500\nvm.dirtytime_expire_seconds = 43200\n```\n\n## vmstat\n\n- http://linuxinsight.com/proc_vmstat.html\n\n```bash\n$ vmstat\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 1  0    736 6376312 384324 7006248    0    0     6   263  120  102 10  2 89  0  0\n\n$ vmstat 1\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 2  0    736 6244880 384288 7006204    0    0     6   264  119  101 10  2 89  0  0\n 0  0    736 6245036 384288 7006236    0    0     0     0  269  611  1  0 99  0  0\n 0  0    736 6245548 384288 7006236    0    0     0     0  304  604  1  0 99  0  0\n```\n\n## メモリの情報\n\n- /proc/meminfo\n- /proc/sys/vm/...\n\n## ページ\n\n- http://www.coins.tsukuba.ac.jp/~yas/coins/os2-2010/2011-01-11/\n\n## buddy\n\n- 「Buddy システム」は、Linux で使われている外部フラグメンテーションを起こしにくいメモリ割当てアルゴリズム\n- 利用可能なメモリ・ブロックのリストを管理する\n- メモリ・ページを「ゾーン」と呼ばれる領域に分割して管理する\n- ZONE_DMA: DMA でアクセス可能なページ・フレーム x86 では 0-16M\n- ZONE_NOMAL: DMA ではアクセスできないが、カーネルの仮想アドレス空間に常にマップされてる\n  -x86 では 16M-896M\n- NONE_HIGMEM: 普段はカーネルの仮想アドレス空間にマップされていない\n  - 使うときにはマップして使い、使い終わったらアンマップする\n  - x86 では 896M より大きいところ\n- buddyinfo で現在の空き情報が確認できる\n\n```bash\n#                         4K     8k     16k    32k    64k   128k   256k   512k     1M     2M     4M\n$ cat /proc/buddyinfo\nNode 0, zone      DMA      0      1      1      0      2      1      1      0      1      1      3\nNode 0, zone    DMA32    710    596    407    454    362    241     89     14      7      0      0\nNode 0, zone   Normal    106    486    327    178    135     56     40      7      4      0      0\n```\n\n## pmap -x [PID]\n\n- メモリには file map と anon がある\n- file map はプログラムファイルや共有ライブラリタの内容を読み込んだメモリ領域\n- anonymouse はファイルとは無関係の領域で、ヒープ（malloc で得られるメモリ）やスタックに使う\n\n```bash\n$ sudo pmap -x 23572\n[sudo] password for owner:\n23572:   /usr/bin/qemu-system-x86_64 -name centos7 -S -machine pc-i440fx-trusty,accel=kvm,usb=off -cpu Nehalem,+invpcid,+erms,+fsgsbase,+abm,+rdtscp,+pdpe1gb,+rdrand,+osxsave,+xsave,+tsc-deadline,+movbe,+pcid,+pdcm,+xtpr,+tm2,+est,+vmx,+ds_cpl,+monitor,+dtes64,+pclmuldq,+pbe,+tm,+ht,+ss,+acpi,+ds,+vme -m 12192 -realtime mlock=off -smp 2,sockets=2,cores=1,threads=1 -uuid ab775a1c-10cc-5010-7523-010f517993ed -nographic -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/centos7.moni\nAddress           Kbytes     RSS   Dirty Mode  Mapping\n00007fcc32000000 12484608 8425204 8418308 rw---   [ anon ]\n00007fcf2c000000    1240     136     100 rw---   [ anon ]\n00007fcf2c136000   64296       0       0 -----   [ anon ]\n...\n00007fcf4a160000    4800    1588       0 r-x-- qemu-system-x86_64\n00007fcf4a810000     896      44      44 r---- qemu-system-x86_64\n00007fcf4a8f0000     324      24      24 rw--- qemu-system-x86_64\n...\n```\n\n## HugePage\n\n- http://dpdk.org/doc/guides-16.04/sample_app_ug/vhost.html\n- https://access.redhat.com/documentation/ja-JP/Red_Hat_Enterprise_Linux/7/html/Virtualization_Tuning_and_Optimization_Guide/sect-Virtualization_Tuning_Optimization_Guide-Memory-Tuning.html\n\n```\n# デフォルトを確認\n$ cat /proc/meminfo\nAnonHugePages:     81920 kB\nHugePages_Total:    2048\nHugePages_Free:     2048\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\n\n# hugepage を1G * 8page 確保\n$ sudo vim /etc/default/grub\nGRUB_CMDLINE_LINUX=\"default_hugepagesz=1G hugepagesz=1G hugepages=8\"\n\n$ sudo grub-mkconfig -o /boot/grub/grub.cfg\n$ reboot\n\n# 確認\n$ cat /proc/meminfo\nAnonHugePages:     53248 kB\nHugePages_Total:      13\nHugePages_Free:       13\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:    1048576 kB\n\n# hugepageサイズ\n$ cat /proc/sys/vm/nr_hugepages\n13\n\n$ vim /etc/fstab\nhugetlbfs   /mnt/huge   hugetlbfs defaults,pagesize=1G 0 0\n\n$ sudo mount -a\n\n# mount確認\n# デフォルトで、kvmも/run/hugepages/kvm にhugetlbfsをマウントしてるマウントしてる？\n$ mount\nhugetlbfs-kvm on /run/hugepages/kvm type hugetlbfs (rw,mode=775,gid=125)\nhugetlbfs on /mnt/huge type hugetlbfs (rw,pagesize=1G)\n\n# hugepageをバッキングメモリにして2GのVMを起動すると、hugepageから確保される\n$ sudo pmap 5150\n5150:   /usr/bin/qemu-system-x86_64 -name centos7 -S -machine pc-i440fx-trusty,accel=kvm,usb=off -cpu Nehalem,+invpcid,+erms,+fsgsbase,+abm,+rdtscp,+pdpe1gb,+rdrand,+osxsave,+xs\nave,+tsc-deadline,+movbe,+pcid,+pdcm,+xtpr,+tm2,+est,+vmx,+ds_cpl,+monitor,+dtes64,+pclmuldq,+pbe,+tm,+ht,+ss,+acpi,+ds,+vme -m 1954 -mem-prealloc -mem-path /run/hugepages/kvm/l\nibvirt/qemu -realtime mlock=off -smp 2,sockets=1,cores=2,threads=1 -uuid 2c28f6ae-5fb4-11e6-95b5-cce1d540d3fa -nographic -no-user-config -nodefaults -chardev socket\n00007f8980000000 2097152K rw--- qemu_back_mem.pc.ram.ZedopK (deleted)\n00007f8a34000000   1144K rw---   [ anon ]\n0\n\n\n$ virsh dumpxml xxxx\n\u003cmemoryBacking\u003e\n   \u003chugepages/\u003e\n\u003c/memoryBacking\u003e\n```\n\n## /proc/[PID]/smaps\n\n```\n$ sudo cat /proc/23572/smaps | less\n7fcc32000000-7fcf2c000000 rw-p 00000000 00:00 0\nSize:           12484608 kB\nRss:             8429404 kB\nPss:             5329099 kB\nShared_Clean:        528 kB\nShared_Dirty:    3487568 kB\nPrivate_Clean:      6388 kB\nPrivate_Dirty:   4934920 kB\nReferenced:      8257668 kB\nAnonymous:       8429404 kB\nAnonHugePages:   1107968 kB\nSwap:            1996964 kB\nKernelPageSize:        4 kB\nMMUPageSize:           4 kB\nLocked:                0 kB\nVmFlags: rd wr mr mw me dc ac sd hg mg\n7\n\n# 2Gなので2ページ分消費された\n$ cat /proc/meminfo\nHugePages_Total:      13\nHugePages_Free:       11\nHugePages_Rsvd:        0\nHugePages_Surp:        0\n```\n\n## kswapd\n\n## kcompactd\n\n## khugepaged\n\n## malloc()\n\n## jemalloc()\n\n## tcmalloc()\n\n## kmalloc()\n\n## slub\n\n- カーネルは、メモリの利用効率を高めるために、カーネル空間内のさまざまな メモリ資源を、資源ごとにキャッシュしている領域\n- もし、slub のメモリが断片化してしまうとパフォーマンスにも影響が出てくる\n- スラブの利用量は slubtop で確認できる\n\n```\nActive / Total Objects (% used)    : 1343215 / 1352019 (99.3%)\n Active / Total Slabs (% used)      : 35104 / 35104 (100.0%)\n Active / Total Caches (% used)     : 66 / 98 (67.3%)\n Active / Total Size (% used)       : 193251.77K / 194863.62K (99.2%)\n Minimum / Average / Maximum Object : 0.01K / 0.14K / 15.88K\n\n  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME\n295152 295152 100%    0.10K   7568       39     30272K buffer_head\n238770 238620  99%    0.19K  11370       21     45480K dentry\n150848 148349  98%    0.06K   2357       64      9428K kmalloc-64\n128860 127969  99%    0.02K    758      170      3032K fsnotify_event_holder\n125696 124665  99%    0.03K    982      128      3928K kmalloc-32\n 98304  97251  98%    0.01K    192      512       768K kmalloc-8\n 69003  68498  99%    0.08K   1353       51      5412K selinux_inode_security\n 58368  58368 100%    0.02K    228      256       912K kmalloc-16\n```\n","UpdatedAt":"2021-03-15T21:55:34.7578256+09:00"},{"Text":"# メモリ(ハードウェア)\n\n## 規格\n\n- [DDR4](https://ja.wikipedia.org/wiki/DDR4_SDRAM)\n\n## 物理メモリ\n\n- L1 キャッシュでのアクセスミスは数 10 クロックのペナルティが生じる\n- L2 キャッシュでのアクセスミスは数 10 バスクロックのペナルティが生じる\n- キャッシュに読み込まれるタイミング\n  - アプリケーションが参照したメモリの内容がキャッシュにない場合\n  - アプリケーションがメモリに書き込みを行った内容がキャッシュにない場合\n  - アプリケーションがプリフェッチ命令を実行した場合\n  - ハードウェア・プリフェッチャーが動作した場合\n- 読み込み書き込みの最小単位はキャッシュライン（64 バイト）\n\n- バンド幅\n- ストライド幅\n  - チャンクサイズ／ブロックサイズ(4KB)\n- ストライプ幅\n  - データディスク数 × ストライド\n- レイテンシ\n\n## 用語\n\n- DIMM\n  - Dual Inline Memory Module\n  - 自作 PC でよく使われるメモリモジュール\n- SO-DIMM\n  - Small Out DIMM\n  - 大きさが DIMM の半分くらい\n  - ノートパソコンなどの省スペースな機器で使われる\n- DRAM\n  - Dynamic Random Access Memory\n  - 揮発性で、SRAM に比べてリフレッシュのために常に電力を消費することが欠点だが、大容量を安価に提供されている\n  - DRAM チップとして DIMM にパッケージングされる\n    - DIMM に DRAM が 8 個とか、裏面も利用して 16 個とかついている\n  - SDRAM\n    - Synchronous Dynamic Random Access Memory\n    - システムバスに同期して動作する DRAM\n    - SDRAM のインタフェースは同期式であり、制御入力に応答する前にクロック信号を待つため、コンピュータのシステムバスに同期して動作する\n    - クロックは入ってくる命令をパイプライン化する内部の有限状態機械を駆動するのに使われる\n      - そのため SDRAM のチップは非同期 DRAM よりも複雑な操作パターンを持つことができ、より高速に動作できる。\n- Rank\n  - DIMM は基本的に 64bit のバス幅を持っており、CPU やチップセットのメモリコントローラと 64bit 単位でデータのやり取りを行う\n  - 1 枚の DIMM に 64bit になる DRAM のグループが一つあれば 1Rank、2 つあれば 2Rank となる\n  - メモリコントローラは Rank ごとにやり取りを行うので、2Rank メモリでは電気的には 2 枚のメモリが刺さってることになる\n    - メモリコントローラまでの速度が下がる、搭載できる枚数が減るなどの制約が発生する場合がある\n  - メモリの速度は搭載枚数と、Rank によって変わってくる\n    - 2DIMM の場合、Rank1 だと 2,667MHz、Rank2 だと 2,400MHz\n    - 4DIMM の場合、Rank1 だと 2,133MHz、Rank2 だと 1,866MHz\n  - 参考: [Ryzen で話題になった、メモリの”Rank”って何のことと?](https://pc.watch.impress.co.jp/docs/column/century_micro/1053794.html)\n- DRAM の構成(4GB の例)\n  - 512Mb \\* 8 構成では、一つの DRAM は 512Mb のセルアレイが 8 つからなっている\n  - 512Mb のセルアレイは 512M 個の 1bit のセルからなっていて、このセルに対して行と列を指定することで、DRAM にデータを読み書きしている\n  - この DRAM が DIMM の片面に 8 つ搭載されて、512Mb _ 8bit _ 8 個 = 4GB となる\n  - バス幅も 8bit \\* 8 個なので 64bit = Rank1 となる\n- SPD\n  - Serial Presence Detect\n  - PC やサーバーを起動するさいに、メモリモジュールの情報を BIOS に伝える規格/仕組み\n  - SPD データ(メモリモジュールの情報)\n    - メーカ名、規格、容量、搭載 DRAM(CHIP)の情報(メーカー名・規格・bit 構成・容量)、動作クロック周波数、CAS Latnecy(Column Address Strobe Latency)、動作電圧、ECC(誤り訂正符号)などのが含まれる\n  - SPD データは EEPROM(Electrically Erasable Programmable Read Only Memory)と呼ばれるモジュールに保存される\n    - ライトプロテクトの機能が搭載されている\n    - データの書き換えには専用の機器が必要\n  - 参考: [メモリモジュールに\"SPD\"という情報があるのを知っていますか?](http://pc.watch.impress.co.jp/docs/column/century_micro/1076466.html)\n- ビットクロス\n  - 新製品(DRAM)のビット当たりの単価が、従来品(DRAM)の単価を下回ること\n  - ビットクロスするのとほぼ同時期に出荷量も逆転し、新旧世代交代となる\n- ワード線・ビット線\n  - ワード線は 2 次元状に並んだメモリーセルアレイの中から一列を選択するための制御信号線\n  - メモリーセルは、ワード線とビット線の交点に置かれており、読み出し/書き込みを行なうアドレスに対応するワード線の電圧を上げることで、書き込み/読み出しが可能になる\n\n## コマンド\n\n```\n# 搭載メモリ情報を表示する\n$ sudo dmidecode -t memory\n```\n\n## 記事\n\n- 2017/09/29 (サーバー/ハイエンド PC の主記憶を変革する NVDIMM 技術](http://pc.watch.impress.co.jp/docs/column/semicon/1083346.html)\n- 2011/11/18 [次世代 DRAM 規格「DDR4」と新技術規格「3DS」の概要を公表- Server Memory Forum 2011](http://www.kumikomi.net/archives/2011/11/rp49serv.php?page=2)\n","UpdatedAt":"2021-03-15T21:55:34.759028+09:00"},{"Text":"# memory tuning\n\n- https://doc.opensuse.org/documentation/leap/archive/42.3/tuning/html/book.sle.tuning/cha.tuning.memory.html\n- http://www.slideshare.net/janghoonsim/kvm-performance-optimization-for-ubuntu?qid=fb99f565-8ae4-44d3-9b58-8d8487197566\u0026v=\u0026b=\u0026from_search=3\n\n## THP\n\n### Paging level\n\n| Platform | Page size    | Address bit used | Paging levels | splitting  |\n| -------- | ------------ | ---------------- | ------------- | ---------- |\n| x86_64   | 4KB(default) | 48               | 4             | 9+9+9+9+12 |\n| x86_64   | 2MB(THP)     | 48               | 3             | 9+9+9+21   |\n\n```\n# THPが有効かどうか調べる\n# alwaysが有効\n$ cat /sys/kernel/mm/transparent_hugepage/enabled\n[always] madvise never\n\n# set mode\n$ sudo sh -c 'echo \u003cmode\u003e \u003e /sys/kernel/mm/transparent_hugepage/enabled'\n\n# Huge pageを確認\n$ cat /proc/meminfo| grep Huge\nAnonHugePages:    391168 kB\nHugePages_Total:       0\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\n\n# パラメータを調整\n$ sudo ls /sys/kernel/mm/transparent_hugepage/khugepaged\nalloc_sleep_millisecs  defrag  full_scans  max_ptes_none  pages_collapsed  pages_to_scan  scan_sleep_millisecs\n\n$ grep thp /proc/vmstat\nthp_fault_alloc 1393\nthp_fault_fallback 0\nthp_collapse_alloc 44\nthp_collapse_alloc_failed 0\nthp_split 976\nthp_zero_page_alloc 1\nthp_zero_page_alloc_failed 0\n\n```\n\n## ksm\n\n```\nKSM (kernel samepage merging)\necho \"1\" \u003e /sys/kernel/mm/ksm/run\n```\n\n## for numa\n\n```\n/sys/kernel/mm/ksm/merge_across_nodes\n```\n","UpdatedAt":"2021-03-15T21:55:34.7600307+09:00"},{"Text":"# Network\n\n## NAPI 対応の受信処理(Kernel2.6 以降)\n\n- Kernel2.6 ではデバイスドライバに対して NAPI(New API)と呼ばれる新しい受信 API が提供されるようになった。\n- ネットワークインタフェースで受信したパケットは、デバイスドライバの H/W 割り込み処理処理で刈り取られる。\n- ハードウェア割り込みが発生すると、H/W 割り込みを禁止してポーリング処理によってデバイスの受信バッファからパケットを取り出していく。\n- バッファが空になって受信処理が完了すると、割り込みを再度許可状態にして、次の受信が発生するのを待つようにしている。\n- 参考\n  - http://wiki.bit-hive.com/linuxkernelmemo/pg/%C1%F7%BC%F5%BF%AE\n  - http://syuu1228.hatenablog.com/entry/20101015/1287095708\n\n## ソケットインタフェース\n\n- TCP ソケットは listen()関数の第二引数 backlog に指定した数の、完全に確立された接続要求を待ち受けることができるキューを作成します。\n- キューがいっぱいになった状態で新たに接続を受け取ると、サーバは ECONNREFUSED を返します。\n- 下位層のプロトコルが再送をサポートしていれば、ECONNREFUSED は無視され、リトライが成功するかもしれません。\n- net.core.somaxconn は、TCP ソケットが受け付けた接続要求を格納する、キューの最大長です。\n- backlog \u003e net.core.somaxconn のとき、キューの大きさは暗黙に net.core.somaxconn に切り詰められます。\n- 参考\n  - [listen backlog](http://wiki.bit-hive.com/linuxkernelmemo/pg/listen%20backlog%20%A1%DA3.6%A1%DB)\n\n```\n# backlogの最大値\nnet.core.netdev_max_backlog = 1000\n\n# synbacklogの最大値\nnet.ipv4.tcp_max_syn_backlog = 128\n\n# 実際のbacklogの設定はアプリケーションにゆだねられる\n# syn backlogはbacklog値を8~max_syn_backlogの範囲に収めた後、一つ上の2のべき乗の値に切り上げた値となる\n# 例えばbacklogが200ならsyn backlogは256となり、backlogが256ならsyn backlogサイズは512になる\n\n# TCPセッションのキュー\n# TCPセッション数をbacklogで管理し、それを超えたものがこのキューで管理される\n# tcp_max_syn_backlogを大きくしても、somaxconnが小さいと、backlogも小さく切り詰められ、syn backlogもつ遺作なる\n$ sysctl net.core.somaxconn\n128\n\n# パケットの取りこぼしは、netstatで確認できる\n$ netstat -s\n...\nTcpExt:\n    107708 times the listen queue of a socket overflowed\n    107708 SYNs to LISTEN sockets dropped\n```\n\n## TCP\n\n```\n# sysctlの各種パラメータの詳細はmanで調べるとよい\n$ man tcp 7\n\n# TIME_WAIT状態がタイムアウトする時間\nnet.ipv4.tcp_fin_timeout = 30\n\n# [low, pressure, high] からなるベクトル値(単位はページ: 通常は4k）\n# アロケートしたページがlow以下であれば、メモリアロケーションは調整しない\n# pressureを超えると、TCPはメモリ消費を調整するようになる\n# highはアロケートできる最大値\nnet.ipv4.tcp_mem = 383457       511277  766914\n\n# 単位はバイト\nnet.ipv4.tcp_rmem = 4096        131072  6291456\nnet.ipv4.tcp_wmem = 4096        16384   4194304\n```\n\n## stat\n\n#### /proc/net/protocols\n\n- 各プロトコルが memory pressure モードであるかを確認する\n- press が yes となってれば pressure モードとなっているので注意\n- memory が実際にアロケートしているページ数\n- memory が、tcp_mem の pressure を超えると pressure モードとなり、以下の処理をする\n  - TCP ソケットの送信・受信バッファのサイズを制限する\n  - 受信の TCP ウィンドウサイズを小さくする（もしくは ZeroWindow にする)\n  - 受信キューに入ったセグメントから重複したシーケンス番号をもつセグメントをマージして、空きメモリの確保を試みる(collapse 処理）\n  - シーケンス番号順に受信できなかったセグメントを保持する Ouf of Order キューに入った SACK 済みセグメントを破棄して空きメモリを確保する（prune 処理）\n  - 受信スロースタートの閾値を制限する\n- memory が、tcp_mem の high を超える以下の処理を行う\n  - セグメントの受信処理で、新規に受信したセグメントを破棄する（Drop）\n  - セグメントの送信処理で、メモリを確保できるまでプロセスをブロックして待機させる\n  - セグメント受信処理で、Ouf of Order キューのセグメントを破棄して空きを確保しようとする(SACK renege)\n  - 一部の TCP のタイマー処理をやり直す\n  - 送信バッファに一定量のデータをもったままのソケットを close すると TCP oom を起こす\n    - TCP oom では、RST を送信してコネクションをクローズさせ、dmesg に TCP oom のログを出す\n\n```\n$ cat /proc/net/protocols\nprotocol  size sockets  memory press maxhdr  slab module     cl co di ac io in de sh ss gs se re sp bi br ha uh gp em\nPACKET    1408      2      -1   NI       0   no   kernel      n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n\nPINGv6    1136      0      -1   NI       0   yes  kernel      y  y  y  n  n  y  n  n  y  y  y  y  n  y  y  y  y  y  n\nRAWv6     1136      2      -1   NI       0   yes  kernel      y  y  y  n  y  y  y  n  y  y  y  y  n  y  y  y  y  n  n\nUDPLITEv6 1280      0       1   NI       0   yes  kernel      y  y  y  n  y  y  y  n  y  y  y  y  n  n  n  y  y  y  n\nUDPv6     1280      2       1   NI       0   yes  kernel      y  y  y  n  y  y  y  n  y  y  y  y  n  n  n  y  y  y  n\nTCPv6     2304      2      36   yes    304   yes  kernel      y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y\nXDP        960      0      -1   NI       0   no   kernel      n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n\nUNIX      1024    111      -1   NI       0   yes  kernel      n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n\nUDP-Lite  1088      0       1   NI       0   yes  kernel      y  y  y  n  y  y  y  n  y  y  y  y  y  n  n  y  y  y  n\nPING       928      0      -1   NI       0   yes  kernel      y  y  y  n  n  y  n  n  y  y  y  y  n  y  y  y  y  y  n\nRAW        936      0      -1   NI       0   yes  kernel      y  y  y  n  y  y  y  n  y  y  y  y  n  y  y  y  y  n  n\nUDP       1088      4       1   NI       0   yes  kernel      y  y  y  n  y  y  y  n  y  y  y  y  y  n  n  y  y  y  n\nTCP       2144     28      36   yes👈  304   yes  kernel      y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y\nNETLINK   1064     15      -1   NI       0   no   kernel      n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n  n\n\n```\n\n#### /proc/net/netstat\n\n- netstat: TCPMemoryPressures\n  - memory pressure モードに入ると 1 インクリメントされる\n- netstat: TCPMemoryPressuresChrono\n  - memory pressure モードであった時間が加算される\n- netstat: OfoPruned\n  - Ofo は Ouf of Order の略で、TCP の Ouf of Order キューを指す\n  - memory pressure モードで Ofo キューのパケットを破棄すると 1 インクリメントされる\n- netstat: TCPAbortOnMemory\n  - TCP oom が発生すると 1 インクリメントされる\n- netstat: PruneCalled, RcvPrune\n  - net.ipv4.tcp_mem の high を超えていて、受信キューの collapse 処理と Ofo キューのパケットを破棄(Drop)を行ってもなお、空きメモリが確保できないと、1 インクリメントされる\n- netstat: TCPRcvQDrop\n  - net.ipv4.tcp_mem の high を超えていて、受信したパケットをドロップした際に 1 インクリメントされる\n- netstat: TCPAbortFailed\n  - TCP oom が発生すると該当のソケットで RST を送りコネクションを切断する\n  - この際に、ソケットバッファの割り当てに失敗すると、1 インクリメントされる\n    - ソケットバッファの割り当て失敗は、TCP クォータの制限による失敗ではなく、Slab アロケータによる割り当て失敗が原因となる\n\n## DDOS 対策\n\n```\n# syn flood攻撃対策\n# syn flood状態になると、synパケットにメモリ領域を割り当てずに、シーケンス番号を正しくチェックするため特殊な情報をSYN ACKパケットに含ませて返すようになる\n# そして、クライアントが正しくACKを返せば、TCP接続を行う\nnet.ipv4.tcp_syncookies = 1\n\n# syn flood状態になると、新規のSYN_RECVは登録せずに破棄する\nnet.ipv4.tcp_syncookies = 0\n\n# Smurf攻撃対策\nnet.ipv4.icmp_echo_ignore_broadcasts = 1\n\n# ICMPエラー無視\nnet.ipv4.icmp_ignore_bogus_error_responses = 1\n```\n\n## Bufferbloat\n\n- 不適切なネットワークキューイングや過剰なバッファによりレイテンシーが悪化、または不安定な状態\n- レイテンシーの悪化、揺らぎにより、TCP の服装制御の混乱を招き、スループットの低下も起こる\n\n## メモ\n\n```\nLinux の Network Scheduler デフォルトは FIFO\nab -\u003e IP Stack -\u003e queue queue queue -\u003e NIC Buffer\ndb -\u003e | -- Buffer Size -- |\n\nNic のバッファサイズが大きいとロスが増加\n\n遅いもの、大きいものは処理に時間がかかる\n早いもの、小さいものはその逆 VoIP とか DNS とか\n\nアクティブキューイング\nFIFO を廃止し、キューイングを様々な方法で制御\n\nCoDel(Controlled Delay)\nRTT やプロトコル等を考慮して必要に応じてキューの先頭に割り込む\n\nTCP small queues\nByte Queue Limits\n```\n\n## sysctl\n\n- net.\\*.conf.lo.rp_filter\n  - Reverse Path Filter\n  - 複数 NIC のサーバに置いて、ある IF から受信したパケットが、別の IF から送信される場合がある\n  - パケットが入ってくる経路と、出てく経路が異なる場合に、そのパケットをフィルタリングする設定\n  - 設定値\n    - 0: 無効\n    - 1: Strict Mode: ある IF で受信したパケットの返信先がその IF のネットワークアドレスと同じ場合は、許可\n    - 2: Loose Mode: ある IF で受信したパケットの送信元 IP がいずれかの IF より到達可能の場合は、許可\n- net.\\*.conf.lo.arp_filter\n  - ARP Filter\n  - 設定値\n    - 0: 別のインターフェースからのアドレスの ARP 要求に対応できます\n    - 1: 同じサブネットで複数のネットワークインターフェースを持つことを可能にし、カーネルがインターフェースから ARP 要求 の IP パケットをルーティングするかどうかに基づいて、各インターフェースの ARP が応答できるようにします\n\n## 複数 NIC 利用時の注意点\n\n- 複数の NIC が別々のセグメントに所属する場合\n  - routeing table に NIC ごとの定義してあれば、routing table どおりに通信が可能\n  - ルート定義がないセグメントから、デフォルトゲートウェイ以外の NIC に通信があった場合\n    - 返信時の送信はデフォルトゲートウェイが利用される\n    - このとき、デフォルトゲートウェイからの到達性がない場合はパケットは到達しない\n- 複数の NIC が同一のセグメントに所属する場合\n  - ARP Filter が 0 の場合、デフォルトゲートウェイ側の NIC が ARP 要求に応答してしまう\n    - このため、デフォルトゲートウェイ以外の NIC への ARP は、ただしく解決できない\n  - ARP Filter が 1 の場合、ARP を受信した NIC から応答するようになる\n\n## netlink\n\n- Linux カーネルのサブシステムの名称で、このサブシステムと、ユーザ空間のアプリケーションがやり取りするためのソケットベースの IPC が定義されている\n- アプリケーションはソケット通信によって、Linux カーネル管理下のネットワーク関連リソースを操作したり、その状態を取得することができる\n- ip コマンドや、ss コマンドもこの netlink を利用して、リソース情報を取得したり、操作を行っている\n- 参考\n  - [Netlink IPC を使って Linux カーネルのネットワーク情報にアクセスする](http://ilyaletre.hatenablog.com/entry/2019/09/01/205432)\n  - [Kernel Korner - Why and How to Use Netlink Socket](https://www.linuxjournal.com/article/7356)\n  - [Go による実装 2: docker.libcontainer](https://github.com/docker-archive/libcontainer/blob/master/netlink/netlink_linux.go)\n    - container 用の必要最低限の実装なのでわかりやすい\n  - [Go による実装 1: netlink](https://github.com/vishvananda/netlink)\n    - もともとは libcontainer の netlink 機能をフォークしたものだがほぼ別物\n    - netlink に絞ったもりもりの実装\n\n## Refarence\n\n- [Linux カーネルメモ 送受信](http://wiki.bit-hive.com/linuxkernelmemo/pg/%C1%F7%BC%F5%BF%AE)\n- [ネットワーク関係記事まとめ 2013/06](http://syuu1228.hatenablog.com/entry/20130603/1370300554)\n- [ペパボ トラブルシュート伝 - TCP: out of memory -- consider tuning tcp_mem の dmesg から辿る 詳解 Linux net.ipv4.tcp_mem](https://tech.pepabo.com/2020/06/26/kernel-dive-tcp_mem/)\n\n## メモ\n\n- プロトコルスタック\n- ネットワークデバイス\n- netfilter\n- コネクション\n- パケットスケジューラー\n- ルーティングテーブル\n- FIB\n- RIB\n- トライ木\n- busypool\n\n## 仮想ネットワークデバイス\n\n- ipvlan\n  - 親のネットワークデバイスから派生する子デバイス\n  - 親デバイスは L2 と L3 を持ち、子デバイスは L3 のみを持つ\n  - L2 の処理を親デバイスに任せる(L2 モード)\n  - 一つのネットワークデバイスに複数の L3 アドレスを割り振るではダメなのか？\n  - デバイスそのものをわけて使いたい場合がある\n- bridge\n- bonded interface\n- team device\n- vlan\n- vxlan\n- macvlan\n\n## Refarences\n\n- [Introduction to Linux interfaces for virtual networking](https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking/)\n","UpdatedAt":"2021-04-11T22:42:25.8431648+09:00"},{"Text":"# ネットワーク基礎\n\n## OSI 参照モデル\n\n- OSI は、Open System Interconnection の略\n- アプリケーション層(L7)\n  - それぞれのアプリケーションの中での通信に関するプロトコルを定める\n  - HTTP, FTP, DHCP, NTP, SMTP などのプロトコルがこれにあたる\n- プレゼンテーション層(L6)\n  - 文字のエンコード方式や圧縮方式、暗号方式などのデータフォーマット(表現:プレゼンテーション)を定める\n  - 文字のエンコード方式は UTF-8、Shift_JIS など\n  - 画像の圧縮形式であれば、JPEG、MPEG など\n- セッション層(L5)\n  - アプリケーションレベルでの送受信(request, response)による通信の接続を管理する\n  - HTTP セッションなどがこれにあたる\n- トランスポート層(L4)\n  - データ転送を行うための論理的な通信プロトコル\n  - TCP, UDP などがこれにあたる\n  - TCP は、TCP セッションを持ち信頼性のある通信が行える\n  - UDP は、セッションレスであり信頼性は低いが低レイテンシの通信が行える\n- ネットワーク層(L3)\n  - IP アドレスの管理や経路の選択を行い、宛先までデータを届けるプロトコル\n  - IP や ICMP がこれにあたる\n    - IP でのアドレスを単に IP アドレスと呼ぶ\n- データリンク層(L2)\n  - 物理的(もしくは無線的)に隣同士で接続された機器間での通信\n  - 一つの伝送路をリンクと呼ぶ\n  - 0 と 1 のビット列からなるデータをフレームと呼ぶ\n  - 通信手順を定める規格には様々なものがある\n    - Ethernet(IEEE 802.3)\n      - オフィスや一般家庭などで一般的によく利用される優先 LAN における通信規格\n    - PPP(Point-to-Point Protocol)\n    - PPTP(Point-to-Point Tunneling Protocol)\n      - VPN(Virtual Private Network)で用いられる\n    - L2TP(Layer 2 Tunneling Protocol)\n  - Ethernet では MAC(Medium Access Control: 媒体アクセス制御) によって、通信制御が行われる\n    - MAC でのアドレスを単に MAC アドレスと呼ぶ\n- 物理層(L1)\n  - 情報ビット列を電気や光の信号に変換し、同軸ケーブルや光ファイバーといった媒体を返しての通信層\n  - LAN ケーブルでよく目にする 10BASE-T や 100BASE-TX などが物理層の規格\n- コラム\n  - 各層の事を呼ぶときは省略して L1, L2,..,L7 と呼ぶことが多い\n  - 基本的にアプリケーション層からセッション層はアプリケーションによって実装されるため、これらをまとめて単にアプリケーション層(L7)と言うこともよくある\n  - パケットとフレームについて\n    - パケットは厳密にはすべてのレイヤで使われるデータ単位として使われる一般的な用語\n      - L4 でも、L3 でも、L2 でもそのデータ単位をパケットと呼ぶ\n    - フレームは L2 でのパケットのこと\n    - しかし、一般的にはパケットとフレームは同じものを指して使われることが多い\n    - PPS(Packets Per Sec)という言葉も、基本的に L2 でのパケット（つまりフレーム)のことを指す\n\n## RFC\n\n- Request For Comments の略\n- IETF(Internnet Engineering Task Force)が発行している技術仕様群\n- 明確な規格というよりかは、規格を提案する場という意味合いがある\n  - 規格考えたからコメント求むというニュアンス\n- TCP, IP などのほぼすべてのプロトコル仕様はここに記載され日々議論されている\n\n## 階層モデルにおけるデータフレームのフォーマット\n\n- 最終的な通信データのフレームは L2 で完成する\n- 1 フレームは、アプリケーションで扱いたいデータに加え、L4, L3, L2 の各層それぞれで必要となる情報をヘッダとして付与して完成する\n  - TCP ではデータの順序番号や再送制御に関する情報がヘッダに入ってる\n  - IP であれば IP アドレスの情報などがヘッダに入ってる\n  - 一般的にイーサネットフレームの最大値(MTU)は 1500 バイト\n    - このうち TCP ヘッダが 20-60 バイト、IP ヘッダが 20 バイト、アプリケーション層では 1420 バイト\n    - また TCP における TCP ヘッダを含めないデータ単位の事をセグメントとよび、その最大のことを MSS(Maximum Segment Size)と呼ぶ\n      - MTU1500 の場合は、TCP ヘッダの最小 20 と IP ヘッダの最小を引いた 1460 が MSS となる\n  - これに加えてイーサネットのヘッダー 14 バイトとフレーム末尾に付与される誤り検出のための FCS(Frame Check Sequence)4 バイトが付与されて一つのフレームが完成\n    - つまり MTU1500 の場合の最大フレームサイズは 1518 バイトとなる\n- L2 と L3 の対応付け\n  - L3 でルーティングした後に最終的な宛先へ L2 でパケットを転送することとなる\n  - このため、IP アドレスと MAC アドレスの対応関係が必要になる\n  - この対応関係を ARP Table と呼び、ARP(Address Resolution Protocol: アドレス解決プロトコル)により事前に作成される\n- データ転送は各機器ごとに必要となる階層でのみ処理を行えばよい\n\n## path MTU discovery\n\n- MTU は各ネットワーク機器で設定されており、送信者は途中経路のネットワーク機器の MTU を考慮してパケット分割する必要がある\n- このためには途中経路の MTU を検索する必要がありこれを path MTU discovery と呼ぶ\n- これは基本的には ICMP によって行われる、しかし ICMP では ECMP を利用された場合に正しく MTU の解決ができない場合があり TCP で行われる場合もある\n\n## ネットワークの輻輳\n\n- ネットワーク機器には単位時間あたりに伝送可能なデータ量に限りがある\n- ネットワーク機器は受信バッファを持っており、これを超えるデータを受信するとその分のデータは破棄される(overflow)\n- また、データが同時にしたこと(衝突)によってデータが破損することもある\n- このような状況を輻輳と呼ぶ\n- TCP や QUIC などは、輻輳制御によりこのようなネットワーク状況を考慮し、データ破損などがあればネットワークが輻輳してるとみなして通信データ量を減らしたりする\n- 一方で UDP にはこのような仕組みがないので UDP を使う場合は、アプリケーション自体にそのような制御機構がないとネットワーク輻輳の原因となる可能性がある\n\n## UDP\n\n- チェックサム\n  - データ破損がないかチェックする機構\n- UDP で信頼性を確保するために L7 で制御するプロトコルもある\n  - RTP(Real-time Transport Protocol)\n    - UDP にシーケンス番号やタイムスタンプ機能を追加したもの\n  - RUDP(Reliable User Datagram Protocol)\n    - RTP にシーケンス番号の確認応答、再送機能を追加したもの\n  - DCCP(Datagram Congestion Control Protocol)\n    - UDP における輻輳の緩和を目的として考案されたもの\n  - QUIC(Quick UDP Internet Connections)\n    - HTTP を UDP で実現したもので、信頼確保や輻輳制御なども L7 で行われる\n    - QUIC は、2012 年に Google のエンジニアが提案した L4 のプロトコル仕様が発端となった\n      - 初期の仕様は Google-QUIC、gQUIC と呼ばれ、Chrome など実験的に使われてきた\n    - この QUIC 上で HTTP を動かす規格を HTTP/3 と呼ぶ\n\n## TCP\n\n- データの転送単位をセグメントと呼ぶ\n- チェックサム\n  - データ破損がないかチェックする機構\n- コネクション管理\n  - データと ACK をやりとりすることで信頼性を保証する\n  - シーケンス番号\n    - セグメントの順番を保証し、またその消失を検出するために利用される\n    - シーケンス番号は通信データを 1 バイト単位でカウントしたもの\n    - 送信側は、データを送信するときにシーケンス番号を付与して送信する\n    - 受信側は、データを受信したら、その次に受信したいシーケンス番号を ACK に格納して送信側に返す\n      - シーケンス番号が 3001 で 1000 バイトのデータを受信したら、受信側は 4000 までのシーケンス番号のデータを受け取ったということ\n      - 受信側は ACK に 4001 のシーケンス番号を付与して、送信側に返す\n      - 送信側は、該当するセグメントから MSS 分のデータを送信する\n        - MSS は 3WayHandShake によって通知される\n- コネクションの確立(3WayHandShake)\n  - 以下の 3 つの手順により TCP コネクションを確立すること\n    - クライアント側からサーバ側に Syn(SYN=1, ACK=0)を送る\n      - シーケンス番号は、ランダム(111 とする)\n    - サーバ側はクライアント側に SynAck(SYN=1, ACK=1)を送る\n      - シーケンス番号は、ランダム(222 とする)\n      - 確認応答番号は、相手から送られた Syn のシーケンス番号に 1 を加算した値(112)\n    - クライアント側はサーバ側に Ack(SYN=0, ACK=1)を送る\n      - シーケンス番号は、サーバ側から送られた確認応答番号(112)\n      - 確認応答番号は、相手から送られた SynAck のシーケンス番号に 1 を加算した値(223)\n- コネクションの切断(HalfClose)\n  - 以下の手順により TCP コネクションを切断する\n    - 送信側は最初に FIN を送信\n    - 受信側は ACK を送信し、その次に FIN も送信する\n    - 送信側は FIN を受信したらその ACK を返す\n      - 一定時間待ってコネクションを終了する\n  - 送信側のクローズ処理をアクティブクローズと呼ぶ\n  - 受信側のクローズ処理をパッシブクローズと呼ぶ\n- 再送\n  - 再送タイマーを利用する方法\n    - TCP ではセグメントを送信してからそれに対応する ACK が返答されるまでの時間を RTT(Round Trip Time, 往復遅延時間)として計測している\n    - この RTT よりもやや大きい時間を RTO(Retransmission Time Out, 再送タイムアウト)として算出しておく\n    - 送信したシーケンス番号のセグメントに対してタイマーをセットし、これに対応する ACK が RTO を経過しても届かないのであれば、該当セグメントは消失したものとして再送を行う\n  - ACK を利用する方法\n    - 受信側は次に受け取るべきシーケンス番号を ACK に付与して送信側に返すが、これは該当セグメントが届くまで同じシーケンス番号を記載した ACK を返送し続ける\n    - 送信側は同じシーケンス番号を記載した ACK を複数回(3 回)受信した場合、該当セグメントを焼失したと判断し、再送を行う\n- フロー制御\n  - 受信側が受信バッファが溢れないように許容可能なデータ量を送信側に通知して、送信側に送信データ量を調整する制御のことをフロー制御と呼ぶ\n  - TCP では、送信側が一度に送信可能なデータの枠をウィンドウとよび、その大きさをウィンドウサイズと呼ぶ\n    - 送信側は、swnd(send window)というパラメータでこれを指定する\n  - 受信側は、受信可能なデータ量を受信ウィンドウサイズ(rwnd: receive window)というパラメータで、送信に通知する\n  - 送信側はこの rwnd を超えないように送信するデータ量を調整する\n  - 受信バッファがあふれたときの挙動\n    - 受信側で TCP の受信バッファの空きがなくなると、送信側に rwnd==0 でパケットを返す(TCP ZeroWindow)\n    - 送信側は、rwnd==0 のパケットを受信すると、受信側に対する送信を一時停止する\n    - 送信側は、一定時間後に再度チェックとして、TCP ZeroWindowProbe を送信する\n    - 受信側は、受信バッファの空きがあれば、rwnd で教えるが、ない場合はまた rwnd==0 で Ack を返す(TCP ZeroWindowProbeAck)\n    - 送信側は、これが続く場合にはリセットを送信してそのコネクションを切断する\n- 輻輳制御\n  - 輻輳制御アルゴリズムに基づきネットワークの輻輳を判断し、送出するデータ量を調整する\n  - 輻輳ウィンドウサイズ(cwnd: congestion window)というパラメータでデータ量を調整する\n    - swnd は rwnd と cwnd に基づいて決定される\n      - 基本的にはこれらのうち小さいほうが優先される\n  - Loss-based 輻輳制御\n    - データの消失からネットワークの輻輳を判断する\n    - 基本的には以下\n      - ACK を受け取った後に、次のデータを送る\n      - 一度に送信するデータ量を徐々に増加させていく(スロースタート)\n      - データ消失を検知したら、送出量を減らす\n    - このようにウィンドウサイズを増減させながらデータを転送していく方式を「スライディングウィンドウ」という\n    - Tahoe\n      - 初期の TCP アルゴリズム\n      - スロースタート、輻輳回避、高速再転送\n    - Reno\n      - Tahoe に高速リカバリを追加したもの\n    - NewReno\n      - Reno を改良し、高速リカバリ段階において複数の消失セグメントの再送に対応したバージョン\n  - Delayed-based 輻輳制御\n    - RTT 用いてネットワークの輻輳を判断する\n    - Vegas\n      - RTT の変動をもとに混雑量を予測し転送量を調整するアルゴリズム - BBR(Bottleneck Bandwidth and Round-trip propagation time) - Google が 2016 年に発表し、Linux4.9 以降で利用可能となった - 思想的にはバッファ遅延を発生させないようにはするが、データネットワークの帯域はフルで利用することを目指している\n  - Hybrid 輻輳制御\n    - Loss-based と Delayed-based の両者を取り入れた方法\n    - BIC(Binary Increase Congestion control)\n      - 2004 年に発表され、Linux2.6.8 まで使われてきた\n      - 安定性、スケーラビリティ、RTT 公平性という特徴を持ち CUBIC のベースとなるアルゴリズム\n    - CUBIC\n      - BIC の改良版で、Linux2.6.18 で BIC からこれに置き換えられ利用されている\n- Multipath TCP(mptcp)\n  - Linux5.6 から利用可能になった、iOS11 以降でも利用できる\n  - 複数インターフェイスを使って TCP コネクションをはって通信を行う\n\n## ポート番号\n\n- TCP, UDP ともに番号を持っており、ポート番号からどのアプリケーションの通信かを判断し、ソケットを介してアプリケーションとデータのやり取りを行う\n- ポート番号には以下の種類がある\n  - 0-1023 の範囲のポートは、ウェルノウンポートと呼ばれ、一般的なアプリケーションで利用される予約済みのポートである\n  - 1024–49151 の範囲のポートは、登録済みポートと呼ばれ、通常アプリケーションが自由に利用してよいポートである\n    - サーバサイドのアプリケーションで用いられる\n  - 49152–65535 の範囲のポートは、 動的・プライベート ポート番号 とよばれ、一時的な使用に用いられる\n    - 基本的に送信時のエフェメラルポートとして利用され、サーバサイドのアプリケーションで用いられることはない\n    - プロキシサーバなどでは、送信ポート数を潤沢にするためにエフェメラルポートの範囲を 10000–65535 のように変更して利用したりもする\n","UpdatedAt":"2021-03-15T21:55:34.7630302+09:00"},{"Text":"# Network History\n\n- 1968 年、インターネットの前身である ARPANET が発足\n  - 1960 年代前半までは、電話回線を用いた回線交換によるコンピュータの遠隔接続が行われていた\n  - 1966 年にはアメリカ大陸をほぼ横断するコンピュータ間の通信実験も成功\n  - しかし、回線交換方式がゆえに回線を独占的に確保する必要があるし、1 対 1 の接続しかできないという問題があった\n  - これらの問題を解決するためにパケット交換方式が考案された\n  - パケット交換方式は、メッセージをパケットと呼ばれる単位に分割して、ネットワーク機器を経由して目的地まで転送する\n  - このパケット交換によるネットワークの代表的なプロジェクトが ARPA(Advanced Research Project Agency, 現代の DARPA)で発足した ARPANET である\n- 1969 年、UNIX の初期リリース\n  - 開発当初の UNIX は g 千振言語で記述されていたが、1973 年に C 言語で書き直された\n  - ソースコードも大学や研究機関に無償配布され、この UNIX がのちに ARPANET で使用する OS として採用された\n  - 1969 年に AT\u0026T の Bell 研究所の Kenneth Thompson と Dennis Ritchie が UNIX の初期バージョンを作成した\n  - コードの権利は AT\u0026T が持っており、ライセンス費用も高かった\n- 1970 年、University of Hawaii のキャンパスをつなぐネットワークとして ALOHAnet が構築される\n  - これはハワイ諸島を点在するキャンパス間を接続する世界初の無線パケット交換ネットワークであった\n- 1974 年、TCP/IP の開発がスタート\n  - 1974 年には ARPANET にはアメリカ全土に広がるコンピュータネットワークとなった\n  - その頃には、さまざまなネットワークプロトコルが乱立し、相互接続などについての問題が顕在化しており、これを解決するために TCP/IP の開発が始まった\n  - 1983 年初頭には ARPANET の通信プロトコルは従来の NCP から TCP/IP へと完全に切り替えられた\n  - さらに 1983 年には UNIX 系 OS である BSD が TCP/IP を標準サポートするようになり、TCP/IP の普及へとつながっていく\n  - そして、1995 年頃には輻輳制御などの基礎機能がほぼ整った\n- 1980 年、イーサネット規格公開\n  - イーサネット規格は、ALOHAnet をベースに 1970 年代に原形が開発され、1980 年に IEEE に提出公開され、1982 年に IEEE 802.3 として策定された\n- 1986 年、NSFnet の運用開始\n  - NSF(NAtional Science Foundation: 全米科学財団)は、全米 6 か所のスーパーコンピュータセンタを 56kbps の専用線で結んで運用開始した\n  - 日本でも 1984 年に東京大学、東京工業大学、慶応義塾大学の 3 大学を結ぶネットワークとして JUNET の実験が開始されている\n- 1990 年、インターネットへの移行と WWW の誕生\n  - 1980 年代後半になると、ARPANET と NSFnet が相互接続されたネットワークを指す固有名詞として「インターネット」という言葉が使われるようになった\n  - インターネットはネットワーク同士を接続することが容易という特徴もあり、世界規模の TCP/IP ネットワークを構成するようになった\n  - その後、ARPANET プロジェクトも終了した\n  - 1990 年には、WWW(World Wide Web)が提案され、それに続いて HTML、Web ページ、Web ブラウザが開発された\n- 1995 年、Windows 95 の発売とともに TCP/IP も本格普及した\n- 1999 年、IPv6 運用開始\n  - 1990 年代にインターネット利用者が急増したことで IP アドレス枯渇という問題が指摘されるようになり、IPv6 が開発された\n  - 1999 年にこの IPv6 アドレスの割り当てが始まった\n- 1999 年、無線 LAN 登場\n  - 1999 年に無線 LAN の規格として「IEEE 802.11b」が登場\n  - 2000 年から徐々に普及していった\n- 2000 年代初期、インターネット回線の充実化\n  - ADSL(Asymmetric Digital Subscriber Line)\n    - 電話回線で高速な通信を実現した回線\n    - 技術自体は 1990 年代に完成し、日本で利用開始したのは 1999 年から始まり 2000 年初期頃からいっきに普及していった\n  - 光ファイバ\n    - 光ファイバによる通信の開発は 1970 年頃から開始された\n    - 1980 年代には各国で公衆通信網の基幹回線への普及が進んだ\n      - 日本でも 1985 年には日本縦貫光ファイバ伝送路が完成している\n      - ちなみに 1985 年は NTT 発足の日でもある\n    - 2003 年から家庭向けの光回線が登場し普及していく\n- 2007 年、スマートフォンの普及\n  - 2007 年に iPhone が発売されて以降、スマートフォンが普及していき、モバイル通信のトラフィックが激増していく\n  - 2008 年に Android のリリース\n- モバイルネットワークの高速化\n  - 1980 年代、1G\n  - 2000 年初期、2G の i モードといったモバイルネットワークが普及\n  - 2000 年中頃、3G が普及\n  - 2010 年、3.9G(LTE)が普及\n  - 2015 年、4G の LTE Advanced が普及し始め、理論上の通信速度は 1Gbps に達することになった\n  - 2017 年、5G の仕様が策定\n  - 2020 年、5G が開始\n- 様々なインターネットサービスの登場\n  - 1994 年、Yahoo\n  - 1996 年、Yahoo! JAPAN\n  - 1998 年、Google\n  - 2000 年、Google が日本語の検索サービスを開始\n  - 2000 年、Amazon.co.jp が日本でサービスを開始\n  - 2001 年、Wikipedia\n  - 2003 年、Skype\n  - 2004 年、Facebook\n  - 2004 年、mixi\n  - 2005 年、YouTube\n  - 2006 年、Twitter\n  - 2006 年、ニコニコ動画\n  - 2006 年、AWS(Amazon Web Services)\n  - 2011 年、Line\n","UpdatedAt":"2021-03-15T21:55:34.7642868+09:00"},{"Text":"# OS\n\n## OS とは\n\n- OS(Operating System)とは、コンピュータのハードウェアとユーザアプリケーションの中間に位置するシステム\n- ハードウェアの各リソース管理、ユーザアプリケーションの管理、ユーザアプリケーションへ各リソース管理のためのインターフェイスの提供を行う\n\n## OS History\n\n### UNIX\n\n- すべての原点となる OS であり、あらゆる OS がこれを参考にしている\n- 1969 年に AT\u0026T の Bell 研究所の Kenneth Thompson と Dennis Ritchie が UNIX の初期バージョンを作成した\n- コードの権利は AT\u0026T が持っており、ライセンス費用も高かった\n\n### BSD\n\n- 1977 年に UC バークレーで、何人かの学者がライセンスの状況に不満を持ち、研究や教育を目的とした AT\u0026T のライセンスコードを一切含まない Unix のバージョンを作った\n\n### GNU\n\n- GNU プロジェクトは 1983 年にリチャード・ストールマンによって開始され、フリーソフトウェアのみによって「完全な Unix 互換ソフトウェアシステム」を作り上げることをプロジェクトのゴールとしていた\n- 1990 年代初頭までに、オペレーティングシステムに必要な多くのプログラムが完成していたが(ライブラリ、コンパイラ、シェル)、いくつかの要素が未完成であった\n\n### POSIX\n\n- Portable Operating System Interface for UNIX の略\n- 1980 年代中頃、様々な UNIX 系 OS のインターフェイスを標準化するプロジェクトが開始され、その標準化のベースとして UNIX が選択されできたのが、POSIX(IEEE 1003)\n- POSIX は、各種 OS 実装に共通のアプリケーションプログラミングインタフェース (API) を定めた\n- 規格の内容はカーネルへの C 言語のインタフェースであるシステムコールに留まらず、プロセス環境、ファイルとディレクトリ、システムデータベース（パスワードファイルなど）、アーカイブフォーマットなど多岐にわたる\n\n### MINIX\n\n- 1987 年にオランダ・アムステルダム自由大学の教授であるアンドリュー・タネンバウムによって開発された Unix 系 OS\n- 研究や教育を目的にしており、企業ライセンスのしがらみがないように書かれた\n\n### Linux\n\n- 1991 年、リーナス・トーバルズは自分の PC 用のオペレーティングを書き、これを公開した\n- 当時、近代的な OS を動作させる能力を持つ Intel 80386 CPU を搭載した 32 ビット PC/AT 互換パーソナルコンピュータが登場しており広く普及していた\n- 当初の Linux は、Intel 80386 でしか動かなかったが、ライバルの BSD は 1992 年から USL との訴訟を抱えており、1990 年代前半において自由な Unix 互換のカーネルは Linux のみだった\n- このような背景もあり、多くの開発者が Linux により多くの機能を求めて改良されていった\n- ちなみに、Linux の初期の開発は MINIX 上で開始された\n\n### FreeBSD\n\n- 1993 年に、BSD をもとにして開発されリリースされたフリーでオープンソースの OS\n- Appele の Darwin(MacOS/OS X/iOS/watchOS/iPadOS), PlayStation 3, 4 などの OS も FreeBSD のコードをベースとしている\n\n### DOS/MS-DOS/Windows\n\n- Disk Operating System\n- DOS/360\n  - 1966 年にリリースされた IBM メインフレーム用の最初の DOS\n- MS-DOS\n  - Microsoft が開発した 1981 年発売の IBM PC 用の DOS\n  - その後 GUI とユーティリティを DOS に追加し、商用のポータブル OS にしたものが Windows\n  - 1993 年に Windows NT 3.1 がリリースされ、2000,XP,7,10 と続いていく\n\n## Linux Distribution\n\n- Linux Distribution は、Linux Kernel とその他のソフトウェア群を一つにまとめたもの\n- コミュニティで運用されるもの、企業によって運用されるもの、また既存の Distribution からの派生で様々なものがある\n\n### Debian 系\n\n- Debian\n  - コミュニティベース\n- Ubuntu\n  - Canonical によって開発、運用されている\n  - LTS は５年は無料だが、10 年の場合は商用サポートが必要\n\n### Red Hat 系\n\n- Red Hat Linux\n  - Red Hat(IBM)によって開発、運用されている\n  - もともとは無料版と有料版（サポート）を提供していたが、利用者の多くが無料版を利用するため、無料版を Fedora として分離した\n- Fedra\n  - コミュニティベース（Red Hat が支援）\n  - 最新の Kernel や技術を積極的に取り入れている\n- CentOS\n  - Red Hat 派生のコミュニティベースだったが、途中から Red Hat が吸収した\n  - Red Hat Linux をほぼそのまま利用している\n  - Red Hat と同等の品質で無料で利用できるということもあり人気があったが、IBM の買収を契機に CentOS Stream へ移行され、廃止となる\n- CentOS Stream\n  - Redhat のアップストリームとして開発されている\n  - 修正は Red Hat に導入される前に入るのでバグなどの修正も速いが、バグの混在もしやすい\n  - Red Hat によるサポートも 5 年までとなっている\n- Red Hat 派生のディストリビューション\n  - Red Hat Linux 自体は有料だが、Linux Kernel を含む多くのソースコードは GPL なので、その多くの資産を再配布することは許可されている\n    - ただし、Red Hat の商標は取り除く必要がある\n  - Oracle Linux\n    - 無料版と有料版があり、その違いはサポートがあるかどうか\n    - Oracle 製品に最適化した Unbreakable Enterprise Kernel を提供している\n    - Red Hat Kernel を利用することもできる\n\n## References\n\n- [Modern Operating Platforms — Evolution History](https://medium.com/@scan.pratik/modern-operating-platforms-evolution-history-dbc094002aef)\n","UpdatedAt":"2021-04-03T13:50:29.9348095+09:00"},{"Text":"# プロセスとスケジューラ\n\n## 目次\n\n| Link                                                                                      | Description                                                                |\n| ----------------------------------------------------------------------------------------- | -------------------------------------------------------------------------- |\n| [プロセスとスレッド](#プロセスとスレッド)                                                 | プロセス、スレッド、カーネルスレッド、/proc/[pid]の中身                    |\n| [システムコール](#システムコール)                                                         | システムコール、CPU のモード切り替え、strace、プロセス関連のシステムコール |\n| [スケジューラの仕組み](#スケジューラの仕組み)                                             | tick kernel、tickless kerne、スケジューリングポリシー、CFS                 |\n| [mpstat](#mpstat)                                                                         | mpstat コマンドの見方                                                      |\n| [プロセスとスケジュラ関連のカーネルスレッド](#プロセスとスケジュラ関連のカーネルスレッド) | watchdog, khungtaskd, migration                                            |\n| [割り込み](#割り込み)                                                                     | 同期割り込み、非同期割り込み                                               |\n\n## プロセスとスレッド\n\n- プロセスはプログラムの実行を管理するための箱\n  - 実態はカーネル内のデータ構造\n  - プロセスは、プロセス ID(pid)を持っており、/proc/[pid]/ にそのプロセスのデータを見ることができる\n- スレッドはプロセスの実行単位\n  - スレッドとは、プロセスの実行単位で、スケジューラはこのスレッドの単位でスケジューリング(CPU の割り当て)を行う\n  - /proc/[pid]/task/ にスレッド(プロセス)のデータ構造が格納されている\n    - プロセスにはスレッドが必ず一つ以上含まれており、プログラムを実行したときに最初のスレッドが作成され、このスレッドの ID(tid)がプロセスの pid となる\n    - スレッドを作成すると、task_struct と mm_struct が生成される\n    - /proc/[pid]/task/[tid] にそのスレッドのデータを見ることができる\n      - /proc/[tid]でも実は見れる\n      - しかし、ls /proc するとスレッド[tid]は表示されない\n  - プロセスは少し抽象的な言い方で、スレッド単体の事をプロセスと呼ぶこともあるし、あるスレッドの子スレッドも含めた集合をプロセスと呼ぶこともある\n- プロセスの親子関係\n  - プロセスは親子関係を持つ\n  - 新規のプロセスは基本的に 1 番(init プロセス)の子プロセスになる\n    - 新規プロセスの中で子プロセスを生成することも可能\n    - 全プロセスは init プロセスを頂点としたツリー構造となる\n      - pstree コマンドで確認できる\n  - 新規のプロセス(子プロセス)は、clone()システムコールにより生成される\n    - 昔は fork()システムコールで生成していたので、その名残で子プロセスの生成を fork と呼ぶこともある\n    - clone はスレッドも作れる\n- スレッドの状態\n  - R: CPU 実行中、CPU 割り当て待ち\n  - S: 割り込み可能なイベント待ち\n    - network, pipe など\n    - kill などでシグナル送れる\n  - D: 割り込み不かなイベント待ち\n    - disk io など\n  - T: stopped/traced, シグナル(Ctrl+z)やデバッガで一時停止中\n  - Z: 実行終了後は終了、親プロセスへ渡す終了ステータスだけ維持\n    - 親がこれを取らないと、残り続ける\n  - X: 終了、ほぼ観測できない\n- カーネルスレッド\n  - カーネルが生成するスレッド\n    - kthread, migration/0, ksoftirqd/0, watchdog/0, events/0 など\n    - スレッドなので通常スレッドと同様にスケジューリングされるが、init には属していない\n    - しかし、重要なスレッドなので実行の優先度が高く、スケジューリングされやすくなっている\n  - プロセス名には慣例として[]がついており、CPU ごとに作成される場合には/0 のように数字がついてる\n    - 例: [watchdog/0], [watchdog/1]\n    - CPU ごとに作成されるカーネルスレッド\n      - https://www.kernel.org/doc/Documentation/kernel-per-CPU-kthreads.txt\n  - カーネルでは今すぐに実行しなくてもよいタスクは、キューに登録しておき、別スレッドで登録されたタスクを実行している\n    - トップハーフ: 今やらないといけない処理、タスクの登録などもする\n    - ボトムハーフ: 非同期で処理していいもの、カーネルスレッドで処理する\n- /proc/[pid]/の中身\n  - cmdline - プロセスのコマンドライン\n  - environ - 環境変数\n  - cwd - ワーキングディレクトリへのリンク\n  - exe - 実行したバイナリファイルへのリンク\n  - fd, fdinfo - ファイルディスクリプタ\n  - oom_adj, oom_score - OOM についての情報\n  - pagemap, maps, smaps, numa_maps - メモリ情報\n  - autogroup, cgroup, sessionid - プロセスグループの情報\n  - wchan - カーネル内で待機してる理由\n  - stack - カーネルスタック\n  - stat, statm, status - プロセスの統計情報\n\n## システムコール\n\n- スレッドがシステムコールを呼び出すとき、通常は libc(glibc など)によってラップされた関数を利用する\n- カーネルは番号でシステムコールを認識してる\n- CPU のモード切替\n  - システムコールや割り込みなどでカーネルの処理が必要になった場合、CPU の保護モードを特権モードに変更\n  - その後、カーネルスタックに切り替えたのち（通常の関数呼び出しと同様)スタックにレジスタを退避し、カーネル上の必要な個所へジャンプして処理する\n  - ちなみに、これはコンテキストスイッチとは言わない\n    - コンテキストスイッチはスレッドを切り替えることを指す\n- あるプログラムがどのようなシステムコールを読んでるかは、strace を実行すると見ることができる\n  - strace [cmd]\n    - コマンドをシステムコールをトレースする\n    - システムコールがエラーを起こした場合、カーネルは理由を示すエラー値(整数、errno)を返す、E から始まる名前がついてる\n    - 例: strace ls\n  - strace -e [systemcalls][cmd]\n    - システムコールをフィルタリングして表示する\n    - 例: strace -e open,close ls\n  - strace -p [pid]\n    - 既存プロセスにアタッチしてトレースする\n  - strace -f ...\n    - 通常のトレースはそのスレッドのみをトレースするが、-f を付けるとスレッドが新たにスレッドやプロセスの作成したときにそのスレッドの処理もトレースする\n  - strace -o [outputfile] ...\n    - トレースの結果をファイルに書き出す\n  - strace ... 2\u003e\u00261 | less\n    - トレースの結果を less で見る\n  - strace -t ...\n    - タイムスタンプを表示する\n- プロセス関連のシステムコール\n  - fork\n    - プロセスを生成する\n  - clone\n    - スレッドを作成する、プロセスを作成する\n    - clone は fork を汎用化したシステムコール\n      - 親プロセスに割り当てられたリソースをコピーするよう指定するとプロセスを作成(fork と同じ動作)する\n      - 親プロセスとリソースを共有するよう指定した場合、スレッドを作成する\n      - glibc の内部では fork 関数を呼ぶと内部では clone を呼び出してる\n  - execve\n    - 現在のプロセス内で実行しているプログラムを指定したプログラムと置き換え、実行開始する\n  - wait/wait4\n    - clone で作成した子プロセスの終了を待つ\n  - exit\n    - プロセスの終了、プロセスは終了ステータスだけを持つ状態になる\n  - getpid, geteuid, getegrp\n    - プロセスの基本的な情報取得\n- プロセス生成の流れ\n  - clone でプロセスを作成する\n  - execve でプログラムを実行する\n    - execve 時には実行するファイルのパス、引数、環境変数を指定する\n  - プロセスは exit で終了し、終了ステータスだけを持つ状態になる(zonbi 状態)\n  - 親プロセスが wait でこの終了ステータスを拾うとプロセスは消える\n- 子プロセスが終了する前に親プロセスが終了すると、子プロセスは init プロセスの子プロセスとなる\n- man\n  - man 1 strace\n  - man 2 syscalls\n  - man 2 syscall\n\n## スケジューラの仕組み\n\n- tick kernel\n  - centos5 以前\n  - 定期的にスケジューラがタイマー割り込みで入り(tick)、次に実行するスレッドへコンテキストスイッチを実行する\n    - この割り込みは CPU ごとに行われるので、スケジューラは CPU 単位で実行される\n    - スケジューラはスレッドではなく、スケジューリング処理のことを指す\n- tickless kernel\n  - centos6 以降\n  - システムコールが呼ばれたり、タイマー割り込みや、ハードウェア割り込みなどのイベントが発生すると、スケジューラが呼ばれる\n    - このようなイベントは CPU 単位で発生し、スケジューラもまた CPU 単位で実行される\n    - スケジューラはスレッドではなく、スケジューリング処理のことを指す\n  - スケジューラは呼ばれると、ロックをかけてその CPU でスケジューリングが多重実行されないようにする\n  - スケジューラは次に実行すべきスレッドを決定し、ロックを外して、コンテキストスイッチを行う\n  - そして、またイベントが発生するとスケジューラが呼ばれ、次のスレッドがスケジューリングされて実行される\n  - イベントが発生するまでは idle になる\n    - しかし、c3_state(低消費電力状態)から動作再開には少し時間がかかる\n    - c3_state にならないようにするには、/dev/cpu_dma_latency に 0 を書き込む\n- コンテキストスイッチ\n  - CPU で実行するスレッドを切り替えること\n    - 同一プロセス内の別スレッド間で切り替える場合\n      - CPU のレジスタ内容を差し替える\n    - 別プロセスのスレッド間で切り替える場合(コストが高い)\n      - CPU のレジスタの内容を差し替える\n      - TLB の吐き出し、仮想アドレス空間(PageTable）を切り替える\n- スレッドはキューに積まれる\n  - CPU ごとに run キューがあり、実行可能なスレッドは run キューに積まれる\n  - IO の要求などでディスクなどの IO 待ちの場合は、IO 待ち用の wait キューに積まれる\n- スケジューリングポリシー\n  - top の PR の項目を見るとプロセスの優先度、スケジューリングポリシーがわかる\n  - スケジューリングポリシーは大きく、リアルタイムポリシーと通常のポリシーに分けられる\n    - 最初にリアルタイムスレッドがスケジュールされ、すべてのリアルタイムスレッドの後で通常のスレッドがスケジュールされる\n    - カーネルスレッドは重要度が高く、リアルタイムスレッドであることがよくある\n  - リアルタイムポリシー\n    - SCHED_FIFO\n      - 実行可能で、優先度が一番高いスレッドをスケジュールする\n    - SCHED_RR\n      - SCHED_FIFO と同じだが、与えられた quantum を消費すると停止する\n  - 通常ポリシー\n    - SCHED_OTHER(or SCHED_NORMAL)\n      - デフォルト\n      - CFS を使ってこのポリシーを利用するすべてのスレッドに対して公平なスケジューリングを行う\n    - SCHED_BATCH\n      - CPU 時間を付与するが、I/O 待ちによる優先度上昇なし\n    - SCHED_IDLE\n      - 他に実行すべきスレッドがない時にのみスケジューリングする\n  - chrt コマンド、sched_setscheduler システムコールで設定できる\n- CFS(Completely Fair Scheduler)\n  - centos6 以降で使われてるスケジューラ\n  - スケジュール期間(period)を実行可能なスレッド間で分割し、その時間(slice)をスレッドに割り当てる\n    - 優先度の高いスレッドに slice を多めに割り当てる\n    - period = スレッド数 \\* sched_min_granularity_ns\n  - vruntime が最も小さいスレッドを選択してスレッドを割り当てる\n    - vruntime は、/proc/[pid]/sched に書かれてる\n    - 直近の割り当てが sched_min_granularity_ns を下回るときは、スケジューリングしない（再度同じスレッドに CPU を割り当てる)\n    - /proc/sys/kernel/sched_min_granularity_ns\n      - 大きくするとバッチ処理向き\n      - 小さくすると対話処理向き\n    - /proc/[pid]/schedstat\n      - スケジューリングに関する統計情報\n- スケジューラ関連のシステムコール\n  - nice\n    - プロセスの優先度を設定\n  - sched_yield\n    - 自分から CPU を手放したい時にスケジューラを明示的に呼び出す\n    - 他に cpu 実行したいスレッドがいなければまた自分スレッドが割り当てられる\n  - sched_setscheduler, sched_getscheduler\n    - スケジューリングポリシーを設定する\n  - sched_setaffinity, sched_getaffinity\n    - スレッドが動作可能な CPU のセットを設定する\n\n## mpstat\n\n- %idle プロセスがいない場合は idle に分類\n- %iowait プロセスがいないけど、IO 待ちがある場合は iowait に分類される\n- %guest ゲストで使ってる cpu の割合\n- %steal ゲスト側からどのくらい cpu 割り当てられなかったか(ゲスト、ホスト間で情報交換して計算してる)\n- %user ユーザプロセスを実行していた時間\n- %system カーネルの実行していた時間\n- %nice 優先度を下げたプロセスを実行していた時間\n- %irq 非同期割り込みの処理時間(hardware interrupt)\n- %softirq softiq の処理時間(software interrupt)\n\n```\n$ mpstat 1\nCPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle\nall   10.05    0.00   40.20    0.00    0.00    0.00    0.00    0.00    0.00   49.75\nall   12.06    0.00   38.19    0.00    0.00    0.00    0.00    0.00    0.00   49.75\n```\n\n## プロセスとスケジュラ関連のカーネルスレッド\n\n- watchdog カーネルスレッド\n  - スケジューラが正しく動いてるかをチェックするカーネルスレッド\n  - 「BUG: soft lockup - CPU#0 stuck for 30s!..」というログについて\n    - 同じスレッドがずっと同じ CPU に割り当てられていると、スケジューラのバグではと watchdog が判断してこのログが出る\n    - 仮想環境でよくある\n    - vcpu にスレッドが割り当てられたが、vcpu 自体のスレッドに物理 CPU が割り当てられてないと、vcpu はその間実行できず、スレッドも割り当て状態となる\n      - そして次に vcpu に物理 CPU が割り当てられた際に、同じ vcpu に同じスレッドがずっと割り当てられてると watchdog が勘違いする\n- khungtaskd カーネルスレッド\n  - ブロックされてハングアップしたプロセスを定期巡回して、プロセスのスタックダンプを取る\n  - 「INFO: task ... blocked for more than 120 seconds.」というログについて\n    - スレッドが割り込み禁止状態のまま 120 秒以上経過してると、何かおかしいのではと、このログが出る\n    - メッセージにスタックトレースも出力されるので、待ち状態のスレッドが何を待っているのかを確認するとよい\n- migration カーネルスレッド\n  - migration とはスレッドを実行する CPU を変更すること、これを行うのが migration カーネルスレッド\n  - /proc/\u003cpid\u003e/sched の se.nr_migrations を見るとそのスレッドが何回マイグレーションされたかわかる\n  - コスト設定(/proc/sys/kernel/sched_migration_cost_ns を設定)することで、その時間を使うではマイグレーションしないようになる\n  - スレッドが配置可能な CPU を制御することができる\n    - /proc/[pid]/status の Cpu_allowd で確認できる\n    - numactl や taskset、sched_setaffinity システムコールで設定できる\n\n## 割り込み\n\n- 割り込みには番号があり、割り込みが発生すると CPU はその番号に対応するハンドラを実行する\n  - cat /proc/interrupts で割り込み番号、処理回数を確認できる\n- 割り込みハンドラはカーネルが設定する\n- 非同期割り込み\n  - CPU 外のハードウェアから通知される割り込み\n  - タイマー割り込み\n  - HDD の DMA 転送が終わった\n  - Ethernet カードがパケットを受信した\n- 同期割り込み(例外)\n\n  - ゼロで除算した\n  - アクセス許可のないメモリにアクセスしようとした\n  - 特権がないのに特権命令を実行した\n\n- 割り込みのカウンタを確認\n\n```\n$ cat /proc/interrupts\n           CPU0       CPU1\n  0:         18          0   IO-APIC   2-edge      timer\n  1:          2          0   IO-APIC   1-edge      i8042\n  8:          1          0   IO-APIC   8-edge      rtc0\n  9:          0          0   IO-APIC   9-fasteoi   acpi\n 12:          4          0   IO-APIC  12-edge      i8042\n 16:         26          3   IO-APIC  16-fasteoi   ehci_hcd:usb1\n```\n\n- 割り込み処理を割り込みハンドラですべて動的に処理すると、遅延してしまうので、実施すべき処理を登録だけしてあとはスケジューラに任せる\n- この登録した処理を softirq と呼び、CPU ごとにこれを実行するスレッド(ksoftirqd)が用意されている\n\n```\n$ ps ax | grep ksoftirqd\n  3 ? S 0:00 [ksoftirqd/0]\n  13 ? S 0:00 [ksoftirqd/1]\n\n$ cat /proc/softirqs\nCPU0 CPU1\nHI: 3 2\nTIMER: 1246628 2315450\nNET_TX: 1 14868\nNET_RX: 22540 209467\nBLOCK: 19536 500141\nBLOCK_IOPOLL: 0 0\nTASKLET: 58 333\nSCHED: 1240211 2250434\nHRTIMER: 0 0\nRCU: 682194 2333117\n\n```\n\n## ハードウェア割り込み\n\n- IO APCI による割り込み\n  - 割り込みは CPU ごとに存在する Local APIC(Advanced Programmable Interrupt Controller)と呼ばれる割り込みコントローラと、ICH(South bridge)に内蔵される IO APIC によって実現される\n  - IO APIC は、PCI デバイスからの割り込みを受け取り、IO APIC の設定に基づいて割り込みを Local APIC にリダイレクトする\n  - Local APIC は割り込み用のレジスタにアクセスされると、CPU にその割り込みの種類に応じたハンドラを実行させる\n  - この割り込みハンドラは、IDT(Interrupt Descriptor Table)と呼ばれ最大 255 エントリのテーブルで管理されており、1 エントリには 0-255 のベクタ番号と、ハンドラのアドレスを格納している\n  - IDT はメモリ上に展開され、そのアドレスを IDTR レジスタに格納することで CPU から参照される\n- MSI/MSIX による割り込み\n  - IO APIC によるデバイス割り込みはあまり使われていない\n  - IO APIC を経由する PCI デバイス割り込みは、各 PCI デバイスからの物理的な割り込み線を利用するため、ひとつのデバイスで複数の割り込みを持つことができなかった\n  - そこで、PCI バス経由のメッセージとして割り込みを送る Message Signalled Interrupt(MSI)、Extended MSI(MSI-X)が導入された\n  - これらは、IO-APIC を経由せずに直接 Local APIC へ配送される\n  - この時の宛先 CPU の設定は各 PCI デバイスの Configuration Space の Destination ID に設定される\n\n## 内部割込みと外部割込み\n\n- 内部割込みとは、CPU 内部の要因で発生する割り込みの事で、ソフトウェア割り込み(スーパーバイザコール割り込み)と、例外(プログラム割り込みとも呼ばれる)に分類される\n  - ソフトウェア割り込みとは、システムコールなどで使われる INT 命令による割り込みのこと\n  - 例外割込みとは、ゼロ除算・オーバーフロー・無効な命令の実行・ページフォルトなどが発生したときにおこる割り込みのこと\n- 外部割込みとは、ハードウェアから CPU へ「キーボードが押された」などのイベント通知を行うのに用いられる\n  - 外部割込みは、マスク可能な割り込みと、ソフトウェアからマスク可能な割り込み(NMI)の 2 種類に分かれる\n    - マスク可能な通常のデバイスから割り込みに用いられる\n    - NMI はハードウェア障害の通知などの特殊な用途に用いられる\n\n```\n\n```\n","UpdatedAt":"2021-03-15T21:55:34.7672908+09:00"},{"Text":"# Systemd\n\n## systemd\n\n## initscript\n\n- centos6 では、/etc/init.d/にデーモンサービス用の initscript を配置します\n  - centos7 でも、systemd 用のサービスを使わずに initscript を利用する場合もある\n- initscript の中身はただのシェルスクリプトで、 これが service や chkconfig コマンドから呼び出されて、 サービスの start, stop, restart などを提供します\n- 作り方\n  - ひな形が、/usr/share/doc/initscripts-\\*/svsvinitfiles にあるので、これを/etc/init.d に入れて使うといい\n  - 基本的にサンプルの中に作り方などがすべて書かれているので、それを参考にすると initscript が作成できます\n  - 既存の initscript を参考するのも良い\n\n```bash\n$ sudo cp /usr/share/doc/initscripts-9.03.46/sysvinitfiles /etc/init.d/openstack-keystone\n$ sudo chmod 755 /etc/init.d/openstack-keystone\n```\n","UpdatedAt":"2021-03-15T21:55:34.7692895+09:00"},{"Text":"# Testing\n\n## syzkaller\n\n- [github: syzkaller](https://github.com/google/syzkaller)\n- [1870 件以上のカーネルの不具合修正に貢献した再現用プログラムを自動生成する syzkaller のテスト自動化技術](https://qiita.com/fujiihda/items/e86c46c003cceb703a9c)\n- Google が公開している OSS のファジングツール\n\n## kselftest\n\n- [kselftest](https://www.kernel.org/doc/html/v4.15/dev-tools/kselftest.html)\n","UpdatedAt":"2021-03-15T21:55:34.7722862+09:00"},{"Text":"# XDP\n\n## References\n\n- [BPF and XDP Reference Guide](https://docs.cilium.io/en/latest/bpf/)\n- [XDP Hands-On Tutorial](https://github.com/xdp-project/xdp-tutorial)\n- [valinux: eBPF](https://github.com/oda-g/eBPF)\n- [valinux: AF_XDP アプリケーション性能特性の定性的評価 〜レイテンシ編](https://valinux.hatenablog.com/entry/20200924)\n","UpdatedAt":"2021-03-15T21:55:34.7736629+09:00"}],"IdPathMap":["README.md","blockdevice.md","container.md","cpu.md","cpu_hardware.md","debugging.md","debugging_strace.md","device.md","filesystem.md","iptables.md","kernel.md","kernel_boot.md","kernel_build.md","kernel_driver.md","kernel_panic.md","memory.md","memory_hardware.md","memory_tuning.md","network.md","network_basic.md","network_history.md","os.md","process_scheduler.md","systemd.md","testing.md","xdp.md"],"PathMap":{"README.md":0,"blockdevice.md":1,"container.md":2,"cpu.md":3,"cpu_hardware.md":4,"debugging.md":5,"debugging_strace.md":6,"device.md":7,"filesystem.md":8,"iptables.md":9,"kernel.md":10,"kernel_boot.md":11,"kernel_build.md":12,"kernel_driver.md":13,"kernel_panic.md":14,"memory.md":15,"memory_hardware.md":16,"memory_tuning.md":17,"network.md":18,"network_basic.md":19,"network_history.md":20,"os.md":21,"process_scheduler.md":22,"systemd.md":23,"testing.md":24,"xdp.md":25}}